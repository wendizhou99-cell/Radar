# 设计模式应用

- **标题**: AI协作开发设计模式实践指南
- **当前版本**: v1.0
- **最后更新**: 2025-09-10
- **负责人**: Kelin

---

## 接口设计模式

### 🎯 策略模式 (Strategy Pattern)

#### 数据处理策略抽象
```cpp
/**
 * @brief 数据处理策略接口
 *
 * 定义了数据处理的统一接口，支持运行时切换不同的处理算法。
 * 适用场景：GPU/CPU处理切换、不同算法选择、性能优化策略。
 */
class IProcessingStrategy {
public:
    virtual ~IProcessingStrategy() = default;

    /**
     * @brief 处理输入数据
     * @param input 原始输入数据
     * @param output 处理结果输出
     * @return 处理状态码
     */
    virtual ProcessingResult process(
        const RawDataPacket& input,
        ProcessedData& output
    ) = 0;

    /**
     * @brief 获取策略名称
     * @return 策略的描述性名称
     */
    virtual std::string getStrategyName() const = 0;

    /**
     * @brief 获取策略性能特征
     * @return 延迟、吞吐量等性能指标
     */
    virtual PerformanceCharacteristics getPerformanceProfile() const = 0;
};

/**
 * @brief GPU加速处理策略
 *
 * 利用CUDA实现高性能并行处理，适合大数据量、计算密集的场景。
 */
class GPUProcessingStrategy : public IProcessingStrategy {
private:
    std::unique_ptr<CUDAProcessor> cudaProcessor_;
    cudaStream_t processingStream_;

public:
    explicit GPUProcessingStrategy(const CUDAConfig& config);

    ProcessingResult process(
        const RawDataPacket& input,
        ProcessedData& output
    ) override;

    std::string getStrategyName() const override {
        return "High-Performance GPU Processing";
    }

    PerformanceCharacteristics getPerformanceProfile() const override {
        return {
            .latency = std::chrono::microseconds(50),      // 超低延迟
            .throughput = 10e9,                            // 10GB/s吞吐量
            .powerConsumption = PowerLevel::High,          // 高功耗
            .memoryRequirement = MemorySize::Large         // 大内存需求
        };
    }
};

/**
 * @brief CPU优化处理策略
 *
 * 基于多核CPU的并行处理，平衡性能和资源消耗。
 */
class CPUProcessingStrategy : public IProcessingStrategy {
private:
    std::unique_ptr<ThreadPool> threadPool_;
    std::vector<std::unique_ptr<ProcessingWorker>> workers_;

public:
    explicit CPUProcessingStrategy(int threadCount);

    ProcessingResult process(
        const RawDataPacket& input,
        ProcessedData& output
    ) override;

    std::string getStrategyName() const override {
        return "Multi-threaded CPU Processing";
    }

    PerformanceCharacteristics getPerformanceProfile() const override {
        return {
            .latency = std::chrono::microseconds(200),     // 中等延迟
            .throughput = 2e9,                             // 2GB/s吞吐量
            .powerConsumption = PowerLevel::Medium,        // 中等功耗
            .memoryRequirement = MemorySize::Medium        // 中等内存需求
        };
    }
};
```

#### 策略管理器实现
```cpp
/**
 * @brief 处理策略管理器
 *
 * 负责策略的注册、选择和动态切换。支持基于性能要求、
 * 系统资源状况的自动策略选择。
 */
class ProcessingStrategyManager {
private:
    std::map<std::string, std::unique_ptr<IProcessingStrategy>> strategies_;
    std::string currentStrategyName_;
    std::unique_ptr<PerformanceMonitor> performanceMonitor_;

public:
    /**
     * @brief 注册处理策略
     * @param name 策略名称
     * @param strategy 策略实现
     */
    void registerStrategy(
        const std::string& name,
        std::unique_ptr<IProcessingStrategy> strategy
    ) {
        strategies_[name] = std::move(strategy);
    }

    /**
     * @brief 设置当前活动策略
     * @param strategyName 要激活的策略名称
     * @return 是否成功切换
     */
    bool setActiveStrategy(const std::string& strategyName) {
        auto it = strategies_.find(strategyName);
        if (it != strategies_.end()) {
            currentStrategyName_ = strategyName;
            LOG_INFO("Switched to strategy: " + strategyName);
            return true;
        }
        return false;
    }

    /**
     * @brief 基于性能要求自动选择策略
     * @param requirements 性能需求
     * @return 选中的策略名称
     */
    std::string selectOptimalStrategy(const PerformanceRequirements& requirements) {
        std::string bestStrategy;
        double bestScore = 0.0;

        for (const auto& [name, strategy] : strategies_) {
            auto profile = strategy->getPerformanceProfile();
            double score = calculateStrategyScore(profile, requirements);

            if (score > bestScore) {
                bestScore = score;
                bestStrategy = name;
            }
        }

        if (!bestStrategy.empty()) {
            setActiveStrategy(bestStrategy);
        }

        return bestStrategy;
    }

    /**
     * @brief 使用当前策略处理数据
     */
    ProcessingResult processWithCurrentStrategy(
        const RawDataPacket& input,
        ProcessedData& output
    ) {
        if (currentStrategyName_.empty() ||
            strategies_.find(currentStrategyName_) == strategies_.end()) {
            throw std::runtime_error("No active processing strategy");
        }

        auto& currentStrategy = strategies_[currentStrategyName_];

        // 性能监控
        auto startTime = std::chrono::high_resolution_clock::now();
        auto result = currentStrategy->process(input, output);
        auto endTime = std::chrono::high_resolution_clock::now();

        // 记录性能数据
        performanceMonitor_->recordProcessingTime(
            currentStrategyName_,
            std::chrono::duration_cast<std::chrono::microseconds>(endTime - startTime)
        );

        return result;
    }

private:
    /**
     * @brief 计算策略评分
     * @param profile 策略性能特征
     * @param requirements 性能需求
     * @return 策略适合度评分
     */
    double calculateStrategyScore(
        const PerformanceCharacteristics& profile,
        const PerformanceRequirements& requirements
    ) {
        double score = 0.0;

        // 延迟权重评分
        if (profile.latency <= requirements.maxLatency) {
            score += 0.4 * (1.0 - static_cast<double>(profile.latency.count()) /
                                   requirements.maxLatency.count());
        }

        // 吞吐量权重评分
        if (profile.throughput >= requirements.minThroughput) {
            score += 0.4 * (profile.throughput / requirements.minThroughput);
        }

        // 资源消耗权重评分
        score += 0.2 * calculateResourceScore(profile, requirements);

        return std::min(score, 1.0);  // 限制最大评分为1.0
    }
};
```

### 🏭 工厂模式 (Factory Pattern)

#### 抽象工厂设计
```cpp
/**
 * @brief 雷达组件抽象工厂
 *
 * 定义创建相关雷达组件族的接口。确保创建的组件之间
 * 兼容和协调工作。
 */
class IRadarComponentFactory {
public:
    virtual ~IRadarComponentFactory() = default;

    // 核心组件创建接口
    virtual std::unique_ptr<IDataReceiver> createDataReceiver() = 0;
    virtual std::unique_ptr<IDataProcessor> createDataProcessor() = 0;
    virtual std::unique_ptr<IDisplayController> createDisplayController() = 0;
    virtual std::unique_ptr<ITaskScheduler> createTaskScheduler() = 0;

    // 辅助组件创建接口
    virtual std::unique_ptr<IConfigurationManager> createConfigManager() = 0;
    virtual std::unique_ptr<IPerformanceMonitor> createPerformanceMonitor() = 0;
    virtual std::unique_ptr<ILogger> createLogger() = 0;

    // 工厂元信息
    virtual std::string getFactoryType() const = 0;
    virtual std::vector<std::string> getSupportedConfigurations() const = 0;
};

/**
 * @brief 高性能GPU工厂
 *
 * 创建基于GPU加速的高性能雷达组件族。
 * 适用于实时高吞吐量处理场景。
 */
class HighPerformanceGPUFactory : public IRadarComponentFactory {
private:
    CUDADeviceInfo deviceInfo_;
    GPUFactoryConfig config_;

public:
    explicit HighPerformanceGPUFactory(const GPUFactoryConfig& config)
        : config_(config) {
        // 初始化CUDA设备信息
        initializeCUDADevice();
    }

    std::unique_ptr<IDataReceiver> createDataReceiver() override {
        return std::make_unique<HighSpeedEthernetReceiver>(
            config_.networkConfig,
            config_.bufferSize
        );
    }

    std::unique_ptr<IDataProcessor> createDataProcessor() override {
        return std::make_unique<CUDAGPUProcessor>(
            deviceInfo_,
            config_.processingConfig
        );
    }

    std::unique_ptr<IDisplayController> createDisplayController() override {
        return std::make_unique<OpenGLDisplayController>(
            config_.displayConfig,
            deviceInfo_.deviceId  // 使用同一GPU设备
        );
    }

    std::unique_ptr<ITaskScheduler> createTaskScheduler() override {
        return std::make_unique<GPUOptimizedScheduler>(
            config_.schedulingConfig,
            deviceInfo_.maxConcurrentKernels
        );
    }

    std::string getFactoryType() const override {
        return "High-Performance GPU Factory";
    }

    std::vector<std::string> getSupportedConfigurations() const override {
        return {
            "real-time-processing",
            "high-throughput-batch",
            "low-latency-streaming",
            "gpu-memory-optimized"
        };
    }
};

/**
 * @brief 嵌入式低功耗工厂
 *
 * 创建适用于嵌入式系统的低功耗组件族。
 * 平衡性能和功耗，适用于资源受限环境。
 */
class EmbeddedLowPowerFactory : public IRadarComponentFactory {
private:
    EmbeddedSystemInfo systemInfo_;
    PowerManagementConfig powerConfig_;

public:
    explicit EmbeddedLowPowerFactory(const EmbeddedFactoryConfig& config)
        : powerConfig_(config.powerConfig) {
        systemInfo_ = getEmbeddedSystemInfo();
    }

    std::unique_ptr<IDataReceiver> createDataReceiver() override {
        return std::make_unique<LowPowerSerialReceiver>(
            powerConfig_.receiverConfig,
            systemInfo_.maxBaudRate
        );
    }

    std::unique_ptr<IDataProcessor> createDataProcessor() override {
        return std::make_unique<ARMOptimizedProcessor>(
            systemInfo_.cpuInfo,
            powerConfig_.processingConfig
        );
    }

    std::unique_ptr<IDisplayController> createDisplayController() override {
        return std::make_unique<FrameBufferDisplayController>(
            powerConfig_.displayConfig,
            systemInfo_.displayCapabilities
        );
    }

    std::unique_ptr<ITaskScheduler> createTaskScheduler() override {
        return std::make_unique<PowerAwareScheduler>(
            powerConfig_.schedulingConfig,
            systemInfo_.thermalConstraints
        );
    }

    std::string getFactoryType() const override {
        return "Embedded Low-Power Factory";
    }
};
```

#### 工厂注册和管理
```cpp
/**
 * @brief 工厂管理器
 *
 * 管理各种组件工厂的注册、查找和实例化。
 * 支持基于配置的自动工厂选择。
 */
class ComponentFactoryManager {
private:
    using FactoryCreator = std::function<std::unique_ptr<IRadarComponentFactory>()>;
    std::map<std::string, FactoryCreator> factoryCreators_;
    std::unique_ptr<IRadarComponentFactory> activeFactory_;

public:
    /**
     * @brief 注册工厂创建器
     */
    template<typename FactoryType, typename... Args>
    void registerFactory(const std::string& name, Args&&... args) {
        factoryCreators_[name] = [args...]() {
            return std::make_unique<FactoryType>(args...);
        };
    }

    /**
     * @brief 基于系统配置选择最优工厂
     */
    std::string selectOptimalFactory(const SystemConfiguration& config) {
        // 基于硬件能力评估
        if (hasHighEndGPU() && config.performanceLevel == PerformanceLevel::High) {
            return "high-performance-gpu";
        }

        // 基于功耗限制评估
        if (config.powerConstraints.maxPowerConsumption < 50.0) {  // 50W限制
            return "embedded-low-power";
        }

        // 基于实时性要求评估
        if (config.latencyRequirements.maxLatency < std::chrono::microseconds(100)) {
            return "real-time-optimized";
        }

        // 默认选择通用工厂
        return "general-purpose";
    }

    /**
     * @brief 创建并激活工厂
     */
    bool activateFactory(const std::string& factoryName) {
        auto it = factoryCreators_.find(factoryName);
        if (it != factoryCreators_.end()) {
            activeFactory_ = it->second();
            LOG_INFO("Activated factory: " + factoryName);
            return true;
        }
        return false;
    }

    /**
     * @brief 创建完整的雷达系统组件
     */
    std::unique_ptr<RadarSystem> createRadarSystem(const SystemConfiguration& config) {
        if (!activeFactory_) {
            throw std::runtime_error("No active factory");
        }

        auto system = std::make_unique<RadarSystem>(config);

        // 创建核心组件
        system->setDataReceiver(activeFactory_->createDataReceiver());
        system->setDataProcessor(activeFactory_->createDataProcessor());
        system->setDisplayController(activeFactory_->createDisplayController());
        system->setTaskScheduler(activeFactory_->createTaskScheduler());

        // 创建辅助组件
        system->setConfigManager(activeFactory_->createConfigManager());
        system->setPerformanceMonitor(activeFactory_->createPerformanceMonitor());
        system->setLogger(activeFactory_->createLogger());

        return system;
    }
};
```

### 🎭 观察者模式 (Observer Pattern)

#### 事件驱动架构
```cpp
/**
 * @brief 雷达系统事件基类
 *
 * 定义所有雷达系统事件的公共接口。支持事件时间戳、
 * 优先级和传播控制。
 */
class RadarSystemEvent {
public:
    enum class Priority { Low, Normal, High, Critical };
    enum class Type {
        DataReceived,
        ProcessingCompleted,
        ErrorOccurred,
        StateChanged,
        PerformanceAlert,
        ConfigurationUpdated
    };

private:
    Type type_;
    Priority priority_;
    std::chrono::system_clock::time_point timestamp_;
    std::string source_;
    bool propagationStopped_;

public:
    explicit RadarSystemEvent(Type type, Priority priority = Priority::Normal)
        : type_(type), priority_(priority),
          timestamp_(std::chrono::system_clock::now()),
          propagationStopped_(false) {}

    virtual ~RadarSystemEvent() = default;

    // 访问器
    Type getType() const { return type_; }
    Priority getPriority() const { return priority_; }
    auto getTimestamp() const { return timestamp_; }
    const std::string& getSource() const { return source_; }

    // 事件控制
    void setSource(const std::string& source) { source_ = source; }
    void stopPropagation() { propagationStopped_ = true; }
    bool isPropagationStopped() const { return propagationStopped_; }

    // 虚拟方法供子类实现
    virtual std::string getDescription() const = 0;
    virtual std::map<std::string, std::string> getDetails() const = 0;
};

/**
 * @brief 数据处理完成事件
 */
class ProcessingCompletedEvent : public RadarSystemEvent {
private:
    ProcessingResult result_;
    std::chrono::microseconds processingTime_;
    size_t dataSize_;

public:
    ProcessingCompletedEvent(
        const ProcessingResult& result,
        std::chrono::microseconds processingTime,
        size_t dataSize
    ) : RadarSystemEvent(Type::ProcessingCompleted),
        result_(result), processingTime_(processingTime), dataSize_(dataSize) {}

    const ProcessingResult& getResult() const { return result_; }
    auto getProcessingTime() const { return processingTime_; }
    size_t getDataSize() const { return dataSize_; }

    std::string getDescription() const override {
        return "Data processing completed with " +
               std::to_string(result_.detectedTargets.size()) + " targets detected";
    }

    std::map<std::string, std::string> getDetails() const override {
        return {
            {"processing_time_us", std::to_string(processingTime_.count())},
            {"data_size_bytes", std::to_string(dataSize_)},
            {"targets_detected", std::to_string(result_.detectedTargets.size())},
            {"snr_db", std::to_string(result_.averageSNR)},
            {"processing_status", result_.success ? "success" : "failed"}
        };
    }
};
```

#### 观察者接口和实现
```cpp
/**
 * @brief 雷达系统事件观察者接口
 */
class IRadarEventObserver {
public:
    virtual ~IRadarEventObserver() = default;

    /**
     * @brief 处理雷达系统事件
     * @param event 接收到的事件
     */
    virtual void onRadarEvent(const std::shared_ptr<RadarSystemEvent>& event) = 0;

    /**
     * @brief 获取观察者名称
     */
    virtual std::string getObserverName() const = 0;

    /**
     * @brief 获取感兴趣的事件类型
     * @return 事件类型列表，空表示对所有事件感兴趣
     */
    virtual std::vector<RadarSystemEvent::Type> getInterestedEventTypes() const {
        return {};  // 默认对所有事件感兴趣
    }

    /**
     * @brief 获取最低关注优先级
     */
    virtual RadarSystemEvent::Priority getMinimumPriority() const {
        return RadarSystemEvent::Priority::Low;
    }
};

/**
 * @brief 性能监控观察者
 *
 * 专门监控系统性能事件，统计处理时间、吞吐量等指标。
 */
class PerformanceMonitorObserver : public IRadarEventObserver {
private:
    struct PerformanceStatistics {
        double averageProcessingTime = 0.0;
        double maxProcessingTime = 0.0;
        double totalThroughput = 0.0;
        size_t totalProcessedPackets = 0;
        std::chrono::system_clock::time_point startTime;
    };

    PerformanceStatistics stats_;
    std::mutex statsMutex_;
    std::shared_ptr<ILogger> logger_;

public:
    explicit PerformanceMonitorObserver(std::shared_ptr<ILogger> logger)
        : logger_(std::move(logger)) {
        stats_.startTime = std::chrono::system_clock::now();
    }

    void onRadarEvent(const std::shared_ptr<RadarSystemEvent>& event) override {
        if (event->getType() == RadarSystemEvent::Type::ProcessingCompleted) {
            auto processingEvent = std::dynamic_pointer_cast<ProcessingCompletedEvent>(event);
            if (processingEvent) {
                updatePerformanceStatistics(*processingEvent);
            }
        }
    }

    std::string getObserverName() const override {
        return "Performance Monitor";
    }

    std::vector<RadarSystemEvent::Type> getInterestedEventTypes() const override {
        return {RadarSystemEvent::Type::ProcessingCompleted};
    }

    PerformanceStatistics getStatistics() const {
        std::lock_guard<std::mutex> lock(statsMutex_);
        return stats_;
    }

private:
    void updatePerformanceStatistics(const ProcessingCompletedEvent& event) {
        std::lock_guard<std::mutex> lock(statsMutex_);

        double processingTimeMs = event.getProcessingTime().count() / 1000.0;

        // 更新平均处理时间
        stats_.averageProcessingTime =
            (stats_.averageProcessingTime * stats_.totalProcessedPackets + processingTimeMs) /
            (stats_.totalProcessedPackets + 1);

        // 更新最大处理时间
        stats_.maxProcessingTime = std::max(stats_.maxProcessingTime, processingTimeMs);

        // 更新吞吐量
        stats_.totalThroughput += static_cast<double>(event.getDataSize()) / 1024.0 / 1024.0; // MB

        stats_.totalProcessedPackets++;

        // 记录性能日志
        if (stats_.totalProcessedPackets % 1000 == 0) {
            logger_->info("Performance Update: Avg=" + std::to_string(stats_.averageProcessingTime) +
                         "ms, Max=" + std::to_string(stats_.maxProcessingTime) +
                         "ms, Packets=" + std::to_string(stats_.totalProcessedPackets));
        }
    }
};
```

#### 事件发布器
```cpp
/**
 * @brief 雷达系统事件发布器
 *
 * 管理观察者的注册/注销，负责事件的分发和过滤。
 * 支持异步事件处理和优先级队列。
 */
class RadarEventPublisher {
private:
    struct ObserverInfo {
        std::weak_ptr<IRadarEventObserver> observer;
        std::vector<RadarSystemEvent::Type> interestedTypes;
        RadarSystemEvent::Priority minimumPriority;
        std::string name;
    };

    std::vector<ObserverInfo> observers_;
    std::mutex observersMutex_;

    // 异步事件处理
    std::queue<std::shared_ptr<RadarSystemEvent>> eventQueue_;
    std::mutex queueMutex_;
    std::condition_variable queueCondition_;
    std::thread eventProcessingThread_;
    std::atomic<bool> shouldStop_;

public:
    RadarEventPublisher() : shouldStop_(false) {
        eventProcessingThread_ = std::thread(&RadarEventPublisher::processEvents, this);
    }

    ~RadarEventPublisher() {
        shouldStop_ = true;
        queueCondition_.notify_all();
        if (eventProcessingThread_.joinable()) {
            eventProcessingThread_.join();
        }
    }

    /**
     * @brief 注册事件观察者
     */
    void registerObserver(std::shared_ptr<IRadarEventObserver> observer) {
        std::lock_guard<std::mutex> lock(observersMutex_);

        ObserverInfo info;
        info.observer = observer;
        info.interestedTypes = observer->getInterestedEventTypes();
        info.minimumPriority = observer->getMinimumPriority();
        info.name = observer->getObserverName();

        observers_.push_back(info);

        LOG_INFO("Registered observer: " + info.name);
    }

    /**
     * @brief 注销事件观察者
     */
    void unregisterObserver(std::shared_ptr<IRadarEventObserver> observer) {
        std::lock_guard<std::mutex> lock(observersMutex_);

        auto it = std::remove_if(observers_.begin(), observers_.end(),
            [&observer](const ObserverInfo& info) {
                return info.observer.lock() == observer;
            });

        if (it != observers_.end()) {
            LOG_INFO("Unregistered observer: " + it->name);
            observers_.erase(it, observers_.end());
        }
    }

    /**
     * @brief 发布事件（异步）
     */
    void publishEvent(std::shared_ptr<RadarSystemEvent> event) {
        {
            std::lock_guard<std::mutex> lock(queueMutex_);
            eventQueue_.push(event);
        }
        queueCondition_.notify_one();
    }

    /**
     * @brief 发布事件（同步）
     */
    void publishEventSync(std::shared_ptr<RadarSystemEvent> event) {
        std::lock_guard<std::mutex> lock(observersMutex_);

        for (auto it = observers_.begin(); it != observers_.end();) {
            auto observer = it->observer.lock();
            if (!observer) {
                // 观察者已被销毁，从列表中移除
                it = observers_.erase(it);
                continue;
            }

            // 检查事件类型过滤
            if (!it->interestedTypes.empty()) {
                bool typeMatches = std::find(it->interestedTypes.begin(),
                                           it->interestedTypes.end(),
                                           event->getType()) != it->interestedTypes.end();
                if (!typeMatches) {
                    ++it;
                    continue;
                }
            }

            // 检查优先级过滤
            if (event->getPriority() < it->minimumPriority) {
                ++it;
                continue;
            }

            // 通知观察者
            try {
                observer->onRadarEvent(event);
            } catch (const std::exception& e) {
                LOG_ERROR("Observer " + it->name + " threw exception: " + e.what());
            }

            // 检查事件传播是否被停止
            if (event->isPropagationStopped()) {
                break;
            }

            ++it;
        }
    }

private:
    /**
     * @brief 异步事件处理线程
     */
    void processEvents() {
        while (!shouldStop_) {
            std::unique_lock<std::mutex> lock(queueMutex_);
            queueCondition_.wait(lock, [this] {
                return !eventQueue_.empty() || shouldStop_;
            });

            while (!eventQueue_.empty() && !shouldStop_) {
                auto event = eventQueue_.front();
                eventQueue_.pop();
                lock.unlock();

                publishEventSync(event);

                lock.lock();
            }
        }
    }
};
```

---

## 资源管理模式

### 💎 RAII模式 (Resource Acquisition Is Initialization)

#### GPU资源管理
```cpp
/**
 * @brief CUDA设备内存RAII包装器
 *
 * 自动管理GPU内存的分配和释放，确保异常安全。
 * 支持内存对齐、零拷贝等高级特性。
 */
template<typename T>
class CUDADeviceMemory {
private:
    T* devicePtr_;
    size_t sizeInElements_;
    size_t alignment_;
    bool isOwner_;

public:
    /**
     * @brief 分配设备内存构造函数
     * @param count 元素数量
     * @param alignment 内存对齐字节数（默认256字节，适合GPU）
     */
    explicit CUDADeviceMemory(size_t count, size_t alignment = 256)
        : devicePtr_(nullptr), sizeInElements_(count),
          alignment_(alignment), isOwner_(true) {

        size_t sizeInBytes = count * sizeof(T);

        // 计算对齐后的大小
        size_t alignedSize = ((sizeInBytes + alignment - 1) / alignment) * alignment;

        cudaError_t result = cudaMalloc(reinterpret_cast<void**>(&devicePtr_), alignedSize);
        if (result != cudaSuccess) {
            throw std::runtime_error("Failed to allocate CUDA device memory: " +
                                   std::string(cudaGetErrorString(result)));
        }

        // 初始化内存为零（可选）
        cudaMemset(devicePtr_, 0, alignedSize);
    }

    /**
     * @brief 从现有指针构造（非拥有）
     * @param ptr 现有设备指针
     * @param count 元素数量
     */
    CUDADeviceMemory(T* ptr, size_t count)
        : devicePtr_(ptr), sizeInElements_(count),
          alignment_(0), isOwner_(false) {}

    // 禁用拷贝构造和赋值
    CUDADeviceMemory(const CUDADeviceMemory&) = delete;
    CUDADeviceMemory& operator=(const CUDADeviceMemory&) = delete;

    // 支持移动语义
    CUDADeviceMemory(CUDADeviceMemory&& other) noexcept
        : devicePtr_(other.devicePtr_), sizeInElements_(other.sizeInElements_),
          alignment_(other.alignment_), isOwner_(other.isOwner_) {
        other.devicePtr_ = nullptr;
        other.isOwner_ = false;
    }

    CUDADeviceMemory& operator=(CUDADeviceMemory&& other) noexcept {
        if (this != &other) {
            reset();
            devicePtr_ = other.devicePtr_;
            sizeInElements_ = other.sizeInElements_;
            alignment_ = other.alignment_;
            isOwner_ = other.isOwner_;
            other.devicePtr_ = nullptr;
            other.isOwner_ = false;
        }
        return *this;
    }

    /**
     * @brief 析构函数，自动释放内存
     */
    ~CUDADeviceMemory() {
        reset();
    }

    /**
     * @brief 获取设备指针
     */
    T* get() const noexcept { return devicePtr_; }

    /**
     * @brief 获取元素数量
     */
    size_t size() const noexcept { return sizeInElements_; }

    /**
     * @brief 获取字节大小
     */
    size_t sizeInBytes() const noexcept { return sizeInElements_ * sizeof(T); }

    /**
     * @brief 检查是否有效
     */
    bool isValid() const noexcept { return devicePtr_ != nullptr; }

    /**
     * @brief 从主机拷贝数据到设备
     */
    void copyFromHost(const T* hostPtr, size_t count = 0, cudaStream_t stream = 0) {
        if (!hostPtr || !devicePtr_) {
            throw std::invalid_argument("Invalid pointer for memory copy");
        }

        size_t copyCount = (count == 0) ? sizeInElements_ : std::min(count, sizeInElements_);
        size_t copySize = copyCount * sizeof(T);

        cudaError_t result;
        if (stream == 0) {
            result = cudaMemcpy(devicePtr_, hostPtr, copySize, cudaMemcpyHostToDevice);
        } else {
            result = cudaMemcpyAsync(devicePtr_, hostPtr, copySize,
                                   cudaMemcpyHostToDevice, stream);
        }

        if (result != cudaSuccess) {
            throw std::runtime_error("Failed to copy from host to device: " +
                                   std::string(cudaGetErrorString(result)));
        }
    }

    /**
     * @brief 从设备拷贝数据到主机
     */
    void copyToHost(T* hostPtr, size_t count = 0, cudaStream_t stream = 0) const {
        if (!hostPtr || !devicePtr_) {
            throw std::invalid_argument("Invalid pointer for memory copy");
        }

        size_t copyCount = (count == 0) ? sizeInElements_ : std::min(count, sizeInElements_);
        size_t copySize = copyCount * sizeof(T);

        cudaError_t result;
        if (stream == 0) {
            result = cudaMemcpy(hostPtr, devicePtr_, copySize, cudaMemcpyDeviceToHost);
        } else {
            result = cudaMemcpyAsync(hostPtr, devicePtr_, copySize,
                                   cudaMemcpyDeviceToHost, stream);
        }

        if (result != cudaSuccess) {
            throw std::runtime_error("Failed to copy from device to host: " +
                                   std::string(cudaGetErrorString(result)));
        }
    }

    /**
     * @brief 手动释放资源
     */
    void reset() {
        if (devicePtr_ && isOwner_) {
            cudaFree(devicePtr_);
        }
        devicePtr_ = nullptr;
        isOwner_ = false;
    }
};

/**
 * @brief CUDA流RAII包装器
 */
class CUDAStream {
private:
    cudaStream_t stream_;
    bool isOwner_;

public:
    /**
     * @brief 创建新的CUDA流
     */
    explicit CUDAStream(unsigned int flags = cudaStreamDefault)
        : stream_(nullptr), isOwner_(true) {
        cudaError_t result = cudaStreamCreateWithFlags(&stream_, flags);
        if (result != cudaSuccess) {
            throw std::runtime_error("Failed to create CUDA stream: " +
                                   std::string(cudaGetErrorString(result)));
        }
    }

    /**
     * @brief 从现有流构造（非拥有）
     */
    explicit CUDAStream(cudaStream_t stream)
        : stream_(stream), isOwner_(false) {}

    // 禁用拷贝
    CUDAStream(const CUDAStream&) = delete;
    CUDAStream& operator=(const CUDAStream&) = delete;

    // 支持移动
    CUDAStream(CUDAStream&& other) noexcept
        : stream_(other.stream_), isOwner_(other.isOwner_) {
        other.stream_ = nullptr;
        other.isOwner_ = false;
    }

    /**
     * @brief 析构函数，自动销毁流
     */
    ~CUDAStream() {
        if (stream_ && isOwner_) {
            cudaStreamDestroy(stream_);
        }
    }

    /**
     * @brief 获取原生流句柄
     */
    cudaStream_t get() const noexcept { return stream_; }

    /**
     * @brief 等待流中所有操作完成
     */
    void synchronize() const {
        cudaError_t result = cudaStreamSynchronize(stream_);
        if (result != cudaSuccess) {
            throw std::runtime_error("Failed to synchronize CUDA stream: " +
                                   std::string(cudaGetErrorString(result)));
        }
    }

    /**
     * @brief 检查流是否空闲
     */
    bool isIdle() const {
        cudaError_t result = cudaStreamQuery(stream_);
        return (result == cudaSuccess);
    }
};
```

#### 资源池模式
```cpp
/**
 * @brief 通用资源池模板
 *
 * 管理昂贵资源的创建、重用和销毁。支持自动扩容、
 * 资源健康检查和统计信息收集。
 */
template<typename ResourceType, typename ResourceKey = std::string>
class ResourcePool {
public:
    using ResourcePtr = std::shared_ptr<ResourceType>;
    using ResourceFactory = std::function<ResourcePtr(const ResourceKey&)>;
    using ResourceValidator = std::function<bool(const ResourcePtr&)>;

private:
    struct PooledResource {
        ResourcePtr resource;
        std::chrono::steady_clock::time_point lastUsed;
        std::chrono::steady_clock::time_point created;
        size_t useCount;
        bool inUse;

        PooledResource(ResourcePtr res)
            : resource(std::move(res)),
              lastUsed(std::chrono::steady_clock::now()),
              created(std::chrono::steady_clock::now()),
              useCount(0), inUse(false) {}
    };

    mutable std::mutex poolMutex_;
    std::map<ResourceKey, std::vector<std::unique_ptr<PooledResource>>> pool_;
    ResourceFactory factory_;
    ResourceValidator validator_;

    // 配置参数
    size_t maxPoolSize_;
    size_t maxIdleCount_;
    std::chrono::minutes maxIdleTime_;
    std::chrono::minutes maxResourceAge_;

    // 统计信息
    mutable std::atomic<size_t> totalRequests_{0};
    mutable std::atomic<size_t> cacheHits_{0};
    mutable std::atomic<size_t> cacheMisses_{0};

    // 清理线程
    std::thread cleanupThread_;
    std::atomic<bool> shouldStop_{false};

public:
    /**
     * @brief 构造资源池
     */
    ResourcePool(
        ResourceFactory factory,
        ResourceValidator validator = nullptr,
        size_t maxPoolSize = 100,
        size_t maxIdleCount = 10,
        std::chrono::minutes maxIdleTime = std::chrono::minutes(30),
        std::chrono::minutes maxResourceAge = std::chrono::hours(24)
    ) : factory_(std::move(factory)), validator_(std::move(validator)),
        maxPoolSize_(maxPoolSize), maxIdleCount_(maxIdleCount),
        maxIdleTime_(maxIdleTime), maxResourceAge_(maxResourceAge) {

        // 启动清理线程
        cleanupThread_ = std::thread(&ResourcePool::cleanupLoop, this);
    }

    /**
     * @brief 析构函数
     */
    ~ResourcePool() {
        shouldStop_ = true;
        if (cleanupThread_.joinable()) {
            cleanupThread_.join();
        }
    }

    /**
     * @brief 获取资源
     */
    ResourcePtr acquire(const ResourceKey& key) {
        totalRequests_++;

        std::lock_guard<std::mutex> lock(poolMutex_);

        auto& resources = pool_[key];

        // 查找可用资源
        for (auto& pooledRes : resources) {
            if (!pooledRes->inUse) {
                // 验证资源是否仍然有效
                if (validator_ && !validator_(pooledRes->resource)) {
                    continue;
                }

                // 检查资源年龄
                auto now = std::chrono::steady_clock::now();
                if (now - pooledRes->created > maxResourceAge_) {
                    continue;
                }

                // 标记为使用中
                pooledRes->inUse = true;
                pooledRes->lastUsed = now;
                pooledRes->useCount++;

                cacheHits_++;
                return pooledRes->resource;
            }
        }

        // 没有可用资源，创建新资源
        if (resources.size() < maxPoolSize_) {
            auto newResource = factory_(key);
            if (newResource) {
                auto pooledRes = std::make_unique<PooledResource>(newResource);
                pooledRes->inUse = true;
                pooledRes->useCount = 1;

                auto result = pooledRes->resource;
                resources.push_back(std::move(pooledRes));

                cacheMisses_++;
                return result;
            }
        }

        cacheMisses_++;
        throw std::runtime_error("Failed to acquire resource from pool");
    }

    /**
     * @brief 释放资源
     */
    void release(const ResourceKey& key, const ResourcePtr& resource) {
        std::lock_guard<std::mutex> lock(poolMutex_);

        auto it = pool_.find(key);
        if (it != pool_.end()) {
            for (auto& pooledRes : it->second) {
                if (pooledRes->resource == resource && pooledRes->inUse) {
                    pooledRes->inUse = false;
                    pooledRes->lastUsed = std::chrono::steady_clock::now();
                    return;
                }
            }
        }
    }

    /**
     * @brief 获取池统计信息
     */
    struct PoolStatistics {
        size_t totalRequests;
        size_t cacheHits;
        size_t cacheMisses;
        double hitRate;
        size_t totalResources;
        size_t activeResources;
        size_t idleResources;
    };

    PoolStatistics getStatistics() const {
        std::lock_guard<std::mutex> lock(poolMutex_);

        PoolStatistics stats;
        stats.totalRequests = totalRequests_;
        stats.cacheHits = cacheHits_;
        stats.cacheMisses = cacheMisses_;
        stats.hitRate = (stats.totalRequests > 0) ?
                       static_cast<double>(stats.cacheHits) / stats.totalRequests : 0.0;

        stats.totalResources = 0;
        stats.activeResources = 0;
        stats.idleResources = 0;

        for (const auto& [key, resources] : pool_) {
            stats.totalResources += resources.size();
            for (const auto& pooledRes : resources) {
                if (pooledRes->inUse) {
                    stats.activeResources++;
                } else {
                    stats.idleResources++;
                }
            }
        }

        return stats;
    }

private:
    /**
     * @brief 清理循环，定期清理过期资源
     */
    void cleanupLoop() {
        while (!shouldStop_) {
            std::this_thread::sleep_for(std::chrono::minutes(5));

            if (shouldStop_) break;

            cleanupExpiredResources();
        }
    }

    /**
     * @brief 清理过期资源
     */
    void cleanupExpiredResources() {
        std::lock_guard<std::mutex> lock(poolMutex_);

        auto now = std::chrono::steady_clock::now();

        for (auto& [key, resources] : pool_) {
            // 计算空闲资源数量
            size_t idleCount = 0;
            for (const auto& pooledRes : resources) {
                if (!pooledRes->inUse) {
                    idleCount++;
                }
            }

            // 移除过期或多余的空闲资源
            resources.erase(
                std::remove_if(resources.begin(), resources.end(),
                    [&](const std::unique_ptr<PooledResource>& pooledRes) {
                        if (pooledRes->inUse) {
                            return false;  // 正在使用的资源不清理
                        }

                        // 检查是否过期
                        bool tooOld = (now - pooledRes->created) > maxResourceAge_;
                        bool idleTooLong = (now - pooledRes->lastUsed) > maxIdleTime_;
                        bool tooManyIdle = idleCount > maxIdleCount_;

                        if (tooManyIdle && idleCount > 0) {
                            idleCount--;
                            return true;
                        }

                        return tooOld || idleTooLong;
                    }),
                resources.end()
            );
        }
    }
};
```

---

## 变更记录

| 版本 | 日期       | 修改人 | 变更摘要                       |
| :--- | :--------- | :----- | :----------------------------- |
| v1.0 | 2025-09-10 | Kelin  | 创建AI协作开发设计模式实践指南 |
