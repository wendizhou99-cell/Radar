# 模块集成策略设计
---

**文档版本**: v3.3.0
**最后更新**: 2025-09-26
**负责人**: Klein

---
## 1 文档职责

### 1.1 文档职责
本文件详细设计雷达数据处理系统的**模块集成策略**，侧重"事件驱动架构下的模块协调与系统级集成"的设计实现：

1) **事件驱动集成架构**：基于统一事件总线的模块协调机制，定义模块间如何通过事件进行解耦通信
2) **标准化接口规范**：遵循`05_接口设计/01_模块接口规范.md`中定义的模块间通信协议，确保模块集成的一致性
3) **依赖注入与服务管理**：基于构造函数注入的模块依赖解析策略，实现服务的松耦合集成
4) **生命周期协调**：事件驱动的模块状态管理和启停协调，确保系统整体的可靠运行
5) **故障恢复机制**：智能化的系统级异常处理和恢复策略，保障系统的稳定性
6) **全链路可观测性**：基于TraceID的端到端监控和追踪，支持系统运行状态的全面观测

**集成策略重点**：
- **模块角色清晰**：明确区分主动执行引擎、被动服务和UI模块的不同集成方式
- **通信机制统一**：通过EventBus实现异步解耦通信，通过构造函数注入实现服务依赖
- **零拷贝数据流**：基于共享内存池和指针队列的高性能数据传递机制
- **配置驱动集成**：通过ConfigManager统一管理模块配置，支持运行时热更新

**设计原则对齐**：本设计严格遵循项目统一的架构原则，与各模块设计保持一致：
- **事件驱动架构**：完全基于异步事件，避免轮询和同步调用
- **ExecutionEngine模式**：统一的执行引擎接口和组件命名
- **依赖注入**：通过服务提供者模式实现松耦合
- **接口驱动开发**：所有交互基于标准接口，支持策略模式
- **接口隔离原则 (ISP)**：**【核心原则】** 将传统的大而全IModule接口拆分为基于角色的小接口，模块根据需求灵活组合实现，避免接口污染和实现负担

已明确不在本文件范围内的内容：单个模块的内部实现细节、具体算法策略、UI界面设计、外部系统集成等（这些在相应的专项文档中维护）。

### 1.2 目录
- [模块集成策略设计](#模块集成策略设计)
  - [1 文档职责](#1-文档职责)
    - [1.1 文档职责](#11-文档职责)
    - [1.2 目录](#12-目录)
  - [2 事件驱动集成架构](#2-事件驱动集成架构)
    - [2.1 整体架构设计](#21-整体架构设计)
      - [2.1.1 分布式系统宏观架构](#211-分布式系统宏观架构)
      - [2.1.2 核心处理服务器内部分层架构](#212-核心处理服务器内部分层架构)
    - [2.2 事件总线设计](#22-事件总线设计)
      - [2.2.1 事件流架构图](#221-事件流架构图)
      - [2.2.2 数据流架构图](#222-数据流架构图)
      - [2.2.3 EventBus实现约束与线程模型](#223-eventbus实现约束与线程模型)
    - [2.3 ExecutionEngine协调机制](#23-executionengine协调机制)
    - [2.4 服务依赖注入模式](#24-服务依赖注入模式)
    - [2.5 模块交互模式](#25-模块交互模式)
      - [2.6 模块调度协调机制](#26-模块调度协调机制)
  - [3 标准化接口规范](#3-标准化接口规范)
    - [3.1 IModule基础接口](#31-imodule基础接口)
      - [3.1.1 接口隔离原则 (Interface Segregation Principle)](#311-接口隔离原则-interface-segregation-principle)
      - [3.1.2 基于角色的接口体系](#312-基于角色的接口体系)
      - [3.1.3 模块实现示例 - 接口组合的灵活应用](#313-模块实现示例---接口组合的灵活应用)
      - [3.1.4 接口选择指导原则](#314-接口选择指导原则)
    - [3.2 ExecutionEngine接口](#32-executionengine接口)
    - [3.3 事件接口规范](#33-事件接口规范)
    - [3.4 数据交换格式](#34-数据交换格式)
    - [3.5 通信协议](#35-通信协议)
  - [4 生命周期协调策略](#4-生命周期协调策略)
    - [4.1 事件驱动启动流程](#41-事件驱动启动流程)
      - [4.1.1 优雅关闭与超时处理机制](#411-优雅关闭与超时处理机制)
    - [4.2 故障恢复机制](#42-故障恢复机制)
      - [4.2.1 熔断器模式 - 防恢复风暴保护机制](#421-熔断器模式---防恢复风暴保护机制)
      - [4.2.2 广播风暴防护机制](#422-广播风暴防护机制)
      - [4.2.3 双重依赖类型定义](#423-双重依赖类型定义)
      - [4.2.4 依赖管理系统实现](#424-依赖管理系统实现)
    - [4.3 配置热更新协调](#43-配置热更新协调)
      - [4.3.1 配置原子性应用机制](#431-配置原子性应用机制)
  - [5 集成测试方案](#5-集成测试方案)
    - [5.1 测试策略](#51-测试策略)
      - [测试金字塔结构](#测试金字塔结构)
    - [5.2 测试环境](#52-测试环境)
      - [测试环境组成](#测试环境组成)
    - [5.3 测试用例](#53-测试用例)
      - [核心测试场景](#核心测试场景)
  - [6 模块协作机制](#6-模块协作机制)
    - [6.1 数据协作](#61-数据协作)
    - [6.2 控制协作](#62-控制协作)
    - [6.3 事件协作](#63-事件协作)
  - [7 端到端关键场景设计](#7-端到端关键场景设计)
      - [7.1 "Happy Path" 数据处理流程](#71-happy-path-数据处理流程)
    - [7.2 配置热更新完整流程](#72-配置热更新完整流程)
    - [7.3 故障恢复协调流程](#73-故障恢复协调流程)
    - [7.4 GPU资源协调流程](#74-gpu资源协调流程)
    - [7.5 序列图使用指南](#75-序列图使用指南)
      - [7.5.1 Trace ID追踪指南](#751-trace-id追踪指南)
      - [7.5.2 扩展与定制指南](#752-扩展与定制指南)
  - [8 部署与发布](#8-部署与发布)
    - [8.1 部署架构](#81-部署架构)
      - [部署层次架构](#部署层次架构)
    - [8.2 发布流程](#82-发布流程)
    - [8.3 回滚策略](#83-回滚策略)
      - [回滚策略体系](#回滚策略体系)
      - [回滚机制](#回滚机制)
      - [验证步骤](#验证步骤)
  - [9 模块约束说明](#9-模块约束说明)
  - [10 相关文档](#10-相关文档)
  - [11 变更历史](#11-变更历史)

---

## 2 事件驱动集成架构

### 2.1 整体架构设计

系统采用**事件驱动的分布式架构**。此架构在物理上将**核心处理服务器**与**显控终端**分离为两个独立的进程，它们通过标准化的网络协议进行高效、可靠的通信。这种设计为系统的部署、扩展和独立开发提供了最大的灵活性。

#### 2.1.1 分布式系统宏观架构

下图从最高层级描绘了系统的分布式宏观视图，展示了后端服务器、前端客户端以及连接它们的“双通道”网络通信模型。

```mermaid
---
config:
  theme: neo
  flowchart:
    curve: basis
---
graph TD
    subgraph "核心处理服务器 (后端进程)"
        API_Gateway["<b>RESTful API 网关</b><br/>(HTTP/TCP)"]
        EventBus(("EventBus"))
        TaskScheduler["任务调度器"]
        ConfigManager["配置管理器"]
        DataProcessor["DataProcessor"]
        DisplayController["DisplayController<br/>(角色: <b>UDP数据网关</b>)"]
    end

    subgraph "网络通信"
        ControlChannel["<b>双向控制流 (可靠)</b><br/>(RESTful API over HTTP)"]
        DataChannel["<b>单向数据流 (高效)</b><br/>(UDP 单播)"]
    end

    subgraph "显控终端 (前端独立进程)"
        UI_HttpClient["HTTP 客户端"]
        UI_UdpListener["UDP 监听器"]
        UI_Display["显示与交互界面"]
    end

    %% 控制流
    UI_HttpClient -- "命令/配置请求" --> ControlChannel
    ControlChannel -- "HTTP请求" --> API_Gateway
    API_Gateway -- "转换为内部事件" --> EventBus
    EventBus -- "路由给..." --> TaskScheduler & ConfigManager
    TaskScheduler & ConfigManager -- "HTTP响应" --> ControlChannel
    ControlChannel -- "操作结果" --> UI_HttpClient


    %% 数据流
    DataProcessor -- "推送TrackData" --> DisplayController
    DisplayController -- "序列化并通过UDP发送" --> DataChannel
    DataChannel -- "UDP数据包" --> UI_UdpListener
    UI_UdpListener -- "反序列化并更新" --> UI_Display

    classDef server fill:#e6f7ff,stroke:#1890ff,stroke-width:2px
    classDef network fill:#f6ffed,stroke:#52c41a,stroke-width:2px
    classDef client fill:#fffbe6,stroke:#d4b106,stroke-width:2px

    class API_Gateway,EventBus,TaskScheduler,ConfigManager,DataProcessor,DisplayController server
    class ControlChannel,DataChannel network
    class UI_HttpClient,UI_UdpListener,UI_Display client
```

**宏观架构核心特点**:

1.  **物理分离**: 核心处理服务器和显控终端是**两个独立的进程**，可以通过网络部署在不同的物理机器上。
2.  **双通道通信**:
      * **控制流**通过可靠的**RESTful API (HTTP/TCP)** 进行，用于状态查询、命令下发和配置更新。
      * **数据流**通过高效的**UDP单播**进行，用于从服务器向客户端高速推送处理后的航迹数据。
3.  **职责明确**: 服务器负责所有计算密集型任务和硬件交互；显控终端专注于数据可视化和用户交互。

#### 2.1.2 核心处理服务器内部分层架构

下图深入到“核心处理服务器”进程内部，展示了其组件的分层结构和内部的交互关系。

```mermaid
---
config:
  theme: base
  flowchart:
    curve: basis
---
flowchart TB
    subgraph " "
      direction LR
      TASK_ENGINE["<b>TaskScheduler</b><br/>任务调度器"]
      CORE_SERVICES["<b>核心基础服务层</b><br/>(Config & Logging)"]
      EVENTBUS["<b>EventBus</b><br/>事件总线"]
    end
    subgraph Pipeline["<b>数据处理流水线</b>"]
        direction LR
        DATA_RECV["数据接收<br/>DataReceiver"]
        SIGNAL_PROC["信号处理<br/>SignalProcessor"]
        DATA_PROC["数据处理<br/>DataProcessor"]
        GW["数据网关<br/>DisplayController"]
    end
    CLIENT["<b>显控终端</b><br/>(独立进程)"]
    DATA_RECV == "<i>原始数据</i>" ==> SIGNAL_PROC
    SIGNAL_PROC == "<i>检测结果</i>" ==> DATA_PROC
    DATA_PROC == "<i>航迹数据</i>" ==> GW
    GW == "<b>数据流 (UDP)</b>" ==> CLIENT
    TASK_ENGINE -- "<b>生命周期管理</b>" --> Pipeline
    CLIENT -- "<b>控制流 (REST API)</b>" --> TASK_ENGINE
    Pipeline -.-> |"<i>使用</i>"| CORE_SERVICES
    TASK_ENGINE -.-> |"<i>使用</i>"| CORE_SERVICES
    Pipeline <.-> |"<i>发布/订阅</i>"| EVENTBUS
    TASK_ENGINE <.-> |"<i>发布/订阅</i>"| EVENTBUS
    classDef scheduler fill:#e6f7ff,stroke:#1890ff,stroke-width:2px,color:#0050b3
    classDef services fill:#f6ffed,stroke:#52c41a,stroke-width:2px,color:#389e0d
    classDef eventbus fill:#fffbe6,stroke:#d4b106,stroke-width:3px,color:#876800
    classDef pipeline fill:#f0f5ff,stroke:#597ef7,stroke-width:2px,color:#003a8c
    classDef client fill:#fff0f6,stroke:#eb2f96,stroke-width:2px,color:#c41d7f
    class TASK_ENGINE scheduler
    class CORE_SERVICES services
    class EVENTBUS eventbus
    class Pipeline,DATA_RECV,SIGNAL_PROC,DATA_PROC,GW pipeline
    class CLIENT client
```

**内部分层架构说明**:

1.  **任务调度与基础服务层**: 这是服务器的“大脑”和“公共服务部门”。`TaskScheduler`负责管理所有业务模块的生命周期，而`EventBus`, `ConfigManager`, `LoggingService`则为所有模块提供统一的通信、配置和日志服务。
2.  **数据处理流水线层**: 这是服务器的“生产线”。数据从`DataReceiver`进入，依次流经`SignalProcessor`和`DataProcessor`进行加工，最终由`DisplayController`（数据网关）打包发送出去。模块之间通过专用的零拷贝数据缓冲区进行高效的数据传递。
3.  **内部交互模式**:
      * **事件流**: 所有模块的**控制和状态**信息都通过`EventBus`进行异步、解耦的通信。
      * **服务依赖**: 所有模块都通过**依赖注入**的方式获取基础服务，实现了高度的可测试性。

### 2.2 事件总线设计
EventBus作为系统通信的核心中枢，提供统一的事件路由和分发机制。为了清晰展示不同类型的交互，我们将事件流和数据流分别展示：

#### 2.2.1 事件流架构图
展示控制事件在系统中的流动：

```mermaid
---
config:
  theme: neo
  flowchart:
    curve: natural
---
flowchart LR
    subgraph "事件发布者模块"
        direction TB
        CONFIG[ConfigExecutionEngine<br/>配置变更事件]
        TASK[TaskScheduler<br/>生命周期事件]
        MODULE[Business Modules<br/>状态事件]
    end

    subgraph "EventBus核心"
        direction TB
        ROUTER[EventRouter<br/>智能路由]
        FILTER[EventFilter<br/>过滤与优先级]
        QUEUE[AsyncQueue<br/>异步分发]
    end

    subgraph "事件订阅者模块"
        direction TB
        BUSINESS[业务模块<br/>配置响应]
        LOG[日志服务<br/>审计记录]
        MONITOR[监控服务<br/>状态监控]
    end

    CONFIG --配置事件--> ROUTER
    TASK --控制事件--> ROUTER
    MODULE --状态事件--> ROUTER

    ROUTER --过滤--> FILTER
    FILTER --排队--> QUEUE

    QUEUE --异步分发--> BUSINESS
    QUEUE --异步记录--> LOG
    QUEUE --异步监控--> MONITOR

    classDef publisher fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef eventbus fill:#e8f5e8,stroke:#388e3c,stroke-width:3px
    classDef subscriber fill:#fff3e0,stroke:#f57c00,stroke-width:2px

    class CONFIG,TASK,MODULE publisher
    class ROUTER,FILTER,QUEUE eventbus
    class BUSINESS,LOG,MONITOR subscriber
```

#### 2.2.2 数据流架构图
展示数据在处理管道中的流动：

```mermaid
---
config:
  theme: neo
  flowchart:
    curve: natural
---
flowchart LR
    subgraph "数据处理管道"
        direction LR
        RECV[DataReceiver<br/>网络数据接收]
        PROC1[SignalProcessor<br/>信号处理]
        PROC2[DataProcessor<br/>数据分析]
        DISPLAY[DisplayController<br/>结果展示]
    end

    subgraph "共享内存池"
        direction TB
        POOL1[原始数据缓冲区<br/>Raw Data Buffer]
        POOL2[处理结果缓冲区<br/>Processed Buffer]
        POOL3[最终结果缓冲区<br/>Final Results Buffer]
    end

    RECV -.申请内存.-> POOL1
    POOL1 ==指针传递==> PROC1
    PROC1 -.申请内存.-> POOL2
    POOL2 ==指针传递==> PROC2
    PROC2 -.申请内存.-> POOL3
    POOL3 ==指针传递==> DISPLAY

    PROC1 -.归还内存.-> POOL1
    PROC2 -.归还内存.-> POOL2
    DISPLAY -.归还内存.-> POOL3

    classDef processor fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef buffer fill:#e0f2f1,stroke:#00695c,stroke-width:2px

    class RECV,PROC1,PROC2,DISPLAY processor
    class POOL1,POOL2,POOL3 buffer
```
**协作流程说明**：
1.  **变更通知**：`配置管理器`检测到配置变更后，将变更内容和变更等级封装成一个事件，通知`任务调度器`。
2.  **调度器决策与执行**：`任务调度器`是唯一有权改变模块状态的角色。它根据收到的变更等级执行相应操作：
    *   **重载等级**：调用目标模块的`reload()`方法，让模块自行处理更新，期间模块服务不中断。
    *   **重启等级**：执行“停止 -> 重新初始化 -> 启动”的完整生命周期管理，确保模块在全新配置下安全重启。
3.  **状态反馈**：模块执行完操作后，将结果返回给调度器，调度器再将最终状态报告给配置管理器。

这个流程确保了配置更新操作的原子性和安全性，避免了数据不一致和模块状态错乱的风险。

#### 2.2.3 EventBus实现约束与线程模型

为确保系统的一致性和性能，EventBus必须遵循以下实现约束：

**核心实现模型**：
```cpp
/**
 * @brief EventBus实现要求
 * @details 系统对EventBus实现的强制性约束
 */
class EventBus : public IEventBus {
private:
    // 必须：异步事件队列
    ConcurrentQueue<std::shared_ptr<BaseEvent>> event_queue_;

    // 必须：工作线程池（建议2-4个线程）
    ThreadPool worker_threads_;

    // 订阅者管理
    std::unordered_map<std::string, std::vector<EventHandler>> subscribers_;

public:
    /**
     * @brief 发布事件 - 必须非阻塞
     * @details publish()方法必须立即返回，不能等待事件处理完成
     */
    void publish(std::shared_ptr<BaseEvent> event) override {
        // 1. 验证事件有效性
        if (!event || event->trace_id.empty()) {
            logger_->error("Invalid event: missing trace_id");
            return;
        }

        // 2. 非阻塞入队（必须立即返回）
        if (!event_queue_.try_push(event)) {
            logger_->warn("Event queue full, dropping event: {}", event->getEventType());
            // 可选：触发背压处理
        }

        // 3. 立即返回，不等待处理
    }

private:
    /**
     * @brief 工作线程处理循环
     * @details 在工作线程池中执行，负责事件分发
     */
    void workerThreadLoop() {
        while (running_) {
            std::shared_ptr<BaseEvent> event;
            if (event_queue_.wait_and_pop(event)) {
                // 在工作线程中调用订阅者回调
                dispatchEvent(event);
            }
        }
    }

    void dispatchEvent(std::shared_ptr<BaseEvent> event) {
        auto subscribers = getSubscribers(event->getEventType());

        for (auto& handler : subscribers) {
            try {
                // 关键：设置TraceID上下文
                TraceContextGuard guard(event->trace_id);

                // 调用订阅者回调（在EventBus工作线程中）
                handler(event);

            } catch (const std::exception& e) {
                logger_->error("Event handler failed: {}", e.what());
                // 错误隔离：单个订阅者失败不影响其他订阅者
            }
        }
    }
};
```

**强制性约束**：
1. **异步非阻塞**：`publish()`方法必须立即返回，不能等待事件处理完成
2. **工作线程池**：EventBus内部维护独立的工作线程池（建议2-4个线程）
3. **错误隔离**：单个订阅者的异常不能影响其他订阅者接收事件
4. **TraceID传播**：工作线程在调用订阅者回调前必须设置正确的TraceID上下文
5. **背压处理**：当事件队列满时应有合理的处理策略（丢弃、告警等）

**对订阅者的要求**：
- **线程安全**：回调函数将在EventBus工作线程中执行，必须是线程安全的
- **非阻塞操作**：回调函数不能执行长时间阻塞操作（如同步I/O、长时间计算）
- **异常安全**：回调函数必须妥善处理异常，不能让异常逃逸到EventBus
- **耗时操作分离**：如需执行耗时操作，应派发到模块自己的工作线程

```cpp
// 正确的订阅者实现示例
void DataReceiver::onSystemStateChanged(const SystemStateChangedEvent& event) {
    TraceContextGuard guard(event.trace_id); // 已由EventBus设置，此处可选

    // 快速状态检查和决策
    if (event.new_state == SystemState::RECOVERING) {
        // 轻量级操作：设置标志位
        should_pause_processing_ = true;
        logger_->info("Data processing paused due to system recovery");
    } else {
        // 耗时操作：派发到模块自己的线程池
        module_thread_pool_->submit([this, event]() {
            TraceContextGuard thread_guard(event.trace_id);
            resumeProcessing();
        });
    }
}
```

### 2.3 ExecutionEngine协调机制
系统采用统一的ExecutionEngine模式，每个核心模块都包含一个执行引擎来管理内部状态和对外协调：

```mermaid
sequenceDiagram
    participant TaskScheduler as TaskScheduler
    participant ConfigEngine as ConfigExecutionEngine
    participant DataEngine as DataReceiverEngine
    participant EventBus as EventBus

    Note over TaskScheduler, EventBus: 事件驱动的模块协调流程

    TaskScheduler->>EventBus: 1. 发布StartModuleEvent
    EventBus->>DataEngine: 2. 路由事件到目标模块

    DataEngine->>DataEngine: 3. 执行内部start()逻辑
    DataEngine->>EventBus: 4. 发布ModuleStartedEvent

    EventBus->>TaskScheduler: 5. 通知调度器模块已启动

    Note over TaskScheduler, EventBus: 配置变更场景

    ConfigEngine->>EventBus: 6. 发布ConfigChangedEvent
    EventBus->>DataEngine: 7. 通知配置变更

    DataEngine->>DataEngine: 8. 自主决策：重载 or 重启
    DataEngine->>EventBus: 9. 发布处理结果事件
```

**ExecutionEngine协调特点**：
- **事件驱动生命周期**：所有状态变更通过事件触发，确保操作的可追踪性
- **自主决策机制**：各ExecutionEngine根据事件内容自主决策如何响应
- **统一接口规范**：所有ExecutionEngine实现相同的IModule接口，保证一致性
- **状态同步**：通过事件机制自动同步模块状态到TaskScheduler

### 2.4 服务依赖注入模式
系统采用构造函数注入的方式消除模块间的硬依赖，在main函数中显式创建服务实例并注入到各模块：

```mermaid
sequenceDiagram
    participant Main as main函数
    participant ConfigService as ConfigManager
    participant LogService as LoggingService
    participant BusinessModule as 业务模块
    participant TaskScheduler as TaskScheduler

    Note over Main, TaskScheduler: 基于构造函数注入的初始化流程

    Main->>ConfigService: 1. 创建ConfigManager实例
    Main->>LogService: 2. 创建LoggingService实例

    Note over Main, TaskScheduler: 通过构造函数注入依赖

    Main->>BusinessModule: 3. new BusinessModule(std::shared_ptr<ILogger>)
    Main->>TaskScheduler: 4. new TaskScheduler(std::shared_ptr<ILogger>, std::shared_ptr<IConfigManager>)

    Note over BusinessModule, LogService: 运行时直接使用注入的服务

    BusinessModule->>LogService: 5. logger_->info("模块运行正常")
    TaskScheduler->>ConfigService: 6. config_manager_->getInitialConfigFor("business_module")
```

**构造函数注入示例**：
```cpp
// 典型的业务模块构造函数
class DataReceiver : public IModule {
private:
    std::shared_ptr<ILogger> logger_;
    std::shared_ptr<IEventBus> event_bus_;

public:
    DataReceiver(std::shared_ptr<ILogger> logger,
                 std::shared_ptr<IEventBus> event_bus)
        : logger_(logger), event_bus_(event_bus) {}

    ErrorCode initialize() override {
        logger_->info("DataReceiver initializing...");
        // 初始化逻辑
        return ErrorCode::SUCCESS;
    }
};

// main函数中解决循环依赖的正确启动顺序
int main() {
    // 1. 创建并初始化核心基础服务 (不由TaskScheduler管理)
    auto config_manager = std::make_shared<ConfigManager>();
    config_manager->initialize("config.yaml"); // 直接加载配置文件

    auto logging_service = std::make_shared<LoggingService>(config_manager);
    logging_service->initialize(); // 初始化日志系统

    auto event_bus = std::make_shared<EventBus>(logging_service);
    event_bus->initialize(); // 初始化事件总线

    // 2. 创建TaskScheduler (注入已就绪的基础服务)
    auto task_scheduler = std::make_shared<TaskScheduler>(
        config_manager, event_bus, logging_service);
    task_scheduler->initialize();

    // 3. 创建并注册所有真正的"业务模块" (注入基础服务)
    auto data_receiver = std::make_shared<DataReceiver>(
        logging_service, event_bus, config_manager);
    auto signal_processor = std::make_shared<SignalProcessor>(
        logging_service, event_bus, config_manager);

    // 4. 将业务模块注册到TaskScheduler进行生命周期管理
    task_scheduler->registerModule(data_receiver);
    task_scheduler->registerModule(signal_processor);

    // 5. 启动系统 (TaskScheduler根据依赖图启动所有业务模块)
    task_scheduler->start();

    // 6. 运行主循环...
    // 7. 优雅关闭
    task_scheduler->stop();
    return 0;
}
```

**构造函数注入优势**：
- **依赖关系显式**：在编译时就明确了模块的依赖关系
- **无运行时查找**：避免了服务定位器模式的运行时开销
- **测试友好**：可以轻松注入Mock服务进行单元测试
- **简单可靠**：无需额外的服务注册表或依赖注入框架
- **类型安全**：编译期即可发现依赖类型错误

### 2.5 模块交互模式
模块间采用多种交互模式，满足不同场景的协作需求：

| 交互类型     | 具体模式         | 应用场景         | 特点                     | 实现方式            |
| ------------ | ---------------- | ---------------- | ------------------------ | ------------------- |
| **数据交互** | 生产消费模式     | 数据流水线传递   | 高效、解耦               | 共享内存池+指针队列 |
|              | 发布订阅模式     | 数据广播分发     | 灵活、多对多             | 事件总线机制        |
| **控制交互** | 命令控制模式     | 模块启停管理     | 统一、可靠               | TaskScheduler协调   |
|              | 状态同步模式     | 状态信息同步     | 实时、一致               | 事件驱动状态机      |
| **事件交互** | 事件通知模式     | 异步事件处理     | 松耦合、响应式           | EventBus异步分发    |
|              | 回调响应模式     | 操作完成通知     | 及时、准确               | 事件回调机制        |
| **服务交互** | **构造函数注入** | **基础服务访问** | **编译期依赖、类型安全** | **直接接口注入**    |

**交互模式详细说明**：

1. **数据交互 - 统一"推"模式**：为避免"推拉"混用导致的责任不清和竞态条件，系统统一采用"推"模式：
   ```cpp
   // 统一的数据队列接口
   template<typename T>
   class IDataQueue {
   public:
       virtual ErrorCode push(std::shared_ptr<T> data) = 0;
       virtual bool isFull() const = 0;
       virtual size_t size() const = 0;
   };

   // 模块构造时注入下游队列
   class DataReceiver : public IModule {
   private:
       std::shared_ptr<IDataQueue<RawDataPacket>> downstream_queue_;
   public:
       DataReceiver(std::shared_ptr<IDataQueue<RawDataPacket>> downstream)
           : downstream_queue_(downstream) {}

       void processData() {
           auto processed_data = processRawData();
           downstream_queue_->push(processed_data); // 统一推送
       }
   };
   ```
   **优势**：数据流向清晰、责任单一、背压机制直观、缓冲区所有权明确

2. **事件交互**：所有模块通过统一的EventBus进行异步事件通信，支持事件过滤、优先级管理和TraceID传播。

3. **服务交互**：基础服务（如LoggingService、ConfigManager）通过构造函数注入的方式提供给业务模块，在main函数中显式创建服务实例并注入，无需运行时服务查找。

4. **缓冲区所有权管理**：所有模块间的共享数据队列由main函数或专门的ConnectionManager创建，通过依赖注入分别传给生产者和消费者，避免所有权模糊。

#### 2.6 模块调度协调机制

任务调度器与数据驱动模块的协调遵循明确的模式，以平衡控制和性能。

```mermaid
sequenceDiagram
    participant Scheduler as 任务调度器
    participant DataDrivenModule as 数据驱动模块<br/>(如: DataReceiver, SignalProcessor, DisplayController)
    participant EventSource as 数据/事件源<br/>(如: 网络, 上游队列)

    Note over Scheduler, EventSource: 模块启动与自主运行流程 (Autonomous Mode)

    Scheduler->>DataDrivenModule: 1. initialize(config)
    DataDrivenModule-->>Scheduler: 2. 初始化完成

    Scheduler->>DataDrivenModule: 3. start()
    activate DataDrivenModule
    DataDrivenModule->>DataDrivenModule: 4. 创建内部线程并进入<br/>ExecutionEngine主循环
    DataDrivenModule-->>Scheduler: 5. start()方法立即返回

    Note over DataDrivenModule, EventSource: 模块进入自主运行状态

    loop 数据驱动的内部循环
        EventSource->>DataDrivenModule: 6. 新数据到达
        DataDrivenModule->>DataDrivenModule: 7. 处理数据
    end

    Note over Scheduler, EventSource: 调度器进行生命周期管理

    Scheduler->>DataDrivenModule: 9. stop()
    DataDrivenModule->>DataDrivenModule: 10. 退出内部循环，清理资源
    deactivate DataDrivenModule
    DataDrivenModule-->>Scheduler: 11. 停止完成
```

**协调机制说明**：
对于**所有**核心的、数据驱动的模块（**包括**`DataReceiver`, `SignalProcessor`, `DataProcessor`以及新的`DisplayController`数据网关），任务调度器采用“**启动后放手 (Fire-and-Forget)**”的策略。

1.  **启动阶段**：调度器调用模块的`start()`方法。
2.  **自主运行**：模块内部会自行创建一个或多个线程，并进入一个由其`ExecutionEngine`驱动的、由外部事件（如网络数据到达）或内部状态（如缓冲区非空）驱动的循环。`start()`方法会立即返回，告知调度器它已开始独立运行。
3.  **生命周期管理**：调度器不再干预模块的单次运行，只在需要时通过`stop()`或`pause()`等方法来管理其生命周期。这种模式最大化了数据驱动模块的响应性能和独立性，符合系统的高性能要求。


---
## 3 标准化接口规范

> **接口定义参考**: 本章节中提及的所有核心接口（`ILifecycleManaged`, `IPausable`, `IMonitorable`, `IDependencyManaged`, `IModule`等）的详细定义，请参阅 **[`05_接口设计/01_模块接口规范.md`](../05_接口设计/01_模块接口规范.md)**。该文档是所有模块接口的权威定义和唯一真实来源（Single Source of Truth）。

本章节重点阐述这些接口在模块集成过程中的**应用模式**和**最佳实践**。

### 3.1 IModule基础接口

#### 3.1.1 接口隔离原则 (Interface Segregation Principle)

为避免传统的"大而全"接口设计问题，系统严格遵循**接口隔离原则 (ISP)**，将原本庞大的 IModule 接口拆分为多个职责单一的基于角色的接口。这种设计使得：

**设计收益**：
- **接口职责更单一**：每个接口专注于单一职责领域，职责边界清晰
- **模块实现更灵活**：模块只需实现所需的接口，避免强制实现不必要的功能
- **代码意图更清晰**：通过接口组合明确表达模块的能力和职责
- **测试和维护更简单**：可以独立测试和维护每个接口的实现

**问题解决**：
- ❌ **避免接口污染**：纯计算模块无需强制实现 `pause/resume` 功能
- ❌ **避免依赖膨胀**：服务模块无需依赖复杂的监控接口
- ❌ **避免实现负担**：简单模块无需提供复杂的依赖管理功能

#### 3.1.2 基于角色的接口体系

```cpp
/**
 * @brief 生命周期管理接口 - 应用接口分离原则(ISP)
 * @details 所有由TaskScheduler管理的模块都实现此接口
 */
class ILifecycleManaged {
public:
    virtual ~ILifecycleManaged() = default;

    /**
     * @brief 初始化模块
     * @return 操作结果错误码
     */
    virtual ErrorCode initialize() = 0;

    /**
     * @brief 启动模块运行
     * @return 操作结果错误码
     */
    virtual ErrorCode start() = 0;

    /**
     * @brief 停止模块运行
     * @return 操作结果错误码
     */
    virtual ErrorCode stop() = 0;

    /**
     * @brief 清理模块资源
     * @return 操作结果错误码
     */
    virtual ErrorCode cleanup() = 0;

    /**
     * @brief 获取模块当前状态
     * @return 模块状态枚举
     */
    virtual ModuleState getState() const = 0;

    /**
     * @brief 获取模块名称
     * @return 模块名称字符串
     */
    virtual const std::string& getModuleName() const = 0;
};

/**
 * @brief 可暂停接口 - 只有支持暂停/恢复的模块才实现
 * @details 数据处理流模块等需要暂停恢复功能的模块实现此接口
 */
class IPausable {
public:
    virtual ~IPausable() = default;

    /**
     * @brief 暂停模块运行
     * @return 操作结果错误码
     */
    virtual ErrorCode pause() = 0;

    /**
     * @brief 恢复模块运行
     * @return 操作结果错误码
     */
    virtual ErrorCode resume() = 0;

    /**
     * @brief 检查是否处于暂停状态
     * @return true if paused
     */
    virtual bool isPaused() const = 0;
};

/**
 * @brief 可监控接口 - 需要健康状态监控的模块实现
 * @details 业务模块实现此接口提供健康状态，不再主动查询性能指标
 */
class IMonitorable {
public:
    virtual ~IMonitorable() = default;

    /**
     * @brief 获取模块健康状态
     * @return 健康状态信息
     */
    virtual HealthStatus getHealthStatus() const = 0;

    /**
     * @brief 设置健康状态变更回调
     * @param callback 状态变更回调函数
     */
    virtual void setHealthStatusCallback(HealthStatusCallback callback) = 0;
};

/**
 * @brief 模块依赖管理接口
 * @details 用于定义模块间的依赖关系，支持依赖注入和启动顺序管理
 */
class IDependencyManaged {
public:
    virtual ~IDependencyManaged() = default;

    /**
     * @brief 声明模块依赖
     * @param dependency_name 依赖模块名称
     * @param type 依赖类型（强依赖/弱依赖）
     * @param optional 是否为可选依赖
     */
    virtual void declareDependency(const std::string& dependency_name,
                                   DependencyType type,
                                   bool optional = false) = 0;

    /**
     * @brief 注入依赖模块
     * @param dependency_name 依赖名称
     * @param dependency 依赖模块实例
     * @return ErrorCode 注入结果
     */
    virtual ErrorCode injectDependency(const std::string& dependency_name,
                                       std::shared_ptr<IModule> dependency) = 0;

    /**
     * @brief 检查依赖是否满足
     * @return ErrorCode 检查结果，SUCCESS表示所有必需依赖已满足
     */
    virtual ErrorCode validateDependencies() const = 0;

    /**
     * @brief 获取模块依赖列表
     * @return 依赖关系映射表
     */
    virtual std::map<std::string, ModuleDependency> getDependencies() const = 0;
};

/**
 * @brief 标准模块接口 - 组合基础接口
 * @details 大多数业务模块实现此接口，组合了生命周期管理、暂停恢复、监控、依赖管理等功能
 */
class IModule : public ILifecycleManaged, public IPausable, public IMonitorable, public IDependencyManaged {
public:
    virtual ~IModule() = default;

    // 继承所有基础接口的方法
    // 模块可以选择性实现需要的功能
};
```

#### 3.1.3 模块实现示例 - 接口组合的灵活应用

基于接口隔离原则，不同类型的模块可以根据自身需求选择性地组合实现接口：

**完整功能业务模块 (如数据接收模块)**：
```cpp
// 需要完整生命周期管理、暂停恢复、监控和依赖管理功能
class DataReceiver : public ILifecycleManaged,
                    public IPausable,
                    public IMonitorable,
                    public IDependencyManaged {
private:
    std::string module_name_ = "DataReceiver";
    ModuleState state_ = ModuleState::UNINITIALIZED;
    bool paused_ = false;

public:
    // ILifecycleManaged 实现
    ErrorCode initialize() override {
        // 初始化网络接收组件
        return SystemErrors::SUCCESS;
    }

    ErrorCode start() override {
        // 启动I/O线程和工作线程池
        state_ = ModuleState::RUNNING;
        return SystemErrors::SUCCESS;
    }

    ErrorCode stop() override {
        // 停止所有线程，清理资源
        state_ = ModuleState::STOPPED;
        return SystemErrors::SUCCESS;
    }

    // IPausable 实现
    ErrorCode pause() override {
        paused_ = true;
        // 暂停数据处理流水线
        return SystemErrors::SUCCESS;
    }

    ErrorCode resume() override {
        paused_ = false;
        // 恢复数据处理流水线
        return SystemErrors::SUCCESS;
    }

    // IMonitorable 实现
    HealthStatus getHealthStatus() const override {
        // 返回网络连接状态、缓冲区使用率等
        return HealthStatus{};
    }

    // IDependencyManaged 实现
    void declareDependency(const std::string& name, DependencyType type, bool optional) override {
        // 声明对ConfigManager等的依赖
    }
};
```

**纯计算模块 (无需暂停恢复功能)**：
```cpp
// 只需要生命周期管理和监控，不需要暂停恢复和复杂依赖管理
class MathProcessor : public ILifecycleManaged, public IMonitorable {
private:
    std::string module_name_ = "MathProcessor";
    ModuleState state_ = ModuleState::UNINITIALIZED;

public:
    // ILifecycleManaged 实现
    ErrorCode initialize() override {
        // 初始化数学库和CUDA上下文
        return SystemErrors::SUCCESS;
    }

    ErrorCode start() override {
        // 纯计算模块，启动即完成
        state_ = ModuleState::RUNNING;
        return SystemErrors::SUCCESS;
    }

    // IMonitorable 实现
    HealthStatus getHealthStatus() const override {
        // 返回GPU状态、内存使用等
        return HealthStatus{};
    }

    // 注意：不实现 IPausable 和 IDependencyManaged
    // 避免了不必要的接口实现负担
};
```

**轻量级服务模块**：
```cpp
// 只需要基本的生命周期管理
class LogService : public ILifecycleManaged {
private:
    std::string module_name_ = "LogService";
    ModuleState state_ = ModuleState::UNINITIALIZED;

public:
    // 仅实现必需的生命周期管理
    ErrorCode initialize() override {
        // 初始化日志输出
        return SystemErrors::SUCCESS;
    }

    ErrorCode start() override {
        state_ = ModuleState::RUNNING;
        return SystemErrors::SUCCESS;
    }

    ErrorCode stop() override {
        // 刷新并关闭日志文件
        state_ = ModuleState::STOPPED;
        return SystemErrors::SUCCESS;
    }

    // 注意：作为基础服务，不需要暂停恢复、监控或依赖管理
    // 保持接口实现的最小化和专注性
};
```

**设计优势总结**：
- **DataReceiver**: 完整功能模块，组合实现所有接口，提供全面的模块管理能力
- **MathProcessor**: 专用计算模块，只实现核心接口，避免不相关功能的实现负担
- **LogService**: 基础服务模块，最小接口实现，保持简洁性

这种设计完全符合**"客户端不应该依赖它不使用的接口"**的ISP原则，确保每个模块只实现其真正需要的功能。

#### 3.1.4 接口选择指导原则

| 模块类型             | 接口组合推荐                                                              | 适用场景                         | 实现收益                           |
| -------------------- | ------------------------------------------------------------------------- | -------------------------------- | ---------------------------------- |
| **完整业务模块**     | `ILifecycleManaged` + `IPausable` + `IMonitorable` + `IDependencyManaged` | 数据接收、信号处理等核心业务模块 | 完整的生命周期控制和运行时管理能力 |
| **计算密集型模块**   | `ILifecycleManaged` + `IMonitorable`                                      | 数学运算、算法处理等纯计算模块   | 避免不必要的暂停/恢复实现复杂度    |
| **基础服务模块**     | `ILifecycleManaged`                                                       | 日志、配置等基础服务             | 最小接口实现，保持服务简洁性       |
| **可暂停数据流模块** | `ILifecycleManaged` + `IPausable` + `IMonitorable`                        | 数据处理管道、流处理引擎         | 支持流控制，便于调试和优化         |
| **独立工具模块**     | `ILifecycleManaged` + `IMonitorable`                                      | 监控工具、诊断组件               | 提供状态反馈，便于系统诊断         |

**选择原则**：
1. **必需性原则**：只实现模块真正需要的接口，避免"为了实现而实现"
2. **职责单一**：每个接口专注于单一职责领域，不混合不相关的功能
3. **组合优于继承**：通过接口组合表达模块能力，而非强制继承大接口
4. **渐进式实现**：可以从最小接口开始，根据需要逐步添加其他接口

### 3.2 ExecutionEngine接口
每个核心模块的ExecutionEngine都遵循统一的接口规范：
```cpp
/**
 * @brief ExecutionEngine基础接口
 * @details 所有ExecutionEngine都必须实现此接口，提供统一的执行模式
 */
template<typename ContextType>
class IExecutionEngine : public IModule {
public:
    /**
     * @brief 处理事件
     * @param event 事件对象
     * @return ErrorCode 处理结果
     */
    virtual ErrorCode handleEvent(std::shared_ptr<BaseEvent> event) = 0;

    /**
     * @brief 获取执行上下文
     * @return 执行上下文的共享指针
     */
    virtual std::shared_ptr<ContextType> getContext() const = 0;

    /**
     * @brief 设置事件总线
     * @param event_bus 事件总线接口
     */
    virtual void setEventBus(std::shared_ptr<IEventBus> event_bus) = 0;

    /**
     * @brief 获取模块健康状态
     * @return HealthStatus 健康状态信息
     */
    virtual HealthStatus getHealthStatus() const = 0;
};

/**
 * @brief 服务注册中心接口
 * @details 实现依赖注入的核心接口
 */
class IServiceRegistry {
public:
    virtual ~IServiceRegistry() = default;

    /**
     * @brief 注册服务
     * @tparam T 服务类型
     * @param service 服务实例
     * @return 注册成功返回true
     */
    template<typename T>
    bool registerService(std::shared_ptr<T> service) = 0;

    /**
     * @brief 获取服务
     * @tparam T 服务接口类型
     * @return 服务实例，未找到返回nullptr
     */
    template<typename T>
    std::shared_ptr<T> getService() const = 0;

    /**
     * @brief 检查服务是否已注册
     * @tparam T 服务类型
     * @return 已注册返回true
     */
    template<typename T>
    bool hasService() const = 0;
};
```
### 3.3 事件接口规范
系统采用统一的事件接口，支持类型安全的事件处理：

```cpp
/**
 * @brief 基础事件接口
 * @details 所有事件都必须继承此接口，并强制携带TraceID确保全链路追踪
 */
struct BaseEvent {
    std::string event_type;         ///< 事件类型标识
    Timestamp timestamp;            ///< 事件时间戳
    TraceId trace_id;              ///< 追踪ID (必须！用于跨线程、异步场景的上下文传递)
    std::string source_module;      ///< 源模块名称
    Priority priority;              ///< 事件优先级

    virtual ~BaseEvent() = default;
    virtual std::string getEventType() const = 0;

    /**
     * @brief 构造函数自动获取当前TraceID
     * @details 如果当前线程有TraceID，自动继承；否则生成新的根TraceID
     */
    BaseEvent() : trace_id(TraceContext::getCurrentOrGenerate()) {}
};

/**
 * @brief 配置变更事件
 */
struct ConfigChangedEvent : BaseEvent {
    std::string key;                ///< 配置键路径
    YAML::Node old_value;          ///< 旧值
    YAML::Node new_value;          ///< 新值
    ChangeLevel change_level;       ///< 变更等级

    std::string getEventType() const override { return "CONFIG_CHANGED"; }
};

/**
 * @brief 模块状态变更事件
 */
struct ModuleStateChangedEvent : BaseEvent {
    std::string module_id;          ///< 模块ID
    ModuleState old_state;          ///< 旧状态
    ModuleState new_state;          ///< 新状态

    std::string getEventType() const override { return "MODULE_STATE_CHANGED"; }
};

/**
 * @brief 事件总线接口
 */
class IEventBus {
public:
    virtual ~IEventBus() = default;

    /**
     * @brief 发布事件
     * @param event 事件对象
     */
    virtual void publish(std::shared_ptr<BaseEvent> event) = 0;

    /**
     * @brief 订阅事件
     * @tparam T 事件类型
     * @param handler 事件处理函数
     * @return 订阅ID
     */
    template<typename T>
    virtual SubscriptionId subscribe(std::function<void(std::shared_ptr<T>)> handler) = 0;

    /**
     * @brief 取消订阅
     * @param subscription_id 订阅ID
     */
    virtual void unsubscribe(SubscriptionId subscription_id) = 0;
};

/**
 * @brief TraceID上下文守护类 (RAII)
 * @details 用于在异步事件处理和线程池任务中恢复TraceID上下文
 */
class TraceContextGuard {
private:
    TraceId previous_trace_id_;
public:
    explicit TraceContextGuard(const TraceId& trace_id)
        : previous_trace_id_(TraceContext::getCurrent()) {
        TraceContext::setCurrent(trace_id);
    }

    ~TraceContextGuard() {
        TraceContext::setCurrent(previous_trace_id_);
    }
};

/**
 * @brief 事件处理器的TraceID恢复模式
 * @details 所有事件处理器必须遵循此模式确保TraceID传递不断裂
 */
/*
使用示例：
void DataProcessor::onNewDataEvent(const NewDataEvent& event) {
    // 关键！事件处理器第一行必须恢复TraceID上下文
    TraceContextGuard guard(event.trace_id);

    // 后续所有日志和操作都会自动带上正确的Trace ID
    logger_->info("Processing new data batch");

    // 提交任务到线程池时也需要传递TraceID
    auto current_trace_id = TraceContext::getCurrent();
    thread_pool_->submit([this, data, current_trace_id]() {
        TraceContextGuard thread_guard(current_trace_id); // 工作线程中恢复
        processDataInBackground(data);
    });
}

void MonitoringModule::periodicTask() {
    // 对于定时任务等无上游TraceID的场景，生成新的根TraceID
    TraceContextGuard guard(TraceContext::generateRootTraceId());

    logger_->info("Starting periodic health check");
    collectSystemMetrics();
}
*/

/**
 * @brief 抽象服务提供者接口
 * @details 虽然实际实现使用构造函数注入，但定义抽象接口体现面向接口编程原则
 */
class IServiceProvider {
public:
    virtual ~IServiceProvider() = default;

    /**
     * @brief 获取服务实例
     * @tparam T 服务接口类型
     * @return 服务实例的共享指针
     * @note 实际实现中，这通常在main函数中直接创建和注入
     */
    template<typename T>
    virtual std::shared_ptr<T> getService() const = 0;

    /**
     * @brief 检查服务是否可用
     * @tparam T 服务接口类型
     * @return 服务可用性
     */
    template<typename T>
    virtual bool hasService() const = 0;
};

/**
 * @brief 基础服务接口定义
 * @details 定义系统中关键基础服务的抽象接口
 */

// 日志服务接口
class ILogger {
public:
    virtual ~ILogger() = default;
    virtual void info(const std::string& message, const TraceId& trace_id = TraceId::current()) = 0;
    virtual void warn(const std::string& message, const TraceId& trace_id = TraceId::current()) = 0;
    virtual void error(const std::string& message, const TraceId& trace_id = TraceId::current()) = 0;
    virtual void debug(const std::string& message, const TraceId& trace_id = TraceId::current()) = 0;
};

// 配置管理服务接口
class IConfigManager {
public:
    virtual ~IConfigManager() = default;
    virtual ErrorCode getInitialConfigFor(const std::string& module_name, YAML::Node& config) = 0;
    virtual void subscribeToConfigChanges(const std::string& key_pattern, std::function<void(const ConfigChangedEvent&)> callback) = 0;
};
```

### 3.4 数据交换格式
系统采用统一的数据交换格式，确保模块间数据的一致性和可追踪性：

```cpp
/**
 * @brief 统一数据包装器
 * @details 所有模块间传递的数据都使用此包装格式
 */
template<typename PayloadType>
struct DataPacket {
    // 包头信息
    struct Header {
        Timestamp timestamp;        ///< 数据时间戳
        uint64_t sequence_id;      ///< 序列号
        std::string data_type;     ///< 数据类型标识
        TraceId trace_id;          ///< 全链路追踪ID
        std::string source_module; ///< 源模块名称
        Priority priority;         ///< 数据优先级
        uint32_t payload_size;     ///< 负载大小
    } header;

    // 负载数据
    PayloadType payload;

    /**
     * @brief 创建数据包
     * @param data 负载数据
     * @param source 源模块名称
     * @param trace_id 追踪ID
     * @return 数据包实例
     */
    static DataPacket<PayloadType> create(
        const PayloadType& data,
        const std::string& source,
        const TraceId& trace_id = TraceId::generate()) {

        DataPacket<PayloadType> packet;
        packet.header.timestamp = Timestamp::now();
        packet.header.sequence_id = generateSequenceId();
        packet.header.data_type = typeid(PayloadType).name();
        packet.header.trace_id = trace_id;
        packet.header.source_module = source;
        packet.header.priority = Priority::NORMAL;
        packet.header.payload_size = sizeof(PayloadType);
        packet.payload = data;

        return packet;
    }
};

// 核心数据类型定义
using RawDataPacket = DataPacket<RawIQData>;
using DetectionPacket = DataPacket<DetectionResult>;
using TrackDataPacket = DataPacket<TrackData>;
```

**数据格式特点**：
- **统一包装**：所有数据都使用DataPacket模板进行包装，确保格式一致
- **全链路追踪**：每个数据包都携带TraceID，支持端到端追踪
- **类型安全**：使用模板确保编译时类型检查
- **性能优化**：头部信息紧凑，减少序列化开销
- **可扩展性**：支持任意类型的负载数据
### 3.5 通信协议
模块间通信采用基于消息的协议，支持同步和异步交互：
```mermaid
sequenceDiagram
    participant 生产者 as 生产模块
    participant 缓冲区 as 数据缓冲区
    participant 消费者 as 消费模块
    participant 事件 as 事件通知

    Note over 生产者,事件: 模块通信流程

    生产者->>缓冲区: 1.写入数据
    缓冲区-->>生产者: 2.确认写入成功

    缓冲区->>消费者: 3.通知有新数据
    消费者->>缓冲区: 4.读取数据
    缓冲区-->>消费者: 5.返回数据内容

    消费者->>事件: 6.发送处理完成通知
    事件->>生产者: 7.转发完成消息

    Note over 生产者,事件: 通信完成
```
**协议说明**：模块间的通信就像邮递系统一样简单明了。生产模块把数据放到"邮箱"（缓冲区）里，系统确认收到；当有新数据时，系统会通知消费模块去取；消费模块取到数据处理完后，会发送一个"已处理"的回执。整个过程是异步的，不会相互阻塞，保证了系统的高效运行。

---
## 4 生命周期协调策略

### 4.1 事件驱动启动流程
系统采用事件驱动的模块启动流程，通过TaskScheduler统一协调各模块的生命周期：

```mermaid
sequenceDiagram
    participant Main as 应用主入口
    participant TaskScheduler as TaskScheduler
    participant EventBus as EventBus
    participant ConfigEngine as ConfigExecutionEngine
    participant LogService as LoggingService
    participant DataReceiver as DataReceiver

    Note over Main, DataReceiver: 系统启动流程

    Main->>Main: 1. 创建服务注册中心
    Main->>ConfigEngine: 2. 创建并初始化ConfigEngine
    Main->>LogService: 3. 创建并初始化LogService
    Main->>EventBus: 4. 创建并初始化EventBus

    Main->>TaskScheduler: 5. 创建TaskScheduler
    TaskScheduler->>TaskScheduler: 6. 初始化，注册事件处理器

    Note over TaskScheduler, DataReceiver: 业务模块启动

    TaskScheduler->>EventBus: 7. 发布StartModuleEvent(DataReceiver)
    EventBus->>DataReceiver: 8. 路由启动事件

    DataReceiver->>DataReceiver: 9. 执行initialize()
    DataReceiver->>EventBus: 10. 发布ModuleReadyEvent

    EventBus->>TaskScheduler: 11. 通知模块就绪
    TaskScheduler->>EventBus: 12. 发布StartModuleEvent(下一个模块)

    Note over Main, DataReceiver: 启动完成，系统进入运行状态
```

**启动流程特点**：
- **分阶段启动**：基础服务 → 事件总线 → 任务调度器 → 业务模块
- **事件驱动**：所有启动操作通过事件触发，确保可追踪性
- **依赖解析**：TaskScheduler根据预定义的模块依赖图来决定启动顺序，确保前置模块已就绪
- **故障处理**：任何模块启动失败都会触发相应的恢复策略

**模块依赖启动顺序**：
1. **无依赖基础服务**：LoggingService（被动服务，无需启动流程）
2. **配置服务层**：ConfigExecutionEngine（依赖LoggingService接口注入）
3. **通信基础设施**：EventBus（依赖LoggingService接口注入）
4. **任务调度层**：TaskScheduler（依赖LoggingService、EventBus、ConfigManager接口注入）
5. **业务模块层**：DataReceiver → SignalProcessor → DataProcessor → DisplayController

**重要说明**：TaskScheduler在发布`StartModuleEvent`之前，会先检查模块依赖图，确保所有前置模块已经处于`READY`状态。例如，DataReceiver的启动需要等待EventBus和ConfigManager都已就绪。

#### 4.1.1 优雅关闭与超时处理机制

系统提供带超时保护的优雅关闭机制，避免关闭过程卡死：

```cpp
/**
 * @brief TaskScheduler的超时关闭实现
 */
class TaskScheduler {
private:
    static constexpr std::chrono::seconds DEFAULT_STOP_TIMEOUT{5};

public:
    ErrorCode stopAllModules() {
        logger_->info("Starting graceful shutdown sequence");

        auto modules_to_stop = getModulesInReverseStartupOrder();

        for (auto& module : modules_to_stop) {
            ErrorCode result = stopModuleWithTimeout(module, DEFAULT_STOP_TIMEOUT);

            if (result != ErrorCode::SUCCESS) {
                logger_->fatal("Module {} failed to stop gracefully within timeout",
                              module->getModuleName());

                // 记录超时模块，但继续关闭其他模块
                recordFailedShutdown(module->getModuleName());

                // 尝试强制中断（如果安全且可能）
                attemptForceTermination(module);
            }
        }

        // 检查是否有模块关闭失败
        if (!failed_shutdowns_.empty()) {
            logger_->fatal("Shutdown completed with {} failed modules",
                          failed_shutdowns_.size());
            return ErrorCode::PARTIAL_FAILURE;
        }

        return ErrorCode::SUCCESS;
    }

private:
    ErrorCode stopModuleWithTimeout(std::shared_ptr<IModule> module,
                                   std::chrono::seconds timeout) {
        std::promise<ErrorCode> promise;
        auto future = promise.get_future();

        // 在独立线程中执行stop()
        std::thread stop_thread([&promise, module]() {
            try {
                ErrorCode result = module->stop();
                promise.set_value(result);
            } catch (...) {
                promise.set_value(ErrorCode::FATAL_ERROR);
            }
        });

        // 等待超时
        if (future.wait_for(timeout) == std::future_status::timeout) {
            // 超时处理：分离线程，记录错误，继续执行
            stop_thread.detach();
            return ErrorCode::TIMEOUT;
        }

        stop_thread.join();
        return future.get();
    }

    void attemptForceTermination(std::shared_ptr<IModule> module) {
        // 实现平台相关的线程中断逻辑
        // 注意：这是最后手段，需要非常小心
        logger_->warn("Attempting force termination of module {}",
                     module->getModuleName());
    }
};
```

**超时关闭策略**：
1. **超时时间配置**：每个模块可配置独立的关闭超时时间（默认5秒）
2. **渐进式关闭**：按启动顺序逆序关闭，确保依赖关系的完整性
3. **超时后继续**：单个模块超时不阻塞整体关闭流程
4. **状态记录**：记录关闭异常，向外部系统（如systemd）报告非正常退出
5. **资源清理**：即使强制终止也要尽力清理关键资源（如文件句柄、网络连接）
### 4.2 故障恢复机制

#### 4.2.1 熔断器模式 - 防恢复风暴保护机制

**设计背景**：针对"重复快速失败"场景，引入熔断器模式防止TaskScheduler的恢复机制反复尝试重启故障模块，形成"恢复风暴"，消耗系统资源并产生无用日志。

**熔断器状态机**：
```mermaid
stateDiagram-v2
    [*] --> CLOSED : 正常运行
    CLOSED --> OPEN : 时间窗口内失败≥阈值
    OPEN --> HALF_OPEN : 熔断超时，允许探测
    HALF_OPEN --> CLOSED : 探测成功
    HALF_OPEN --> OPEN : 探测再次失败，延长熔断时间

    note right of CLOSED : 允许所有恢复操作
    note right of OPEN : 拒绝恢复操作，等待人工干预
    note right of HALF_OPEN : 允许一次探测性恢复
```

**熔断器核心机制**：

```cpp
/**
 * @brief 模块熔断器管理器 - 防止恢复风暴
 */
class ModuleCircuitBreakerManager {
private:
    static constexpr uint32_t FAILURE_THRESHOLD = 3;        // 失败阈值
    static constexpr Duration FAILURE_WINDOW = 60s;         // 时间窗口
    static constexpr Duration BASE_BACKOFF = 5s;           // 基础熔断时间
    static constexpr Duration MAX_BACKOFF = 300s;          // 最大熔断时间

    struct ModuleCircuitState {
        std::string module_id;
        CircuitState state = CircuitState::CLOSED;
        uint32_t failure_count = 0;
        Timestamp first_failure_time;
        Timestamp last_failure_time;
        Duration current_backoff = BASE_BACKOFF;
        bool is_probing = false;                           // 是否正在探测
    };

    std::unordered_map<std::string, ModuleCircuitState> circuits_;

public:
    /**
     * @brief 记录模块失败，更新熔断器状态
     * @param module_id 失败的模块ID
     * @param failure_context 失败上下文信息
     * @return 是否应该继续恢复操作
     */
    bool recordFailure(const std::string& module_id,
                      const ModuleFailureContext& failure_context);

    /**
     * @brief 检查模块是否被熔断
     * @param module_id 模块ID
     * @return 熔断状态
     */
    CircuitState getCircuitState(const std::string& module_id) const;

    /**
     * @brief 尝试将熔断器转换为半开状态（探测）
     * @param module_id 模块ID
     * @return 是否成功开始探测
     */
    bool attemptProbe(const std::string& module_id);

    /**
     * @brief 记录探测结果
     * @param module_id 模块ID
     * @param success 探测是否成功
     */
    void recordProbeResult(const std::string& module_id, bool success);
};

enum class CircuitState {
    CLOSED,      // 正常状态，允许所有恢复操作
    OPEN,        // 熔断状态，拒绝恢复操作
    HALF_OPEN    // 半开状态，允许探测性恢复
};
```

**增强的故障恢复流程**：

```mermaid
sequenceDiagram
    participant DataReceiver as DataReceiver
    participant TaskScheduler as TaskScheduler
    participant CircuitBreaker as CircuitBreakerManager
    participant EventBus as EventBus
    participant OpsTeam as 运维团队

    Note over DataReceiver, OpsTeam: 带熔断保护的智能恢复流程

    DataReceiver->>EventBus: 1. MODULE_FAILED_EVENT (连续第3次)
    EventBus->>TaskScheduler: 2. 路由故障事件

    TaskScheduler->>CircuitBreaker: 3. recordFailure(DataReceiver, context)
    CircuitBreaker->>CircuitBreaker: 4. 检测：60秒内失败3次

    Note over CircuitBreaker: 触发熔断器 CLOSED → OPEN

    CircuitBreaker-->>TaskScheduler: 5. 返回：拒绝恢复 (熔断开启)
    TaskScheduler->>EventBus: 6. MODULE_RECOVERY_HALTED_EVENT

    EventBus->>OpsTeam: 7. 告警通知：模块已熔断，需人工介入

    Note over OpsTeam: 运维团队排查问题 (争取到时间)

    TaskScheduler->>CircuitBreaker: 8. 5分钟后：attemptProbe(DataReceiver)
    CircuitBreaker-->>TaskScheduler: 9. 允许一次探测性恢复

    Note over CircuitBreaker: 熔断器 OPEN → HALF_OPEN

    TaskScheduler->>DataReceiver: 10. 执行探测性重启
    DataReceiver-->>TaskScheduler: 11. 探测成功 (问题已解决)

    TaskScheduler->>CircuitBreaker: 12. recordProbeResult(DataReceiver, success=true)

    Note over CircuitBreaker: 熔断器 HALF_OPEN → CLOSED

    TaskScheduler->>EventBus: 13. MODULE_RECOVERY_COMPLETED_EVENT
```

**指数退避策略**：
- **基础退避**：首次熔断等待5秒
- **指数增长**：每次重新熔断时间翻倍（5s → 10s → 20s → 40s → ...）
- **上限保护**：最大等待时间不超过300秒
- **探测窗口**：熔断期间定期尝试探测性恢复，而非完全停止

**系统自我保护收益**：
1. **防止资源消耗**：避免无意义的频繁重启操作
2. **减少日志噪音**：控制重复失败产生的日志量
3. **争取介入时间**：为运维人员排查问题争取时间
4. **保护整体稳定性**：防止单个故障模块拖垮系统性能

#### 4.2.2 广播风暴防护机制

系统通过状态管理和熔断器双重保护，避免连锁错误事件：

```mermaid
sequenceDiagram
    participant DataReceiver as DataReceiver
    participant DownstreamModules as 下游模块群
    participant TaskScheduler as TaskScheduler
    participant CircuitBreaker as CircuitBreakerManager
    participant EventBus as EventBus

    Note over DataReceiver, EventBus: 双重保护的故障恢复流程

    DataReceiver->>EventBus: 1. MODULE_FAILED_EVENT (根本故障)
    EventBus->>TaskScheduler: 2. 路由到故障协调者

    TaskScheduler->>CircuitBreaker: 3. 检查熔断状态
    CircuitBreaker-->>TaskScheduler: 4. 允许恢复（未熔断）

    TaskScheduler->>TaskScheduler: 5. 切换系统状态为 RECOVERING
    TaskScheduler->>EventBus: 6. SYSTEM_STATE_CHANGED(RECOVERING)

    Note over DownstreamModules: 下游模块因缺数据开始报错

    DownstreamModules->>EventBus: 7. 连锁失败事件 (多个)
    EventBus->>TaskScheduler: 8. 路由连锁失败事件

    TaskScheduler->>TaskScheduler: 9. 检测系统状态为RECOVERING
    Note over TaskScheduler: **暂时忽略**连锁失败 (广播风暴防护)

    TaskScheduler->>DataReceiver: 10. 执行精确恢复
    DataReceiver-->>TaskScheduler: 11. 恢复失败

    TaskScheduler->>CircuitBreaker: 12. recordFailure() - 更新计数
    Note over CircuitBreaker: 如连续失败3次 → 触发熔断器

    TaskScheduler->>EventBus: 13. MODULE_RECOVERY_STATUS_EVENT
```

**双重防护原则**：
1. **系统状态防护**：在RECOVERING状态下忽略连锁失败事件，专注根本原因
2. **熔断器防护**：防止单个模块的重复失败形成"恢复风暴"
3. **精确恢复策略**：基于依赖图分析，优先恢复根本故障模块
4. **协调式恢复**：通过事件机制协调所有模块的恢复行为
5. **运维友好**：熔断后提供明确的人工介入信号和时间窗口

#### 4.2.3 双重依赖类型定义

为解决静态启动依赖与动态恢复策略的矛盾，系统定义两种依赖类型：

```cpp
/**
 * @brief 模块依赖关系定义
 */
enum class DependencyType {
    STRONG_DEPENDENCY,    // 强依赖：启动/关闭依赖，影响生命周期顺序
    WEAK_DEPENDENCY      // 弱依赖：数据流依赖，仅影响故障恢复策略
};

struct ModuleDependency {
    std::string target_module;      // 依赖的目标模块
    DependencyType type;           // 依赖类型
    bool required_for_startup;     // 是否启动必需
    bool critical_for_operation;   // 是否运行关键
};
```

#### 4.2.4 依赖管理系统实现

为支持上述依赖驱动的恢复策略，TaskScheduler内置依赖管理系统：

```mermaid
graph TB
    subgraph "依赖管理系统架构"
        direction TB

        subgraph "依赖注册阶段"
            MODULE_REGISTRY[模块注册器<br/>ModuleRegistry]
            DEPENDENCY_RESOLVER[依赖解析器<br/>DependencyResolver]
            STARTUP_SEQUENCER[启动序列器<br/>StartupSequencer]
        end

        subgraph "运行时管理"
            DEPENDENCY_GRAPH[依赖图<br/>DependencyGraph]
            HEALTH_MONITOR[健康监控<br/>HealthMonitor]
            RECOVERY_ENGINE[恢复引擎<br/>RecoveryEngine]
        end

        subgraph "具体模块实例"
            DATA_RECEIVER[DataReceiver<br/>声明依赖: EventBus]
            SIGNAL_PROC[SignalProcessor<br/>声明依赖: DataReceiver]
            DATA_PROC[DataProcessor<br/>声明依赖: SignalProcessor]
        end
    end

    MODULE_REGISTRY --> DEPENDENCY_RESOLVER
    DEPENDENCY_RESOLVER --> STARTUP_SEQUENCER
    STARTUP_SEQUENCER --> DEPENDENCY_GRAPH
    DEPENDENCY_GRAPH --> HEALTH_MONITOR
    HEALTH_MONITOR --> RECOVERY_ENGINE

    DATA_RECEIVER --> MODULE_REGISTRY
    SIGNAL_PROC --> MODULE_REGISTRY
    DATA_PROC --> MODULE_REGISTRY
```

**依赖管理系统实现**:
```cpp
namespace radar::integration {

/**
 * @brief 模块依赖管理器
 * @details 负责依赖注入、启动顺序计算、故障恢复协调
 */
class DependencyManager {
public:
    /**
     * @brief 注册模块及其依赖声明
     * @param module_name 模块名称
     * @param module 模块实例
     * @return ErrorCode 注册结果
     */
    ErrorCode registerModule(const std::string& module_name,
                             std::shared_ptr<IModule> module) {
        // 1. 获取模块的依赖声明
        auto dependencies = module->getDependencies();

        // 2. 存储模块信息
        registered_modules_[module_name] = {module, dependencies};

        // 3. 更新依赖图
        dependency_graph_.addNode(module_name, dependencies);

        return ErrorCode::SUCCESS;
    }

    /**
     * @brief 解析所有依赖并执行注入
     * @return ErrorCode 解析结果
     */
    ErrorCode resolveDependencies() {
        for (auto& [module_name, module_info] : registered_modules_) {
            auto& module = module_info.module;
            auto& dependencies = module_info.dependencies;

            for (auto& [dep_name, dep_info] : dependencies) {
                auto dep_it = registered_modules_.find(dep_name);
                if (dep_it == registered_modules_.end()) {
                    if (!dep_info.optional) {
                        return ErrorCode::DEPENDENCY_NOT_FOUND;
                    }
                    continue;
                }

                // 执行依赖注入
                ErrorCode result = module->injectDependency(dep_name, dep_it->second.module);
                if (result != ErrorCode::SUCCESS) {
                    return result;
                }
            }
        }

        return ErrorCode::SUCCESS;
    }

    /**
     * @brief 计算启动顺序
     * @return 按依赖关系排序的模块启动列表
     */
    std::vector<std::string> calculateStartupOrder() {
        return dependency_graph_.topologicalSort();
    }

    /**
     * @brief 计算模块故障的影响范围
     * @param failed_module 故障模块名称
     * @return 受影响的模块列表
     */
    std::vector<std::string> calculateImpactScope(const std::string& failed_module) {
        return dependency_graph_.getDownstreamDependents(failed_module);
    }

private:
    struct ModuleInfo {
        std::shared_ptr<IModule> module;
        std::map<std::string, ModuleDependency> dependencies;
    };

    std::map<std::string, ModuleInfo> registered_modules_;
    DependencyGraph dependency_graph_;
};

} // namespace radar::integration
```

**恢复策略矩阵**：

| 故障模块            | 依赖类型分析               | 恢复策略                                          | 影响范围       |
| ------------------- | -------------------------- | ------------------------------------------------- | -------------- |
| **DataReceiver**    | 下游对其为强依赖           | 暂停下游→重启DataReceiver→恢复下游                | 整个数据处理链 |
| **SignalProcessor** | 上游为弱依赖，下游为强依赖 | 只重启SignalProcessor，上游继续工作但临时缓存输出 | 中段模块       |
| **DataProcessor**   | 上游为弱依赖，下游为弱依赖 | 只重启DataProcessor，其他模块不受影响             | 单个模块       |
| **LoggingService**  | 所有模块对其为弱依赖       | 启动日志降级模式，数据流不中断                    | 仅日志功能     |
| **ConfigManager**   | 所有模块对其为弱依赖       | 冻结配置更新，模块用当前配置继续工作              | 仅配置热更新   |

**精确恢复流程**：
1. **根因分析**：TaskScheduler根据依赖图分析故障的根本原因和传播路径
2. **影响评估**：基于弱依赖关系评估实际影响范围，避免过度恢复
3. **最小干预**：优先选择影响最小的恢复策略
4. **协调恢复**：通过SystemStateChanged事件协调相关模块的暂停/恢复
### 4.3 配置热更新协调
系统采用完全事件驱动的配置热更新机制，**ConfigManager只负责配置管理和变更通知，业务模块完全自主决策变更响应**：

```mermaid
sequenceDiagram
    participant User as 用户/运维
    participant ConfigEngine as ConfigExecutionEngine
    participant EventBus as 事件总线
    participant BusinessModule as 业务模块
    participant TaskScheduler as 任务调度器

    User->>ConfigEngine: 1. 提交配置变更请求
    ConfigEngine->>ConfigEngine: 2. 执行通用验证和两阶段业务验证
    ConfigEngine->>ConfigEngine: 3. 原子性更新配置树和持久化
    ConfigEngine->>EventBus: 4. 发布 CONFIG_CHANGED 事件

    EventBus->>BusinessModule: 5. 推送配置变更事件
    BusinessModule->>BusinessModule: 6. 模块自主判断变更影响等级

    alt 动态配置 (模块判断可立即应用)
        BusinessModule->>BusinessModule: 7a. 直接应用新配置值
        BusinessModule->>EventBus: 8a. 发布 CONFIG_APPLIED 确认事件

    else 重载配置 (模块判断需要重载)
        BusinessModule->>EventBus: 7b. 向TaskScheduler发送 REQUEST_MODULE_RELOAD 事件
        EventBus->>TaskScheduler: 8b. 转发重载请求
        TaskScheduler->>BusinessModule: 9b. 调用 IConfigurable.reload()
        BusinessModule->>BusinessModule: 10b. 执行重载逻辑
        BusinessModule->>EventBus: 11b. 发布 RELOAD_COMPLETED 事件

    else 重启配置 (模块判断需要重启)
        BusinessModule->>EventBus: 7c. 发送 MODULE_FATAL_ERROR 事件 (原因: 配置变更需要重启)
        EventBus->>TaskScheduler: 8c. 转发致命错误事件
        TaskScheduler->>BusinessModule: 9c. stop()
        TaskScheduler->>BusinessModule: 10c. initialize(新配置)
        TaskScheduler->>BusinessModule: 11c. start()
        BusinessModule->>EventBus: 12c. 发布 MODULE_RESTARTED 事件
    end
```

**模块响应配置变更的三种策略**：

| 变更影响等级 | 模块决策逻辑                   | 模块响应行为                                                                | 典型示例                        |
| ------------ | ------------------------------ | --------------------------------------------------------------------------- | ------------------------------- |
| **动态配置** | 模块判断此变更可以立即安全应用 | 在事件处理器中直接应用新值                                                  | 日志级别、UI主题、阈值参数      |
| **重载配置** | 模块判断需要重载来安全应用变更 | 向`TaskScheduler`发送`REQUEST_MODULE_RELOAD`事件                            | 算法参数、滤波器系数            |
| **重启配置** | 模块判断变更影响基础架构       | 向`TaskScheduler`发送`MODULE_FATAL_ERROR`事件，原因："关键配置变更需要重启" | 网络端口、GPU设备ID、线程池大小 |

**配置热更新架构优势**：
- ✅ **职责纯粹性**：`ConfigManager`不参与业务决策，只负责配置状态管理和事件发布
- ✅ **模块自主性**：业务模块完全掌控配置变更的影响判断和响应策略
- ✅ **事件驱动一致性**：所有交互通过事件总线进行，无直接模块间调用
- ✅ **去耦合设计**：配置管理与任务调度、模块生命周期管理完全解耦
- ✅ **系统扩展性**：新模块无需修改ConfigManager，只需订阅CONFIG_CHANGED事件

#### 4.3.1 配置原子性应用机制

为避免配置变更在数据处理过程中造成不一致，系统引入"配置暂存与帧边界应用"机制：

```cpp
/**
 * @brief 配置暂存与原子切换示例
 */
class SignalProcessor : public IModule {
private:
    // 当前生效配置
    SignalProcessorConfig current_config_;

    // 待应用配置暂存区
    std::optional<SignalProcessorConfig> pending_config_;
    std::mutex config_mutex_;

public:
    // 事件处理器：收到配置变更时不立即应用
    void onConfigChanged(const ConfigChangedEvent& event) {
        TraceContextGuard guard(event.trace_id);

        if (event.key.starts_with("signal_processor.")) {
            std::lock_guard<std::mutex> lock(config_mutex_);

            // 解析新配置并存入暂存区，不立即应用
            SignalProcessorConfig new_config = parseConfig(event.new_value);
            pending_config_ = new_config;

            logger_->info("Configuration staged for next frame boundary");
        }
    }

    // 主处理循环：在帧边界检查并原子应用配置
    void processDataFrame() {
        // 1. 帧边界检查配置更新
        {
            std::lock_guard<std::mutex> lock(config_mutex_);
            if (pending_config_.has_value()) {
                // 原子地切换配置
                current_config_ = pending_config_.value();
                pending_config_.reset();

                logger_->info("Configuration applied atomically at frame boundary");

                // 发布应用确认事件
                auto applied_event = std::make_shared<ConfigAppliedEvent>();
                applied_event->module_name = getModuleName();
                event_bus_->publish(applied_event);
            }
        }

        // 2. 使用稳定的配置处理整帧数据
        processFrameWithConfig(current_config_);
    }
};
```

**批量配置事件处理**：
对于相互依赖的配置变更（如FFT尺寸和窗函数类型），`ConfigManager`发布包含所有相关变更的单个事件：

```cpp
struct BatchConfigChangedEvent : BaseEvent {
    std::map<std::string, ConfigChange> related_changes;  // 批量变更
    std::string batch_id;  // 批次标识

    std::string getEventType() const override { return "BATCH_CONFIG_CHANGED"; }
};
```

这确保模块能原子地应用一组相关配置，避免无效的中间状态。

---
## 5 集成测试方案
### 5.1 测试策略
模块集成测试采用分层策略，确保系统各层面的质量：

#### 测试金字塔结构

| 测试层级             | 测试类型   | 测试重点     | 测试特点           |
| -------------------- | ---------- | ------------ | ------------------ |
| **第1层 - 单元测试** | 接口测试   | 接口功能验证 | 快速执行，覆盖全面 |
|                      | 功能测试   | 模块内部逻辑 | 逻辑正确，边界处理 |
| **第2层 - 集成测试** | 模块协作   | 模块间配合   | 接口兼容，数据一致 |
|                      | 数据流测试 | 数据传递验证 | 流程正确，格式统一 |
| **第3层 - 系统测试** | 端到端测试 | 完整流程验证 | 全链路，真实数据   |
|                      | 性能测试   | 系统性能指标 | 高负载，压力测试   |
| **第4层 - 验收测试** | 功能验收   | 业务需求验证 | 用户视角，真实场景 |
|                      | 用户体验   | 易用性检查   | 交互流畅，操作简便 |

**测试策略说明**：测试就像质量检查流水线，分为四个层次。最底层是单元测试，检查每个小零件是否合格；第二层是集成测试，检查零件组装后是否配合良好；第三层是系统测试，检查整个产品是否按设计工作；最高层是验收测试，检查产品是否满足用户需求。越往上测试越复杂，但越能发现实际使用中的问题。
### 5.2 测试环境
集成测试环境模拟真实部署场景，提供全面的测试支持：

#### 测试环境组成

| 环境类型       | 具体组件   | 主要功能       | 配置要求             |
| -------------- | ---------- | -------------- | -------------------- |
| **硬件支持**   | 测试服务器 | 提供计算资源   | 多核CPU，大内存      |
|                | GPU设备    | 加速计算支持   | CUDA兼容显卡         |
| **软件工具**   | 测试框架   | 自动化测试执行 | GoogleTest, pytest   |
|                | 模拟服务   | 外部依赖模拟   | Mock服务，桩程序     |
| **监控分析**   | 性能监控   | 运行时指标收集 | 实时监控，告警机制   |
|                | 覆盖率分析 | 代码覆盖率统计 | 行覆盖率，分支覆盖率 |
| **自动化支持** | 自动测试   | 持续集成执行   | CI/CD流水线集成      |
|                | 报告生成   | 测试结果汇总   | HTML报告，趋势分析   |

**环境说明**：测试环境就像一个完整的实验室，包含四个关键部分。硬件支持提供真实的运行环境，就像产品最终使用的环境；软件工具提供测试所需的各种程序和模拟器；监控分析帮助观察测试过程和结果；自动化系统让测试可以自动进行，提高效率。这样的环境确保测试结果真实可靠。
### 5.3 测试用例
关键集成测试用例覆盖系统核心功能：

#### 核心测试场景

| 测试分类         | 具体测试项   | 测试目标         | 验证重点               |
| ---------------- | ------------ | ---------------- | ---------------------- |
| **数据传输测试** | 数据接收验证 | 网络数据正确接收 | 数据完整性，格式正确性 |
|                  | 缓冲区流转   | 模块间数据传递   | 流转效率，无数据丢失   |
| **模块协作测试** | 启动停止测试 | 模块生命周期管理 | 启动顺序，优雅关闭     |
|                  | 配置更新测试 | 运行时配置变更   | 热更新，配置生效       |
| **性能压力测试** | 高负载测试   | 系统极限性能     | 吞吐量，响应时间       |
|                  | 并发处理测试 | 多线程安全性     | 数据一致性，无死锁     |
| **异常恢复测试** | 故障注入测试 | 异常情况处理     | 错误检测，优雅降级     |
|                  | 系统恢复测试 | 故障后恢复能力   | 自动恢复，数据保护     |

**用例说明**：测试用例覆盖四个重要方面，就像全方位体检一样。数据传输测试检查"血液循环"是否畅通；模块协作测试检查各个"器官"是否配合良好；性能压力测试检查系统在高负荷下是否正常工作；异常恢复测试检查系统遇到问题时是否能自我修复。这些测试确保系统在各种情况下都能稳定运行。

---
## 6 模块协作机制
### 6.1 数据协作
模块间数据协作通过环形缓冲区和标准数据格式实现：
```mermaid
sequenceDiagram
    participant 生产者 as 数据生产模块
    participant 缓冲区 as 数据中转站
    participant 消费者 as 数据消费模块
    participant 监控 as 性能监控

    Note over 生产者,监控: 数据协作过程

    生产者->>缓冲区: 1.创建数据包
    生产者->>缓冲区: 2.写入数据包
    缓冲区-->>生产者: 3.确认写入完成

    缓冲区->>消费者: 4.通知有新数据
    消费者->>缓冲区: 5.读取数据包
    缓冲区-->>消费者: 6.返回数据内容

    消费者->>消费者: 7.处理数据
    消费者->>监控: 8.报告处理状态

    监控->>生产者: 9.反馈性能指标

    Note over 生产者,监控: 协作完成
```
**协作说明**：数据协作就像接力赛跑一样，每个模块都有自己的职责。生产模块创建数据包并放到"中转站"，系统确认收到后通知下一个模块来取数据。消费模块取到数据后进行处理，处理完成后向监控系统报告。监控系统会把性能信息反馈给生产模块，帮助它调整工作节奏。这样形成了一个高效的数据传递链条。
### 6.2 控制协作
模块间控制协作通过任务调度器和生命周期管理器实现：
```mermaid
sequenceDiagram
    participant 管理员 as 系统管理员
    participant 调度器 as 任务调度器
    participant 管理器 as 生命周期管理器
    participant 模块 as 目标模块

    Note over 管理员,模块: 控制协作流程

    管理员->>调度器: 1.发送控制命令
    调度器->>管理器: 2.转发控制请求
    管理器->>模块: 3.执行具体操作

    模块->>模块: 4.状态转换
    模块-->>管理器: 5.返回操作结果

    管理器-->>调度器: 6.报告状态变化
    调度器-->>管理员: 7.反馈执行结果

    Note over 管理员,模块: 控制完成
```
**协作说明**：控制协作就像公司的管理层级一样。系统管理员是最高领导，发出指令给任务调度器（中层管理）；调度器再把指令传达给生命周期管理器（直接主管）；管理器负责具体执行，让目标模块进行相应的操作。执行结果逐级上报，让管理员知道操作是否成功。这种层级管理确保了控制命令的有序执行。
### 6.3 事件协作
模块间事件协作通过事件总线和观察者模式实现：
```mermaid
graph TB
    subgraph "事件通信系统"
        subgraph "事件发布方"
            P1[模块A]
            P2[模块B]
            P3[模块C]
        end

        subgraph "事件中心"
            CENTER[事件总线<br/>统一处理中心]
            FILTER[事件过滤器]
            ROUTER[事件分发器]
        end

        subgraph "事件接收方"
            S1[订阅者1]
            S2[订阅者2]
            S3[订阅者3]
        end
    end

    %% 事件流向
    P1 --> CENTER
    P2 --> CENTER
    P3 --> CENTER

    CENTER --> FILTER
    FILTER --> ROUTER

    ROUTER --> S1
    ROUTER --> S2
    ROUTER --> S3

    classDef publisher fill:#e8f4ff,stroke:#0066cc,stroke-width:2px
    classDef center fill:#e8ffe8,stroke:#00aa00,stroke-width:2px
    classDef subscriber fill:#fff0e8,stroke:#ff6600,stroke-width:2px

    class P1,P2,P3 publisher
    class CENTER,FILTER,ROUTER center
    class S1,S2,S3 subscriber
```
**协作说明**：事件协作就像广播电台一样工作。各个模块可以向"电台"（事件总线）发送消息，电台收到消息后通过过滤器筛选，再通过分发器把消息送到感兴趣的"听众"（订阅者）那里。这种方式让模块之间不需要直接联系，就能互相通知重要事情的发生，特别适合处理突发事件和状态通知。

---
## 7 端到端关键场景设计

为帮助开发者和工程师快速理解系统的核心交互流程，本章节提供关键业务场景的端到端序列图。这些序列图展示了多个模块如何协作完成复杂的系统级任务。

#### 7.1 "Happy Path" 数据处理流程

展示从UDP数据包接收到**独立的显控终端**完成显示的完整数据处理链路，这是系统核心的分布式业务流程。

```mermaid
sequenceDiagram
    participant Radar as 雷达阵面
    participant DataReceiver as 数据接收模块
    participant SignalProcessor as 信号处理模块
    participant DataProcessor as 数据处理模块
    participant DisplayController as <b>数据网关模块</b>
    participant Network as <b>物理网络 (UDP)</b>
    participant ClientApp as <b>显控终端 (独立应用)</b>

    Note over Radar,ClientApp: 端到端数据处理流程 (分布式架构)

    %% 阶段1: 数据接收与缓存
    Radar->>DataReceiver: 1. 发送UDP数据包 (原始I/Q数据)
    activate DataReceiver
    DataReceiver->>DataReceiver: 2. UDP解析、验证、封装DataObject
    DataReceiver->>DataReceiver: 3. 生成Trace ID: trace-abc123
    DataReceiver->>SignalProcessor: 4. 推送至raw_data_buffer
    deactivate DataReceiver

    %% 阶段2: GPU信号处理
    SignalProcessor->>SignalProcessor: 5. 从缓冲区获取DataObject
    activate SignalProcessor
    Note right of SignalProcessor: 包含trace_id: trace-abc123
    SignalProcessor->>SignalProcessor: 6. GPU算法流水线执行<br/>(FFT → 滤波 → CFAR检测)
    SignalProcessor->>DataProcessor: 7. 推送DetectionResult到detection_result_buffer
    deactivate SignalProcessor

    %% 阶段3: 目标跟踪处理
    DataProcessor->>DataProcessor: 8. 从缓冲区获取DetectionResult
    activate DataProcessor
    Note right of DataProcessor: 继承trace_id: trace-abc123
    DataProcessor->>DataProcessor: 9. 数据关联与航迹管理<br/>(聚类 → 关联 → 卡尔曼滤波)
    DataProcessor->>DisplayController: 10. 推送TrackData到track_data_buffer
    deactivate DataProcessor

    %% 阶段4: 数据网关处理与网络分发
    DisplayController->>DisplayController: 11. 从缓冲区获取TrackData
    activate DisplayController
    DisplayController->>DisplayController: 12. <b>序列化TrackData (Protobuf)</b>
    Note right of DisplayController: TraceID被一同序列化
    DisplayController->>Network: 13. <b>发送UDP单播数据包</b>
    deactivate DisplayController

    %% 阶段5: 客户端接收与显示
    Network->>ClientApp: 14. 接收UDP数据包
    activate ClientApp
    ClientApp->>ClientApp: 15. <b>反序列化数据</b>
    ClientApp->>ClientApp: 16. 渲染目标点到屏幕
    deactivate ClientApp

    Note over Radar,ClientApp: 完整链路处理完成，Trace ID贯穿服务器与客户端
```

**关键流程说明**:

1.  **数据接收与处理 (1-10)**: 服务器内部的数据接收、信号处理和数据处理流程保持不变，依然遵循零拷贝、事件驱动的原则。`Trace ID`在内部完整传递。
2.  **数据网关处理 (11-13)**:
      * `DisplayController`（数据网关）从内部缓冲区获取最终的`TrackData`。
      * **关键步骤**: 它将C++对象**序列化**为Protobuf字节流，`TraceID`也包含其中。
      * 通过UDP Socket将字节流发送到网络。
3.  **客户端接收与显示 (14-16)**:
      * 独立的显控终端应用通过其`UDP监听器`接收数据包。
      * **关键步骤**: 它对接收到的字节流进行**反序列化**，还原出航迹数据。
      * 最终将数据渲染到屏幕上。

**架构收益**:

  * **物理分离**: 此流程清晰地展示了服务器与客户端的物理分离。
  * **端到端追踪**: `TraceID`从服务器的入口一直传递到客户端，实现了真正的全链路可观测性。
  * **职责明确**: 服务器负责计算密集型任务，客户端专注于数据可视化。

### 7.2 配置热更新完整流程

展示从用户界面发起配置修改到所有相关模块响应完成的端到端配置热更新流程。

```mermaid
sequenceDiagram
    participant User as 用户
    participant ConfigUI as 配置界面
    participant ConfigManager as 配置管理器
    participant EventBus as 系统事件总线
    participant SignalProcessor as 信号处理模块
    participant DataProcessor as 数据处理模块
    participant TaskScheduler as 任务调度器

    Note over User,TaskScheduler: 配置热更新端到端流程

    %% 阶段1: 用户发起配置变更
    User->>ConfigUI: 1. 修改参数: signal_processor.fft_size = 2048
    ConfigUI->>ConfigUI: 2. 生成变更事件 (trace_id: config-xyz789)
    ConfigUI->>EventBus: 3. 发布CONFIG_CHANGE_REQUEST事件

    %% 阶段2: 配置管理器处理
    EventBus->>ConfigManager: 4. 路由配置变更请求
    activate ConfigManager
    ConfigManager->>ConfigManager: 5. 执行通用验证 (YAML语法、键存在性)

    alt 通用验证通过
        ConfigManager->>EventBus: 6a. 发布VALIDATE_CONFIG_CHANGE询问事件
        Note right of ConfigManager: 目标模块: SignalProcessor<br/>trace_id: config-xyz789

        %% 阶段3: 业务模块验证
        EventBus->>SignalProcessor: 7. 转发验证询问
        activate SignalProcessor
        SignalProcessor->>SignalProcessor: 8. 业务验证: fft_size是否为2的幂

        alt 业务验证通过
            SignalProcessor->>EventBus: 9a. 发布VALIDATE_SUCCESS响应
            EventBus->>ConfigManager: 10a. 验证响应收集

            %% 阶段4: 配置应用与持久化
            ConfigManager->>ConfigManager: 11a. 更新内部配置树
            ConfigManager->>ConfigManager: 12a. 原子性持久化到config.yaml
            ConfigManager->>EventBus: 13a. 发布CONFIG_CHANGED确认事件

            %% 阶段5: 模块响应配置变更
            EventBus->>SignalProcessor: 14a. 转发配置变更确认
            activate SignalProcessor
            SignalProcessor->>SignalProcessor: 15a. 应用新的fft_size值
            SignalProcessor->>SignalProcessor: 16a. 重新构建GPU算法流水线
            SignalProcessor->>EventBus: 17a. 发布CONFIG_APPLIED确认
            deactivate SignalProcessor

            EventBus->>DataProcessor: 14b. 如果相关，转发配置变更
            activate DataProcessor
            DataProcessor->>DataProcessor: 15b. 检查是否需要调整关联算法
            DataProcessor->>EventBus: 17b. 发布CONFIG_APPLIED确认
            deactivate DataProcessor

            %% 阶段6: 用户反馈
            EventBus->>ConfigUI: 18. 转发最终结果
            ConfigUI->>User: 19. 显示"配置应用成功"

        else 业务验证失败
            SignalProcessor->>EventBus: 9b. 发布VALIDATE_FAILURE响应
            EventBus->>ConfigManager: 10b. 验证失败收集
            ConfigManager->>EventBus: 13b. 发布CONFIG_CHANGE_REJECTED事件
            EventBus->>ConfigUI: 18b. 转发拒绝结果
            ConfigUI->>User: 19b. 显示"配置验证失败: fft_size必须为2的幂"
        end
        deactivate SignalProcessor

    else 通用验证失败
        ConfigManager->>EventBus: 6b. 发布CONFIG_CHANGE_REJECTED事件
        Note right of ConfigManager: 原因: "参数不存在"或"YAML语法错误"
        EventBus->>ConfigUI: 18c. 转发拒绝结果
        ConfigUI->>User: 19c. 显示"配置格式错误"
    end
    deactivate ConfigManager

    %% 阶段7: 可选的任务调度器监控
    EventBus->>TaskScheduler: 20. 配置变更事件通知 (用于监控)
    TaskScheduler->>TaskScheduler: 21. 记录配置变更历史和系统状态
```

**关键流程说明**：

1. **用户发起阶段（1-3）**：
   - 用户通过配置界面修改参数
   - 界面生成唯一Trace ID用于全程追踪
   - 发布CONFIG_CHANGE_REQUEST事件到系统事件总线

2. **配置验证阶段（4-10）**：
   - ConfigManager执行通用验证（语法、键存在性）
   - 通过验证后，发布VALIDATE_CONFIG_CHANGE询问事件
   - 相关业务模块执行专业验证（如参数范围、业务约束）
   - 验证结果通过事件总线反馈给ConfigManager

3. **配置应用阶段（11-13）**：
   - 所有验证通过后，ConfigManager更新内部配置树
   - 原子性持久化到config.yaml文件
   - 发布CONFIG_CHANGED确认事件

4. **模块响应阶段（14-17）**：
   - 所有相关模块接收配置变更确认
   - 各模块根据自身逻辑应用新配置
   - 可能涉及算法重新初始化、资源重新分配
   - 完成后发布CONFIG_APPLIED确认

5. **用户反馈阶段（18-19）**：
   - 配置界面接收最终结果事件
   - 向用户显示成功或失败信息

**错误处理路径**：
- **通用验证失败**：ConfigManager直接拒绝，返回格式错误信息
- **业务验证失败**：业务模块返回具体失败原因，ConfigManager传递给用户

**性能特征**：
- **热更新延迟**：< 100ms（简单参数）到 < 1秒（复杂重构）
- **原子性保证**：要么全部成功，要么全部回滚
- **零停机更新**：系统在配置更新期间继续运行

### 7.3 故障恢复协调流程

展示从模块故障检测到任务调度器决策和执行恢复的完整故障处理流程。

```mermaid
sequenceDiagram
    participant SignalProcessor as 信号处理模块
    participant EventBus as 系统事件总线
    participant TaskScheduler as 任务调度器
    participant DataReceiver as 数据接收模块
    participant DataProcessor as 数据处理模块
    participant ConfigManager as 配置管理器
    participant MonitoringModule as 监控模块

    Note over SignalProcessor,MonitoringModule: 故障检测与协调恢复流程

    %% 阶段1: 故障发生与检测
    SignalProcessor->>SignalProcessor: 1. 检测到GPU计算超时错误
    SignalProcessor->>SignalProcessor: 2. 生成故障上下文 (trace_id: fault-def456)
    Note right of SignalProcessor: 错误类型: CUDA_ERROR_LAUNCH_TIMEOUT<br/>影响: 算法流水线中断

    %% 阶段2: 故障上报
    SignalProcessor->>SignalProcessor: 3. 设置模块状态: FAILED
    SignalProcessor->>EventBus: 4. 发布MODULE_FATAL_ERROR事件
    Note right of SignalProcessor: 包含: 模块ID, 错误码, 详细描述, trace_id

    %% 阶段3: 任务调度器故障分析
    EventBus->>TaskScheduler: 5. 路由故障事件到调度器
    activate TaskScheduler
    TaskScheduler->>TaskScheduler: 6. 分析故障类型和影响范围
    Note right of TaskScheduler: 决策逻辑:<br/>- GPU超时 → 可重启恢复<br/>- 影响下游: DataProcessor

    %% 阶段4: 协调式恢复策略执行
    TaskScheduler->>EventBus: 7. 发布PAUSE_DATA_FLOW命令
    EventBus->>DataReceiver: 8a. 暂停向信号处理模块推送数据
    activate DataReceiver
    DataReceiver->>DataReceiver: 9a. 停止数据推送，缓存后续数据
    EventBus->>DataProcessor: 8b. 暂停处理，等待上游恢复
    activate DataProcessor
    DataProcessor->>DataProcessor: 9b. 完成当前任务，进入等待状态

    %% 阶段5: 模块重启与资源重建
    TaskScheduler->>SignalProcessor: 10. 调用模块stop()接口
    SignalProcessor->>SignalProcessor: 11. 清理GPU资源，释放CUDA上下文
    SignalProcessor->>TaskScheduler: 12. 确认停止完成

    TaskScheduler->>ConfigManager: 13. 获取SignalProcessor配置
    ConfigManager-->>TaskScheduler: 14. 返回当前有效配置

    TaskScheduler->>SignalProcessor: 15. 调用模块initialize(config)
    SignalProcessor->>SignalProcessor: 16. 重新创建GPU上下文和算法流水线
    SignalProcessor->>TaskScheduler: 17. 确认初始化完成

    TaskScheduler->>SignalProcessor: 18. 调用模块start()接口
    SignalProcessor->>SignalProcessor: 19. 恢复运行状态
    SignalProcessor->>TaskScheduler: 20. 确认启动完成，状态: RUNNING

    %% 阶段6: 数据流恢复
    TaskScheduler->>EventBus: 21. 发布RESUME_DATA_FLOW命令
    EventBus->>DataReceiver: 22a. 恢复数据推送
    DataReceiver->>DataReceiver: 23a. 释放缓存数据，恢复正常推送
    deactivate DataReceiver
    EventBus->>DataProcessor: 22b. 恢复数据处理
    DataProcessor->>DataProcessor: 23b. 恢复处理循环
    deactivate DataProcessor

    %% 阶段7: 恢复验证与监控
    TaskScheduler->>MonitoringModule: 24. 请求恢复后系统状态检查
    MonitoringModule->>MonitoringModule: 25. 收集各模块状态和性能指标
    MonitoringModule->>TaskScheduler: 26. 报告系统恢复正常

    TaskScheduler->>EventBus: 27. 发布RECOVERY_COMPLETED事件
    Note right of TaskScheduler: 包含: 恢复时长, 受影响模块, trace_id

    deactivate TaskScheduler

    %% 阶段8: 系统状态确认
    EventBus->>SignalProcessor: 28. 恢复完成通知
    SignalProcessor->>SignalProcessor: 29. 记录恢复日志，清除故障标记
    EventBus->>DataReceiver: 30. 向所有相关模块广播恢复确认
    EventBus->>DataProcessor: 31. 确保数据流恢复正常
```

**关键流程说明**：

1. **故障检测阶段（1-4）**：
   - SignalProcessor检测到GPU计算超时（CUDA_ERROR_LAUNCH_TIMEOUT）
   - 生成详细故障上下文，包含Trace ID用于追踪
   - 设置模块状态为FAILED，发布MODULE_FATAL_ERROR事件

2. **故障分析阶段（5-6）**：
   - TaskScheduler接收故障事件，分析错误类型和影响范围
   - 基于预设策略决定恢复方案：重启模块而非整个系统

3. **协调暂停阶段（7-9）**：
   - 发布PAUSE_DATA_FLOW命令，协调相关模块暂停
   - DataReceiver停止向故障模块推送数据，开始缓存
   - DataProcessor完成当前任务后进入等待状态

4. **模块重建阶段（10-20）**：
   - 调用SignalProcessor的完整生命周期：stop → initialize → start
   - 重新创建GPU上下文和算法流水线
   - 确认模块恢复到RUNNING状态

5. **数据流恢复阶段（21-23）**：
   - 发布RESUME_DATA_FLOW命令，恢复系统数据流
   - DataReceiver释放缓存数据，恢复正常推送
   - DataProcessor恢复处理循环

6. **恢复验证阶段（24-27）**：
   - MonitoringModule检查系统状态和性能指标
   - 确认恢复成功后，发布RECOVERY_COMPLETED事件

7. **状态确认阶段（28-31）**：
   - 向所有相关模块广播恢复确认
   - 各模块清除故障相关状态，恢复正常运行

**故障处理特点**：
- **精确隔离**：只重启故障模块，其他模块继续运行
- **数据保护**：通过缓存机制避免数据丢失
- **协调恢复**：确保上下游模块的状态一致性
- **完整追踪**：Trace ID贯穿整个恢复流程

**性能影响**：
- **恢复时间**：5-10秒（取决于模块复杂度）
- **数据丢失**：0（通过缓存机制保护）
- **系统可用性**：部分降级但核心功能维持

### 7.4 GPU资源协调流程

展示数据网关模块与计算密集型模块之间的抢占式GPU资源协调机制，防止GPU超载导致的系统响应问题。

```mermaid
sequenceDiagram
    participant Monitor as 监控模块
    participant DisplayController as 数据网关模块
    participant TaskScheduler as 任务调度器
    participant SignalProcessor as 信号处理模块
    participant DataReceiver as 数据接收模块
    participant GpuMonitor as GPU监控器

    Note over Monitor,GpuMonitor: 服务器端GPU资源协调流程

    %% 阶段1: 正常运行状态
    Monitor->>GpuMonitor: 1. 定期监控GPU使用率
    activate Monitor
    GpuMonitor-->>Monitor: 2. 当前GPU使用率: 92% (超过阈值85%)

    %% 阶段2: 资源冲突检测
    Monitor->>Monitor: 3. 检测到GPU资源紧张
    Note right of Monitor: 风险: GPU超载可能导致<br/>处理延迟或系统不稳定

    Monitor->>TaskScheduler: 4. 发送SET_COMPUTE_PRIORITY(LOW)事件
    Note right of Monitor: trace_id: gpu-coord-abc123<br/>原因: GPU_OVERLOAD

    %% 阶段3: 抢占式优先级调度
    activate TaskScheduler
    TaskScheduler->>TaskScheduler: 5. 执行强制性优先级调度决策
    Note right of TaskScheduler: 策略: 保证系统稳定性优先<br/>计算任务适当降级避免GPU超载

    par 并行向所有GPU消费模块广播
        TaskScheduler->>SignalProcessor: 6a. SET_COMPUTE_PRIORITY(LOW)
        TaskScheduler->>DataReceiver: 6b. SET_COMPUTE_PRIORITY(LOW)
    end

    %% 阶段4: 模块响应优先级变更
    SignalProcessor->>SignalProcessor: 7a. 中断当前CUDA Kernel执行
    activate SignalProcessor
    SignalProcessor->>SignalProcessor: 8a. 切换到LowPriorityComputeStream
    SignalProcessor->>SignalProcessor: 10a. 启用简化算法路径
    SignalProcessor->>TaskScheduler: 11a. 确认优先级切换完成

    DataReceiver->>DataReceiver: 8b. 检查当前GPU任务状态
    activate DataReceiver
    DataReceiver->>DataReceiver: 9b. 切换到低优先级数据预处理
    DataReceiver->>DataReceiver: 10b. 减少GPU内存带宽使用
    DataReceiver->>TaskScheduler: 11b. 确认优先级切换完成

    %% 阶段5: GPU资源释放生效
    TaskScheduler->>DisplayController: 12. 优先级调度完成通知
    deactivate TaskScheduler
    DisplayController->>GpuMonitor: 13. 重新查询GPU使用率
    GpuMonitor-->>DisplayController: 14. 当前GPU使用率: 58% (已降低)

    DisplayController->>DisplayController: 15. GPU资源充足，UI渲染流畅
    DisplayController->>User: 16. 界面操作响应恢复正常
    deactivate DisplayController

    Note over User,GpuMonitor: === 一段时间后，用户操作减少 ===

    %% 阶段6: 资源需求降低，恢复计算优先级
    DisplayController->>GpuMonitor: 17. 定期检查GPU使用率
    GpuMonitor-->>DisplayController: 18. 当前GPU使用率: 45% (低于恢复阈值60%)

    DisplayController->>DisplayController: 19. 判断可以恢复计算模块优先级
    DisplayController->>TaskScheduler: 20. 发送SET_COMPUTE_PRIORITY(HIGH)事件
    Note right of DisplayController: trace_id: gpu-coord-abc123<br/>原因: GPU_LOAD_NORMALIZED

    activate TaskScheduler
    TaskScheduler->>TaskScheduler: 21. 执行优先级恢复调度
    par 并行向所有GPU消费模块广播恢复
        TaskScheduler->>SignalProcessor: 22a. SET_COMPUTE_PRIORITY(HIGH)
        TaskScheduler->>DataReceiver: 22b. SET_COMPUTE_PRIORITY(HIGH)
    end

    %% 阶段7: 模块恢复高优先级
    SignalProcessor->>SignalProcessor: 23a. 切换回HighPriorityComputeStream
    SignalProcessor->>SignalProcessor: 24a. 恢复完整算法流水线
    SignalProcessor->>SignalProcessor: 25a. 最大化计算吞吐量
    SignalProcessor->>TaskScheduler: 26a. 确认高优先级恢复

    DataReceiver->>DataReceiver: 23b. 恢复高优先级数据预处理
    DataReceiver->>DataReceiver: 24b. 提升GPU内存带宽利用率
    DataReceiver->>TaskScheduler: 26b. 确认高优先级恢复

    deactivate SignalProcessor
    deactivate DataReceiver

    TaskScheduler->>DisplayController: 27. 优先级恢复完成通知
    deactivate TaskScheduler
    DisplayController->>DisplayController: 28. 记录GPU协调历史
    Note right of DisplayController: 协调周期: 完整的降级→恢复循环<br/>有效防止了GPU驱动TDR

    Note over User,GpuMonitor: 系统恢复到最优性能状态，各模块均以最高效率运行
```

**关键流程说明**：

1. **资源冲突检测阶段（1-5）**：
   - 用户执行复杂界面操作，GPU使用率飙升至92%
   - DisplayController检测到GPU资源紧张（超过85%阈值）
   - 主动发送SET_COMPUTE_PRIORITY(LOW)事件请求降级

2. **抢占式调度阶段（6-7）**：
   - TaskScheduler执行强制性优先级调度决策
   - 并行向所有GPU消费模块广播优先级降级命令
   - 保证UI实时性优先，计算任务适当降级

3. **模块响应阶段（8-11）**：
   - SignalProcessor中断当前CUDA Kernel，切换到低优先级流
   - DataReceiver切换到低优先级数据预处理模式
   - 各模块确认优先级切换完成

4. **资源释放生效阶段（12-16）**：
   - GPU使用率从92%降至58%，资源压力缓解
   - DisplayController界面渲染恢复流畅
   - 用户操作响应恢复正常

5. **优先级恢复阶段（17-27）**：
   - GPU使用率降至45%，低于恢复阈值60%
   - DisplayController发送SET_COMPUTE_PRIORITY(HIGH)恢复命令
   - 各模块切换回高优先级流，恢复最大性能

6. **系统优化状态（28）**：
   - 记录完整的协调历史用于分析
   - 系统恢复到最优性能状态

**协调机制特点**：
- **抢占式控制**：强制性优先级切换，不依赖模块主动让出
- **双向调节**：支持降级和恢复的完整周期
- **防TDR保护**：有效防止GPU驱动超时重置
- **性能均衡**：在UI流畅性和计算性能间动态平衡

**系统收益**：
- **用户体验**：确保界面始终流畅响应
- **系统稳定**：避免GPU过载导致的驱动崩溃
- **资源高效**：最大化GPU资源利用率
- **智能调度**：基于实时负载的动态资源分配

### 7.5 序列图使用指南

#### 7.5.1 Trace ID追踪指南

所有端到端序列图都包含**Trace ID传递链**，支持完整的分布式追踪：

**Trace ID格式**：
- **数据处理流**：`trace-abc123`（16字符随机ID）
- **配置更新流**：`config-xyz789`（带前缀标识类型）
- **故障恢复流**：`fault-def456`（便于故障分析）
- **GPU协调流**：`gpu-coord-abc123`（便于性能分析）

**日志查询示例**：
```bash
# 查询完整数据处理链路
grep "trace-abc123" /var/log/radar/*.log

# 查询配置更新过程
grep "config-xyz789" /var/log/radar/*.log

# 查询故障恢复过程
grep "fault-def456" /var/log/radar/*.log
```

**性能分析应用**：
- **端到端延迟**：通过Trace ID统计各阶段耗时
- **瓶颈识别**：定位处理链路中的性能瓶颈
- **错误定位**：快速定位跨模块的错误传播路径

#### 7.5.2 扩展与定制指南

**新增端到端场景**：
1. **识别关键业务流程**：选择涉及3个以上模块的复杂交互
2. **绘制序列图**：使用Mermaid格式，遵循现有样式
3. **添加说明文档**：提供流程说明和关键特征描述
4. **集成Trace ID**：确保支持端到端追踪

**序列图维护原则**：
- **同步更新**：模块接口变更时同步更新序列图
- **版本控制**：序列图纳入文档版本管理
- **审查机制**：架构变更必须更新相关序列

---

## 8 部署与发布

### 8.1 部署架构
系统部署采用分层架构，支持灵活的部署配置：

#### 部署层次架构

| 层次                   | 组件     | 主要功能           | 部署要求                |
| ---------------------- | -------- | ------------------ | ----------------------- |
| **第1层 - 基础设施层** | 硬件平台 | 计算和存储资源     | 高性能服务器，GPU支持   |
|                        | 操作系统 | 系统底层支撑       | Linux/Windows，稳定版本 |
| **第2层 - 中间件层**   | 消息队列 | 异步消息处理       | 高吞吐，持久化          |
|                        | 缓存系统 | 数据缓存加速       | 内存型，高速访问        |
| **第3层 - 应用层**     | Web服务  | 用户界面和API服务  | 负载均衡，高并发        |
|                        | 应用服务 | 核心业务逻辑处理   | 高性能，可扩展          |
| **第4层 - 监控层**     | 日志系统 | 系统日志收集和分析 | 高可用，大存储          |
|                        | 监控系统 | 实时监控和告警     | 7x24小时运行            |

**部署说明**：系统部署就像建造大楼一样分为四层。最底层是基础设施（硬件和操作系统），就像大楼的地基；第二层是中间件（消息队列和缓存），就像大楼的水电管道；第三层是应用服务，就像各个功能房间；最顶层是监控系统，就像大楼的安保和监控设备。每一层都为上层提供支撑，形成稳定的整体架构。

### 8.2 发布流程
系统发布采用标准化的CI/CD流程，确保发布的质量和可靠性：
```mermaid
flowchart TD
    START[代码提交] --> BUILD[自动构建]
    BUILD --> TEST[运行测试]
    TEST --> SCAN[安全扫描]
    SCAN --> APPROVE{审批通过?}

    APPROVE -->|通过| STAGING[预发布环境]
    APPROVE -->|不通过| FIX[修复问题]

    STAGING --> UAT[用户验收测试]
    UAT --> PROD[生产环境部署]
    PROD --> CHECK[部署后检查]
    CHECK --> SUCCESS[发布成功]

    FIX --> START

    classDef process fill:#e8f4ff,stroke:#0066cc,stroke-width:2px
    classDef test fill:#e8ffe8,stroke:#00aa00,stroke-width:2px
    classDef decision fill:#fff0e8,stroke:#ff6600,stroke-width:2px
    classDef success fill:#f0e8ff,stroke:#6600cc,stroke-width:2px
    classDef error fill:#ffe8e8,stroke:#cc0000,stroke-width:2px

    class START,BUILD,STAGING,PROD,CHECK process
    class TEST,SCAN,UAT test
    class APPROVE decision
    class SUCCESS success
    class FIX error
```
**流程说明**：发布流程就像产品出厂检验一样严格。从代码提交开始，经过自动构建、测试和安全检查，需要通过审批才能进入预发布环境。在预发布环境通过用户验收测试后，才能部署到正式的生产环境。部署完成后还要进行检查确认。如果任何环节发现问题，都要返回修复后重新开始。这样确保每次发布都是高质量的。

### 8.3 回滚策略
系统提供多种回滚策略，确保发布失败时的快速恢复：

#### 回滚策略体系

| 策略类型     | 触发方式     | 具体场景                 | 响应时间 |
| ------------ | ------------ | ------------------------ | -------- |
| **自动触发** | 健康检查失败 | 系统响应超时、服务不可用 | < 30秒   |
|              | 性能指标异常 | CPU/内存使用率过高       | < 60秒   |
| **手动触发** | 用户反馈问题 | 功能异常、界面错误       | < 5分钟  |
|              | 业务影响评估 | 业务指标下降、用户投诉   | < 10分钟 |

#### 回滚机制

| 回滚方式     | 技术原理           | 适用场景     | 恢复时间 |
| ------------ | ------------------ | ------------ | -------- |
| **蓝绿切换** | 两套环境快速切换   | 完整版本回滚 | < 2分钟  |
| **版本回退** | 代码版本回退重部署 | 代码层面问题 | < 10分钟 |

#### 验证步骤

| 验证类型           | 检查内容           | 验证标准         | 通过条件       |
| ------------------ | ------------------ | ---------------- | -------------- |
| **功能验证**       | 核心功能可用性     | 主要业务流程正常 | 100%功能可用   |
| **数据完整性检查** | 数据一致性和完整性 | 无数据丢失或损坏 | 数据一致性100% |

**策略说明**：回滚策略就像应急预案一样重要。系统既可以根据监控指标自动触发回滚（比如发现系统异常），也可以根据人工判断手动触发（比如用户反馈问题）。回滚的方式有两种：蓝绿切换就像有两套设备，随时可以切换；版本回退就像时光倒流，回到之前稳定的版本。回滚完成后还要验证系统功能和数据完整性，确保恢复成功。

---

## 9 模块约束说明
**功能约束**：
- MVP阶段支持7个核心模块的集成
- 模块间通信必须通过标准接口
- 数据交换必须使用统一格式
- 生命周期管理必须由任务调度器统一控制
**性能约束**：
- 模块间通信延迟不超过5ms
- 环形缓冲区容量不超过100MB
- 系统启动时间不超过30秒
- 模块状态转换时间不超过1秒
**技术约束**：
- 所有模块必须实现IModule接口
- 必须使用项目统一的错误处理框架
- 必须支持YAML配置文件
- 必须集成项目统一的日志系统
**扩展约束**：
- 支持模块的热插拔和动态加载
- 预留外部系统集成接口
- 支持分布式部署扩展
- 支持自定义模块和插件

---

## 10 相关文档
- [数据接收模块设计](01_数据接收模块设计.md)
- [信号处理模块设计](02_信号处理模块设计.md)
- [数据处理模块设计](03_数据处理模块设计.md)
- [04_数据网关模块设计](04_数据网关模块设计.md)
- [任务调度器设计](05_任务调度器设计.md)
- [配置管理模块设计](06_配置管理模块设计.md)
- [日志服务设计](07_日志服务设计.md)
- [MVP系统设计文档](../MVP系统设计文档.md)

---

## 11 变更历史

| 版本   | 日期       | 作者  | 变更描述                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| ------ | ---------- | ----- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| v3.3.0 | 2025-09-26 | Klein | **【重大功能】端到端场景设计章节**：新增第7章"端到端关键场景设计"，提供系统级宏观交互视图。包含：<br/>(1) **Happy Path数据处理流程**：从UDP包到屏幕渲染的完整链路；<br/>(2) **配置热更新流程**：用户发起到模块响应的完整生命周期；<br/>(3) **故障恢复协调流程**：从故障检测到系统恢复的智能化处理；<br/>(4) **GPU资源协调流程**：抢占式GPU资源调度机制；<br/>**收益**：极大加速复杂流程理解，提供宝贵的培训和调试材料，补强系统级文档视图，是理解跨模块协作的关键参考。                  |
| v3.2.0 | 2025-09-26 | Klein | **【重大改进】熔断器模式引入**：基于架构评审建议，新增4.2.1熔断器模式设计，解决"恢复风暴"问题。包含：<br/>(1) CircuitBreakerManager实现，支持故障计数、时间窗口和指数退避<br />(2) 三状态熔断器(CLOSED/OPEN/HALF_OPEN)；<br/>(3) 探测性恢复机制；<br />(4) 双重防护策略(系统状态+熔断器)；<br />(5) 运维友好的人工介入时间窗口。<br />**收益**：防止无意义重试消耗资源，减少日志噪音，为运维排查争取时间，保护系统整体稳定性。该改进显著增强了系统的自我保护能力和故障恢复的智能化程度。 |
| v3.1.0 | 2025-09-26 | Klein | **接口隔离原则深度优化**：扩展ISP原则说明和收益描述，新增完整的模块实现示例（DataReceiver、MathProcessor、LogService），添加接口选择指导原则表格，强化基于角色的接口体系设计理念，确保模块实现的灵活性和代码意图的清晰性。                                                                                                                                                                                                                                                               |
| v3.0.0 | 2025-01-20 | Klein | **基于集成策略问题汇总v3.0.0补充分析进行深度架构修复**：重新定位ConfigManager为基础服务、实现接口隔离原则、修复TraceID异步传播、统一推送数据流、完善错误恢复协调、规范EventBus实现约束、优化任务调度策略、强化监控集成、统一模块生命周期管理                                                                                                                                                                                                                                             |
| v2.1.0 | 2025-09-26 | Klein | **基于架构评审问题汇总进行重大修复**：修正模块角色分类错误、重写依赖注入模式为构造函数注入、同步配置热更新流程、补充关键接口定义、重新定义文档职责聚焦集成策略                                                                                                                                                                                                                                                                                                                           |
| v2.0.0 | 2025-09-25 | Klein | 基于最新模块设计全面重构，解决架构冲突、统一事件驱动模型、修复接口不一致问题                                                                                                                                                                                                                                                                                                                                                                                                             |
| v1.2.0 | 2025-09-23 | Klein | 优化Mermaid图表布局，将部分复杂图表改为更直观的表格形式，修正依赖图显示顺序，提升文档可读性                                                                                                                                                                                                                                                                                                                                                                                              |
| v1.1.0 | 2025-09-23 | Klein | 重构所有Mermaid图表，优化图表结构和线条分类，简化文字说明使其更通俗易懂，提升文档可读性和理解性                                                                                                                                                                                                                                                                                                                                                                                          |
| v1.0.0 | 2025-09-23 | Klein | 基于MVP设计文档和各模块设计文档创建模块集成策略设计，包含完整的集成架构、接口规范、依赖管理和测试方案                                                                                                                                                                                                                                                                                                                                                                                    |

---

---

*本模块集成策略设计基于事件驱动架构，为雷达数据处理系统提供统一的模块协调机制。通过ExecutionEngine模式、服务依赖注入、事件总线等核心设计，确保各模块能够高效、可靠地集成运行，实现系统的整体功能和性能目标。该设计与项目的各模块设计保持完全一致，支持MVP阶段的快速迭代和未来的扩展需求。*
