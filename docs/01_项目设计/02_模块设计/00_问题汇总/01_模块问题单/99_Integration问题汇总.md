好的，遵照您的指示，我将扮演一名资深系统架构师的角色，以《07_日志监控问题汇总.md》为模板，对《99_模块# 模块集成策略 - 问题汇总

- **标题**: 模块集成策略问题汇总
- **当前版本**: v2.0.0
- **最后更新**: 2025-09-25
- **负责人**: Kelin

---

## 演进摘要 (Evolution Summary)

本文档记录了模块集成策略设计在历次架构审查中发现的关键问题。该文档作为系统级的集成蓝图，定义了各模块间的协调机制、通信协议和系统级的错误处理策略。主要审查阶段包括：

- **v2.0.0 审查 (2025-09-25)**: 架构评审，发现10个关键问题，涉及架构角色不一致、依赖注入模式过时、配置热更新冲突等核心集成问题
- **v3.0.0 补充分析 (2025-09-25)**: 深度架构分析，发现9个更深层次的潜在风险，涉及边界模糊、隐性耦合、时序依赖等架构性问题

---

## 快速导航

- [审查 v2.0.0 (2025-09-25)](#审查-v200-2025-09-25)
- [审查 v3.0.0 补充分析 (2025-09-25)](#审查-v300-补充分析-2025-09-25)

---

## 审查 v2.0.0 (2025-09-25)

| 问题ID     | 问题描述                                                 | 所在章节         | 影响分析                                                       | 修复建议                                   |
| :--------- | :------------------------------------------------------- | :--------------- | :------------------------------------------------------------- | :----------------------------------------- |
| **INT-01** | **架构角色严重不一致：将"被动服务"错误描述为"执行引擎"** | 2.1 整体架构设计 | **致命影响**。该图表严重误导了开发者对系统核心组件角色的理解。 | 立即修正图表，明确标注服务与执行模块的区别 |

---

## 审查 v3.0.0 补充分析 (2025-09-25).md》这份核心文档进行一次全面的架构评审。

我将仔细比对集成策略文档与您提供的8个最新模块设计文档（数据接收、信号处理、数据处理、显控、任务调度、配置管理、日志服务、监控服务），从**架构耦合、设计语言、文档结构、设计缺陷**等多个维度，识别并汇总其中存在的问题。

---

### **《99_模块集成策略设计》问题汇总**

**文档版本**: v2.0.0
**评审日期**: 2025-09-25
**评审人**: 架构评审组

---

### **1. 核心架构与设计语言问题 (Critical)**

| 问题ID     | 问题描述                                                 | 所在章节              | 影响分析                                                                                                                                                                                                                                                                                                                                                                                                                                         | 修复建议                                                                                                                                                                                                                                                                                                         |
| :--------- | :------------------------------------------------------- | :-------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **INT-01** | **架构角色严重不一致：将“被动服务”错误描述为“执行引擎”** | 2.1 整体架构设计      | **致命影响**。该图表严重误导了开发者对系统核心组件角色的理解。<br>1. **日志服务 (`LoggingService`)** 在 07_日志服务设计.md 中明确定义为**被动基础服务**，但在集成文档的架构图中被错误地标记为 `执行引擎`。<br>2. **显控模块 (`DisplayController`)** 在 04_显控接口模块设计.md 中定义为Qt事件循环驱动的UI模块，同样被错误标记为 `执行引擎`。<br>这混淆了项目中“主动执行引擎模块”和“被动服务/UI模块”的核心区别，可能导致错误的集成实现和维护困难。 | 1. **立即修正图表**：在Mermaid图的classDef中，将 `LOG_ENGINE` 和 `DISPLAY` 从 `engine` 或 `business` 类别中移出，并为其定义新的、准确的类别（如 `service`, `ui_module`）。<br>2. **更新图表注释**：明确标注 `LoggingService` 是被动服务，`DisplayController` 是UI模块，它们不遵循主动的 `ExecutionEngine` 模式。 |
| **INT-02** | **依赖注入模式描述过时且不准确**                         | 2.4 服务依赖注入模式  | **高风险**。该章节描述的依赖注入模式与所有新模块设计（如 07_日志服务设计.md）完全冲突。<br>1. **过时的注册中心**: 文档描述了一个独立的 `ServiceRegistry` 组件。而所有新模块设计均采用**在 `main` 函数中显式创建服务实例，并通过构造函数直接注入**的方式，根本不存在 `ServiceRegistry` 这个组件。<br>2. **误导的实现**: 提供的序列图和说明会引导开发者去实现一个已被废弃的、不必要的服务定位器模式，增加了系统复杂度和耦合风险。                  | 1. **彻底重写章节**：删除所有关于 `ServiceRegistry` 的内容。<br>2. **提供正确示例**：根据 07_日志服务设计.md 或 08_监控服务设计.md 的 `main.cpp` 示例，重写依赖注入流程说明和序列图，正确展示在 `main` 函数中创建服务并将其作为 `std::shared_ptr<ILogger>` 等接口注入到其他模块构造函数中的过程。                |
| **INT-03** | **模块交互模式描述不一致**                               | 2.3 模块交互模式 (旧) | **高风险**。该章节的标题和内容存在重复和矛盾，特别是关于服务交互的描述。<br>1. **章节重复**: 文档中出现了两个 `2.3` 和两个 `2.4` 章节，导致结构混乱。<br>2. **描述不一致**: 第一个 `2.3` 章节（模块交互模式）的表格中，将“服务提供者模式”列为服务交互的一种。而紧接着的 `2.4` 章节（基础服务集成方案）又详细描述了服务提供者模式。这种重复和不一致的表述令人困惑。                                                                               | 1. **合并与精简**: 将两个 `2.3` 和两个 `2.4` 章节合并，修正编号。<br>2. **统一描述**: 将 `2.4` 章节关于服务提供者模式的详细描述，作为对 `2.3` 表格中“服务交互”模式的补充说明，而不是一个独立的设计。确保对“服务提供者模式”的描述与 **INT-02** 的修复建议（即基于构造函数注入）保持一致。                         |
| **INT-04** | **数据流与控制流描述不准确**                             | 2.1 整体架构设计      | **中风险**。架构图中的数据流描述过于简化，与模块的实际零拷贝设计不符。<br>1. **模糊的缓冲区**: 图中仅用“缓冲区”笼统表示，但 01_数据接收模块设计.md 和 02_信号处理模块设计.md 明确定义了基于**页锁定内存池**和**指针队列**的复杂零拷贝机制。<br>2. **数据流向误导**: `DATA_RECV ==> BUFFER1` 的箭头掩盖了“内存块申请与归还”的双向交互，这对于理解和实现零拷贝至关重要。                                                                           | 1. **细化图例**: 在图表下方增加注释，说明“数据通道层”是通过**指针队列**和**共享内存池**实现的零拷贝机制。<br>2. **修正数据流描述**: 在文字说明中，强调数据流是**数据对象指针**的流动，而原始数据存储在共享内存池中，并存在内存块的申请与归还流程。                                                               |

---

### **2. 文档结构与内容质量问题 (Medium)**

| 问题ID     | 问题描述                          | 所在章节          | 影响分析                                                                                                                                                                                                                                                           | 修复建议                                                                                                                                                                                                                                                                                                                                                                                                                 |
| :--------- | :-------------------------------- | :---------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **INT-05** | **文档职责范围与实际内容不符**    | 1.1 文档职责      | **中风险**。文档职责列表中包含了大量与“集成策略”无关的、应属于事件总线专项设计的内容。<br>例如，“异步事件处理”、“事件过滤机制”、“故障隔离”等都是事件总线（EventBus）自身的特性，而非模块间的“集成策略”。这使得本文档的定位变得模糊，像是一份不完整的事件总线设计。 | 1. **重新聚焦职责**: 将文档职责的核心重新定位在“**事件驱动架构下的模块协调与系统级集成**”上。<br>2. **调整职责列表**: 将列表项修改为更能体现集成策略的条目，例如：<br>   - 基于统一事件总线的模块协调机制<br>   - 遵循项目接口规范的模块间通信协议<br>   - 基于ExecutionEngine的模块依赖解析策略<br>   - 事件驱动的模块状态管理和启停协调<br>   - 智能化的系统级异常处理和恢复策略<br>   - 基于TraceID的端到端监控和追踪 |
| **INT-06** | **接口规范不完整，缺少关键接口**  | 3. 标准化接口规范 | **中风险**。接口规范章节缺少了依赖注入模式下最关键的 `IServiceRegistry` 或类似的服务提供者接口定义，导致 `2.4` 节的设计成了空中楼阁。<br>同时，也缺少了对 `IEventBus` 接口的定义，这对于理解事件驱动架构至关重要。                                                 | 1. **补充 `IServiceRegistry` 接口**: 即使最终实现是构造函数注入，也应在接口规范中定义一个抽象的服务提供者接口，以体现“面向接口编程”的原则。<br>2. **补充 `IEventBus` 接口**: 在 `3.3 事件接口规范` 中，增加 `IEventBus` 的接口定义，包含 `publish`, `subscribe`, `unsubscribe` 等核心方法。                                                                                                                              |
| **INT-07** | **Mermaid图表可读性差，信息过载** | 全文多处          | **低风险**。部分图表（如 `2.1` 和 `2.2`）虽然语法正确，但布局混乱，线条交叉严重，未能清晰地传达架构意图。<br>例如，`2.1` 图中将数据流和事件流混杂在一起，难以区分。                                                                                                | 1. **分离关注点**: 将 `2.1` 的图表拆分为两个独立的图：一个专注于**事件流（控制流）**，展示模块与事件总线的关系；另一个专注于**数据流**，展示数据在缓冲区之间的流向。<br>2. **优化布局**: 使用 `subgraph` 对相关的组件进行分组，调整 `direction`，并使用不同样式的线条（如虚线、实线）来区分不同类型的关系。                                                                                                              |
| **INT-08** | **文档版本与更新日期不合理**      | 文档头部          | **低风险**。文档的“最后更新”日期为 `2025-09-25`，但其内容明显落后于 `2025-09-26` 更新的 06_配置管理模块设计.md 等文档。这表明文档的版本管理和同步机制存在问题，降低了文档的可信度。                                                                                | 1. **更新版本信息**: 在修复完所有问题后，将“最后更新”日期更新为当前日期。<br>2. **增加变更历史**: 在 `10 变更历史` 中增加一条新的记录，详细说明本次评审所做的重大修复和架构对齐工作。                                                                                                                                                                                                                                    |

---

### **3. 设计细节与实现缺陷问题 (Low)**

| 问题ID     | 问题描述                             | 所在章节             | 影响分析                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | 修复建议                                                                                                                                                                                                                                                                                                          |
| :--------- | :----------------------------------- | :------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **INT-09** | **配置热更新流程与配置模块设计冲突** | 4.3 配置热更新协调   | **中风险**。该章节描述的配置热更新流程与 06_配置管理模块设计.md 中的最终设计（v2.0.0）存在冲突。<br>1. **决策权归属错误**: 集成文档描述模块可以自主决策“热更新”或“请求重启”。而 06_配置管理模块设计.md (6.1节) 明确指出，**配置模块只发布 `CONFIG_CHANGED` 事件，决策权完全在订阅该事件的业务模块侧**。<br>2. **事件类型过时**: 集成文档中提到了 `ConfigChangeRequestEvent`, `ConfigAppliedEvent`, `RestartRequiredEvent` 等事件，这些在新版配置模块设计中已被废弃，统一为更通用的 `CONFIG_CHANGED` 事件。 | 1. **同步热更新流程**: 完全按照 06_配置管理模块设计.md (6.1.3节) 的事件驱动流程图和说明，重绘序列图并重写说明文字。<br>2. **强调模块自主决策**: 明确指出，是业务模块在收到 `CONFIG_CHANGED` 事件后，根据自身逻辑判断是直接应用、请求`TaskScheduler`重载，还是上报致命错误请求重启。`ConfigManager` 不参与此决策。 |
| **INT-10** | **生命周期协调描述不清晰**           | 4.1 事件驱动启动流程 | **低风险**。启动流程图虽然大致正确，但未能清晰体现模块间的依赖关系和启动顺序。<br>例如，图中 `DataReceiver` 的启动是独立的，但实际上它可能依赖于某些基础服务已经就绪。                                                                                                                                                                                                                                                                                                                                     | 1. **引入依赖图**: 在文字说明中增加对“模块依赖图”的引用，说明 `TaskScheduler` 会根据此图来决定启动顺序。<br>2. **优化序列图**: 在序列图中增加一个 `Note`，说明 `TaskScheduler` 在发布 `StartModuleEvent` 之前，会先进行依赖解析，确保前置模块已就绪。                                                             |

---
**总结**:

《99_模块集成策略设计》作为一份高层设计文档，其核心问题在于**未能与最新的、经过重构的各模块设计文档保持同步**。这导致了在架构角色、核心模式（依赖注入、配置热更新）和设计语言上出现了严重的不一致和冲突。

**首要修复任务**是解决 **INT-01** 到 **INT-04** 的核心架构问题，使集成策略与项目统一的“被动服务/主动模块”、“构造函数注入”、“事件驱动决策”等核心原则完全对齐。其次，应根据 **INT-05** 的建议重新明确文档职责，并完善其他结构和细节问题。修复完成后，本文档才能真正成为指导项目集成的有效蓝图。


---

---

### **《99_集成策略问题汇总》补充分析**
**文档版本**: v3.0.0
**评审日期**: 2025-09-25
**评审人**: 架构评审组

您的担忧非常正确，一份好的集成策略是项目成功的基石。除了您可能已经在《99_集成策略问题汇总.md》中识别的问题外，我发现了**9个更深层次的、具有潜在风险的架构性问题**。这些问题大多涉及**边界模糊、隐性耦合、时序依赖和未明确定义的“魔鬼细节”**，如果不提前解决，极有可能在编码和集成阶段导致重大重构。

以下是详细的分析，按照风险和重要性排序：

---

### 1. 致命风险：`TaskScheduler` 与 `ConfigManager` 的双向依赖与“先有鸡还是先有蛋”的启动悖论

这是最严重的问题，它在多个文档的 `main.cpp` 示例中都存在，但集成策略并未明确解决。

- **问题描述**：
    1.  `TaskScheduler` 在构造时需要 `ConfigManager`，因为它需要读取调度策略、模块依赖等配置。
    2.  `ConfigManager` 自身作为一个 `IModule`（根据其设计文档），需要被 `TaskScheduler` 注册和管理生命周期。
    3.  这就形成了一个**致命的构造循环依赖**：`TaskScheduler` 的创建依赖 `ConfigManager`，而 `ConfigManager` 的完整生命周期管理又依赖 `TaskScheduler`。

- **潜在后果**：
    - 无法编译或在运行时因 `shared_ptr` 循环引用导致内存泄漏。
    - 启动顺序混乱，`ConfigManager` 可能在其他模块需要它之前还未完全就绪。
    - 热更新机制变得异常复杂，`TaskScheduler` 如何安全地重载一个管理着自己的模块？

- **底层重构建议**：
    - **将 `ConfigManager` 降级为纯粹的服务**：移除 `ConfigManager` 的 `IModule` 接口继承。它不应该有由 `TaskScheduler` 管理的复杂生命周期（`start`, `stop` 等）。
    - **明确启动顺序**：`main` 函数中，`ConfigManager`、`LoggingService`、`EventBus` 这三大核心服务应**最先被创建和初始化**，且它们的生命周期由 `main` 函数作用域直接管理，而不是 `TaskScheduler`。
    - **修正 `main.cpp` 范例**：
        ```cpp
        int main() {
            // 1. 创建并初始化核心服务 (不由TaskScheduler管理)
            auto config_manager = std::make_shared<ConfigManager>();
            config_manager->initialize("config.yaml"); // 加载文件

            auto logging_service = std::make_shared<LoggingService>(config_manager);
            logging_service->initialize(); // 初始化日志

            auto event_bus = std::make_shared<EventBus>();
            event_bus->initialize();

            // 2. 创建TaskScheduler (注入已就绪的服务)
            auto task_scheduler = std::make_shared<TaskScheduler>(config_manager, event_bus, logging_service);
            task_scheduler->initialize();

            // 3. 创建并注册所有真正的“模块”
            auto data_receiver = std::make_shared<DataReceiver>(...);
            task_scheduler->registerModule(data_receiver);
            // ... 注册其他模块

            // 4. 启动
            task_scheduler->start();

            // ... 运行与关闭 ...
        }
        ```

### 2. 严重风险：事件驱动错误处理的“广播风暴”与“恢复死锁”

集成策略提倡通过事件总线进行错误上报，但未定义关键的协调机制，存在严重缺陷。

- **问题描述**：
    1.  **广播风暴**：当一个核心模块（如 `DataReceiver`）失败并发布 `ModuleFailedEvent` 后，所有依赖它的下游模块（`SignalProcessor`, `DataProcessor`）会因为没有数据而相继超时或出错，它们也会各自发布 `ModuleFailedEvent`。这会瞬间产生一系列连锁的失败事件，形成“广播风暴”，淹没 `TaskScheduler` 并使其难以判断**根本原因**。
    2.  **恢复死锁**：`TaskScheduler` 在尝试恢复 `DataReceiver` 时，下游模块可能仍在不断报告错误，干扰恢复流程。如果恢复策略是“重启依赖链”，可能会导致不必要的、大规模的模块重启，甚至在恢复过程中再次失败，形成恢复循环。

- **底层重构建议**：
    - **引入“系统状态”概念**：`TaskScheduler` 必须维护一个全局的系统状态（如 `RUNNING`, `DEGRADED`, `RECOVERING`）。
    - **明确错误处理协调者**：`TaskScheduler` 是**唯一**的故障恢复协调者。
    - **定义错误处理流程**：
        1.  当 `TaskScheduler` 收到第一个 `ModuleFailedEvent` 时，立即将系统状态切换为 `RECOVERING`。
        2.  在 `RECOVERING` 状态下，`TaskScheduler` **暂时忽略**所有后续的、由连锁反应引起的 `ModuleFailedEvent`。
        3.  `TaskScheduler` 根据依赖图分析根本原因，并执行精确的恢复策略（例如，只重启 `DataReceiver`）。
        4.  在恢复期间，`TaskScheduler` 可以向其他模块发布 `SystemStateChangedEvent(RECOVERING)`，让它们进入“暂停”或“等待”模式，而不是继续报错。
        5.  恢复成功后，`TaskScheduler` 将系统状态切回 `RUNNING`，并通知所有模块恢复正常工作。

### 3. 高风险：`Trace ID` 在异步和跨线程场景下的传递链断裂

所有文档都强调了 `Trace ID` 的重要性，但集成策略对它在复杂场景下的传递方式定义不足。

- **问题描述**：
    - `TraceContext` 基于 `thread_local` 实现，这在同步调用链中工作良好。但在以下场景中会**立即失效**：
        1.  **异步事件处理**：`EventBus` 的一个实现可能是异步的，事件发布线程和处理线程不是同一个，`thread_local` 的 `Trace ID` 会丢失。
        2.  **线程池任务**：当一个模块（如 `DataProcessor`）将一个任务提交给内部的线程池去执行时，工作线程无法自动继承主线程的 `Trace ID`。
        3.  **定时任务**：由 `ExecutionEngine` 的定时器触发的周期性任务（如 `MonitoringModule` 的数据采集），其执行线程没有来自上游请求的 `Trace ID`。

- **底层重构建议**：
    - **强制事件携带 Trace ID**：规定所有通过 `EventBus` 传递的事件结构体，**必须**包含一个 `TraceId trace_id;` 成员。
    - **上下文恢复**：事件处理器在处理事件的**第一行代码**，必须从事件对象中获取 `Trace ID` 并设置到当前线程的 `TraceContext` 中。使用 `TraceContextGuard` (RAII) 来自动完成设置和恢复。
        ```cpp
        void DataProcessor::onNewDataEvent(const NewDataEvent& event) {
            TraceContextGuard guard(event.trace_id); // 关键！
            // ... 后续所有日志和操作都会自动带上正确的Trace ID
        }
        ```
    - **任务分发时显式传递**：向线程池提交任务时，必须将当前的 `Trace ID` 作为参数或捕获到 lambda 中，并在工作线程的起始处用 `TraceContextGuard` 恢复上下文。
    - **为无源任务生成 Trace ID**：对于定时器等无上游请求源的后台任务，应在任务开始时调用 `TraceContext::generateTraceId()` 生成一个新的根 `Trace ID`。

### 4. 高风险：模块间数据流的“推”/“拉”模式不统一

集成策略和各模块设计在数据如何在模块间流动上存在模糊和矛盾。

- **问题描述**：
    - `DataReceiver` 的设计是“推”模式：它将处理好的 `DataObject` 推入下游（`SignalProcessor`）的输入缓冲区。
    - `SignalProcessor` 和 `DataProcessor` 的设计文档中，其 `ExecutionEngine` 又是“拉”模式：它们从上游模块的输出缓冲区拉取数据。
    - 这种“推”与“拉”的混用会导致责任不清、流控困难和潜在的竞态条件。例如，谁来创建和拥有模块间的共享缓冲区？

- **底层重构建议**：
    - **统一为“推”模式（推荐）**：
        - 定义清晰的 `IInputQueue<T>` 和 `IOutputQueue<T>` 接口。
        - 每个模块在构造时，通过依赖注入接收其下游模块的 `IInputQueue<T>` 引用。
        - 模块处理完数据后，直接调用 `downstream_queue->push(data)`。
        - **优点**：数据流向清晰，责任单一，背压机制更直观。
    - **或者统一为“拉”模式**：
        - 每个模块暴露一个 `getOutputQueue()` 方法。
        - 模块在构造时，获取其上游模块的引用，并在主循环中调用 `upstream_module->getOutputQueue()->pop()`。
        - **缺点**：模块间产生更强的耦合，不推荐。
    - **明确缓冲区所有权**：无论推拉，必须明确模块间的共享缓冲区由谁创建、拥有和管理。推荐由 `main` 函数或一个专门的“连接器”来创建，并通过依赖注入分别赋给生产者和消费者。

### 5. 中风险：`IModule` 接口过于臃肿，违反单一职责原则

`IModule` 接口试图涵盖所有类型的模块，导致接口定义过于宽泛和模糊。

- **问题描述**：
    - `IModule` 包含了 `pause`, `resume`, `getPerformanceMetrics` 等方法。
    - 对于 `ConfigManager` 或 `LoggingService` 这样的服务型组件，`pause`/`resume` 语义不明，实现它们没有意义。
    - 对于 `DataReceiver` 这样的纯数据流模块，`getPerformanceMetrics` 应该是被动上报，而不是主动查询。

- **底层重构建议**：
    - **接口分离原则 (ISP)**：将 `IModule` 拆分为更小的、职责更明确的接口。
        - `ILifecycleManaged`: 提供 `initialize`, `start`, `stop`, `cleanup`。所有由 `TaskScheduler` 管理的模块都实现此接口。
        - `IPausable`: 提供 `pause`, `resume`。只有支持暂停/恢复的模块（如数据处理流模块）才实现此接口。
        - `IMonitorable`: 提供 `getHealthStatus` 或其他监控接口。
    - **移除 `getPerformanceMetrics`**：遵循事件驱动原则，模块应**主动**通过 `EventBus` 发布 `PerformanceMetricsUpdatedEvent` 事件，而不是等待外部查询。`MonitoringModule` 订阅这些事件来聚合系统总览。

### 6. 中风险：配置热更新的“应用时机”与“原子性”未定义

`ConfigManager` 的设计文档描述了它如何发布 `CONFIG_CHANGED` 事件，但集成策略没有说明模块应**如何安全地应用**这些变更。

- **问题描述**：
    - **应用时机不当**：如果一个模块在处理一帧数据的过程中收到配置变更事件并立即应用，可能导致该帧数据的前半部分用旧配置处理，后半部分用新配置处理，造成数据不一致或崩溃。
    - **非原子性变更**：如果一个变更涉及多个参数（例如，FFT尺寸和窗函数类型），而模块分别处理这两个参数的变更事件，可能会在某个瞬间处于一个无效的中间配置状态。

- **底层重构建议**：
    - **引入“配置暂存”与“帧边界应用”机制**：
        1.  模块的事件处理器在收到 `CONFIG_CHANGED` 事件时，**不立即应用**，而是将新配置存入一个“待应用”的暂存区。
        2.  模块的 `ExecutionEngine` 在其主循环的**帧边界**（即处理完一帧数据，开始下一帧之前）检查暂存区。
        3.  如果暂存区有新配置，则**原子地**切换配置，并用新配置处理下一帧数据。
    - **批量配置事件**：对于相互依赖的配置变更，`ConfigManager` 应发布一个包含所有相关变更的**单个事件**，而不是多个独立事件，以保证模块能原子地应用一组变更。

### 7. 中风险：对模块依赖关系的静态定义与动态恢复的矛盾

`TaskScheduler` 的设计提到了依赖图，但集成策略没有说明这个依赖图是静态的还是动态的，以及它如何与故障恢复策略互动。

- **问题描述**：
    - 启动和关闭顺序依赖于一个静态的依赖图。
    - 但在运行时，一个模块的故障可能只需要重启它自身，而不需要重启整个依赖链。一个过于简单的、基于静态依赖图的恢复策略（“A失败了，所有依赖A的都重启”）会导致不必要的服务中断。

- **底层重构建议**：
    - **定义两种依赖类型**：
        - **强依赖 (启动/关闭依赖)**：用于 `TaskScheduler` 决定模块的初始化和销毁顺序。
        - **弱依赖 (数据流依赖)**：用于故障恢复时的影响分析。
    - **细化恢复策略**：`RecoveryEngine` 在选择恢复策略时，应考虑故障类型和弱依赖关系。例如：
        - **`DataReceiver` 失败**：需要暂停下游，重启 `DataReceiver`，然后恢复下游。
        - **`DataProcessor` 失败**：可能只需要重启 `DataProcessor` 自身，上游的 `SignalProcessor` 可以继续运行，只需暂时缓存或丢弃输出数据。

### 8. 低风险：对“优雅关闭”的定义过于理想化

所有文档都提到了“优雅关闭”，但集成策略没有定义在模块拒绝停止或超时的情况下的处理方案。

- **问题描述**：
    - `TaskScheduler` 调用一个模块的 `stop()` 方法，但该模块可能因为一个死锁或长时间的I/O操作而卡住。
    - `TaskScheduler` 会无限等待下去吗？这会导致整个系统关闭进程卡死。

- **底层重构建议**：
    - **为 `stop()` 操作引入超时机制**：
        1.  `TaskScheduler` 在调用 `module->stop()` 后，启动一个可配置的超时计时器（例如5秒）。
        2.  如果模块在超时时间内成功停止，流程继续。
        3.  如果超时，`TaskScheduler` 应：
            a.  记录一条 `FATAL` 级别的日志，指明哪个模块停止超时。
            b.  强制中断该模块的线程（如果可能且安全）。
            c.  继续关闭其他模块，不再等待问题模块。
            d.  最后，程序以一个非零的错误码退出，向外部运维系统（如systemd）表明关闭过程异常。

### 9. 低风险：事件总线（EventBus）的实现细节缺失

集成策略依赖 `EventBus` 作为核心通信机制，但对其关键的实现特性（同步/异步、线程模型）没有约束，这会给模块开发者带来困惑和风险。

- **问题描述**：
    - 如果 `EventBus` 是**同步**的，那么发布事件的模块会**被阻塞**，直到所有订阅者都处理完该事件。这会引入意想不到的延迟和耦合。
    - 如果 `EventBus` 是**异步**的，事件处理将在哪个线程上执行？是专用的事件总线线程，还是发布者线程，还是订阅者自己的线程？这直接影响模块的线程安全设计。

- **底层重G构建议**：
    - **明确 `EventBus` 的核心行为**：在集成策略中明确规定 `EventBus` 的实现模型。推荐采用**异步、带工作线程池的模型**。
    - **定义线程模型**：
        - `publish(event)` 方法应是**非阻塞**的，它只是将事件放入一个内部的并发队列中并立即返回。
        - `EventBus` 内部维护一个工作线程池。
        - 当事件出队时，`EventBus` 的工作线程负责调用所有相关订阅者的回调函数。
    - **明确对订阅者的要求**：由于回调函数将在 `EventBus` 的工作线程中执行，所有模块的事件处理器**必须是线程安全的**，并且**不能执行长时间的阻塞操作**。如果需要执行耗时操作，应将其派发到模块自己的工作线程中。

---

**总结**：

这些问题串联起来，揭示了当前设计在**系统启动、错误恢复、异步通信和数据流一致性**这些动态、复杂的交互场景下存在严重的设计不足。

建议您组织一次架构评审会议，将这些问题作为议题，并基于上述建议对**集成策略**和相关的**核心服务（TaskScheduler, ConfigManager, EventBus）**进行一次集中的底层设计重构。这将在编码开始前消除最大的架构风险，确保项目能够平稳、高质量地推进。
