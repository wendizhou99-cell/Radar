
---

### 发现的设计兼容性问题与解决方案

以下是我识别出的主要矛盾点以及建议的解决方案，以确保系统能够完美耦合。

#### 1. **问题：信号处理与数据处理模块的接口不匹配**

*   **矛盾点**:
    *   02_信号处理模块设计.md (输出边界) 定义其输出为泛指的“**处理后的信号数据**”。
    *   03_数据处理模块设计.md (输入边界) 定义其输入为特指的“**CFAR检测结果**”。
*   **潜在风险**: 这是最主要的接口分歧。如果信号处理模块输出的是通用信号特征，而数据处理模块只期望接收CFAR检测点，将导致数据无法衔接。
*   **解决方案**:
    1.  **明确接口契约**: 必须在 99_模块集成策略.md 的数据流设计部分，为**环形缓冲区B**（或2）定义一个精确的数据结构。
    2.  **建议的数据结构**:
        ````cpp
        // 建议在 include/common/types.h 中定义
        struct DetectionResult {
            float range;          // 距离
            float azimuth;        // 方位
            float elevation;     // 俯仰
            float snr;            // 信噪比
            float doppler;        // 多普勒
            Timestamp timestamp;  // 时间戳
        };

        using DetectionBatch = AlignedVector<DetectionResult>; // 使用项目定义的对齐容器
        ````
    3.  **统一文档**:
        *   在 02_信号处理模块设计.md 中，明确其最终输出是 `DetectionBatch`。
        *   在 03_数据处理模块设计.md 中，明确其输入是 `DetectionBatch`。

#### 2. **问题：数据缓冲区命名不统一**

*   **矛盾点**:
    *   02_信号处理模块设计.md 提到了“环形缓冲区A”和“环形缓冲区B”。
    *   04_显控接口模块设计.md 提到了“环形缓冲区C”。
    *   99_模块集成策略.md 的集成架构图中使用的是“缓冲区1”、“缓冲区2”、“缓冲区3”。
*   **潜在风险**: 命名混淆会在代码实现和后续维护中造成误解。
*   **解决方案**:
    *   **全局统一命名**: 建议在 99_模块集成策略.md 中建立一个权威的命名对应表，并在所有模块设计文档中统一更新。
    *   **建议的命名体系**:
        | 逻辑名称 | 物理名称 | 数据流向 | 数据内容 |
        | :--- | :--- | :--- | :--- |
        | `Buffer_RawData` | 缓冲区1 / A | 数据接收 -> 信号处理 | 原始I/Q数据包 |
        | `Buffer_Detections` | 缓冲区2 / B | 信号处理 -> 数据处理 | `DetectionBatch` |
        | `Buffer_Tracks` | 缓冲区3 / C | 数据处理 -> 显控接口 | 目标航迹信息 |

#### 3. **问题：任务调度器与数据驱动模块的协调机制模糊**

*   **矛盾点**:
    *   05_任务调度器设计.md 侧重于模块的生命周期管理（启停、状态转换）和资源分配。
    *   `01`, `02`, `03` 模块的设计是基于数据驱动的流水线模式（上游数据到达后触发下游处理）。
*   **潜在风险**: 任务调度器如何调度一个“永远在等待数据”的模块？是周期性轮询，还是基于事件通知？这在设计中没有明确。
*   **解决方案**:
    *   **明确调度模式**: 在 05_任务调度器设计.md 中增加一节“**模块调度模式**”，明确区分两类模块：
        1.  **事件/数据驱动型模块**: 如数据接收、信号处理、数据处理。调度器主要负责其生命周期和资源（如线程亲和性），其核心 `run()` 循环由内部的I/O事件（如`epoll`）或缓冲区数据状态驱动。
        2.  **周期/命令驱动型模块**: 如显控接口（定时刷新UI）、日志监控（定时采集指标）。调度器可以按固定频率或在特定事件后调用其处理函数。
    *   **更新集成策略**: 在 99_模块集成策略.md 中，用序列图或活动图清晰地展示调度器如何与一个数据驱动模块（如信号处理模块）进行交互，特别是启动后如何将控制权交由模块内部的事件循环。

#### 4. **问题：基础服务模块的集成方式不明确**

*   **矛盾点**:
    *   06_配置管理模块设计.md 和 07_日志监控模块设计.md 定义了强大的服务能力。
    *   但其他业务模块设计中，只是简单提及“使用统一日志/配置框架”，并未说明集成细节。例如，模块启动时，配置是由调度器传入，还是模块自己去`ConfigManager`获取？
*   **潜在风险**: 不一致的集成方式会导致代码混乱和紧耦合。
*   **解决方案**:
    *   **采用依赖注入 (DI)**: 这是解决此类问题的最佳实践。
    *   **明确集成流程**: 在 99_模块集成策略.md 和 05_任务调度器设计.md 中明确以下流程：
        1.  **系统启动时**: `main` 函数首先初始化 `LogManager` 和 `ConfigManager`。
        2.  **模块创建时**: `TaskScheduler` 在创建每个业务模块实例时，通过其构造函数或 `initialize()` 方法，将 `ConfigManager` 和 `LogManager` 的引用（或`shared_ptr`）**注入**到模块中。
        3.  **模块运行时**: 模块内部直接使用被注入的服务实例，而不是通过全局单例 `getInstance()`。这大大提高了模块的可测试性和解耦性。

---

针对问题 我做出以下解答：
1. 问题一：信号处理与数据处理模块的接口不匹配
解答： 针对这个问题，需要定义数据结构，但是我的文档采用的是职责分离的原则，信号处理模块只负责信号处理，数据处理模块只负责数据处理，中间的数据结构不应该在这两个模块中定义，而应该在D:\work\Radar\docs\01_项目设计\04_数据架构中定义，这样职责更加清晰，所有我们只需要在原理上对齐就可以了。
// TODO: 在数据架构中定义数据结构

1. 问题二：数据缓冲区命名不统一
解答： 针对这个问题，我们可以在 99_模块集成策略.md 中建立一个权威的命名对应表，并在所有模块设计文档中统一更新。这样可以避免命名混淆带来的误解。我最开始在[./MVP系统设计文档.md]中定义了缓冲区的命名，前两个是环形缓存区A/B，没有设计数据处理之后的缓冲区C，而是设计了显控上行的命令队列，这是设计上的问题，我在之后会统一缓存区的设计。
// TODO: 缓存区设计统一

1. 问题三：**任务调度器与数据驱动模块的协调机制模糊**
// TODO1: 在 05_任务调度器设计.md 中增加驱动模式的策略化选择
// TODO2: 在 99_模块集成策略.md 中，用序列图或活动图清晰地展示调度器如何与一个数据驱动模块（如信号处理模块）进行交互

```markdown
### 核心矛盾：两种不同的“驱动”模式

为了让您更好地理解，我们用一个简单的比喻：

*   **任务调度器 (Task Scheduler)**：像一个**工厂经理**。他的职责是：
    *   决定何时**开启**或**关闭**某条生产线（模块生命周期管理）。
    *   给生产线分配工人、电力和原材料（资源分配）。
    *   他习惯于下达**命令**：“生产线A，启动！”、“生产线B，停止！”。这是一种**自上而下 (Top-Down)** 的**命令驱动**模式。

*   **数据驱动模块 (Data-Driven Modules)**：像一条**自动化生产线**（例如：数据接收、信号处理）。它的特点是：
    *   它不是持续“工作”的，而是在**等待**一个外部事件——比如，上游的传送带送来一个零件（一个UDP数据包到达）。
    *   一旦零件到达，它就自动运转，处理完零件，然后再次进入等待状态。
    *   这是一种**自下而上 (Bottom-Up)** 的**事件驱动**或**数据驱动**模式。

**“模糊”之处就在于：**

在您当前的 05_任务调度器设计.md 中，调度器通过 `start()`, `stop()`, `run()` 等方法来管理模块。这对于那些需要调度器**主动、反复调用**才能工作的模块（比如每秒更新一次UI）是完美的。

但是，对于“数据接收模块”这样的自动化生产线，当经理（调度器）喊出“启动！” (`start()`) 之后，这条生产线只是进入了“准备接收零件”的等待状态。经理接下来如果反复对它喊“工作！” (`run()`)，是**毫无意义**的，因为生产线只听“零件到达”这个信号。

**所以，关键问题是：`IModule::run()` 方法在数据驱动模块中，到底应该做什么？**

---

### 两种清晰的设计选择

为了解决这个模糊点，您需要做出一个明确的设计选择。以下是两种最常见的解决方案：

#### 选择 1：调度器作为“启动器”（Fire-and-Forget 模式）

这是最高性能、最常见的模式，特别适用于实时系统。

*   **工作原理**:
    1.  调度器调用模块的 `initialize()` 方法，模块准备好所有资源（如创建线程、绑定Socket）。
    2.  调度器调用模块的 `start()` 方法。
    3.  在 `start()` 方法内部，模块会**自己启动一个或多个专用线程**，并在该线程内进入一个**阻塞式的事件循环**（例如，使用 `epoll_wait` 或 `select` 等待网络数据）。
    4.  `start()` 方法会**立即返回**，告诉调度器：“我已经启动了，现在正在后台独立运行。”
    5.  从此，该模块的运行就与调度器的主循环**完全解耦**了。它由数据驱动，自我运转。
    6.  模块的 `run()` 方法在这种模式下**通常是空的，或者根本不被调度器调用**。
    7.  当调度器调用 `stop()` 时，模块会设置一个标志，使其内部的事件循环退出，并清理线程和资源。

*   **优点**:
    *   **最高性能**：没有调度器介入的开销，模块能以最快速度响应数据。
    *   **强解耦**：模块内部逻辑高度自治，符合其数据驱动的本质。
    *   **简单直接**：调度器只负责“启”和“停”，职责清晰。

*   **缺点**:
    *   **控制力减弱**：调度器无法精细控制模块单次处理的行为（但这通常不是数据驱动模块所需要的）。

#### 选择 2：调度器作为“驱动者”（Cooperative Scheduling 模式）

这种模式给予调度器更多的控制权，但性能较低。

*   **工作原理**:
    1.  调度器调用 `initialize()` 和 `start()`，模块准备好资源，但**不创建自己的线程**。
    2.  调度器在其主循环中，**反复调用**模块的 `run()` 方法。
    3.  在模块的 `run()` 方法内部，它会执行一次**非阻塞**的数据检查（例如，使用超时时间为0的 `select` 或 `poll`）。
    4.  如果有数据，就处理一小批数据，然后**立即返回**，将控制权交还给调度器。
    5.  如果没有数据，也立即返回。

*   **优点**:
    *   **强控制力**：调度器可以精确控制每个模块的执行时机和频率，便于实现复杂的负载均衡。
    *   **单线程友好**：可以在一个单线程环境中模拟多任务。

*   **缺点**:
    *   **性能较低**：调度器轮询和函数调用的开销很大，响应延迟增加。
    *   **实现复杂**：模块需要将自己的逻辑拆分成“一小步”，并保证 `run()` 不会阻塞。
    *   **违背本质**：强行将事件驱动的模式扭转为命令驱动，不够自然。

---

### 结论与建议

根据您项目**高性能、GPU加速、实时处理**的核心目标，我**强烈推荐采用“选择 1：调度器作为启动器”的模式**。

这不仅是业界在构建此类高性能I/O密集型应用时的标准实践，也最符合您各个模块（数据接收、信号处理等）基于生产者-消费者模式和环形缓冲区的设计。

#### 需要在文档中明确的内容

为了解决这个模糊点，您应该在 05_任务调度器设计.md 中增加一节，例如：

> #### 4.4 模块调度模式
>
> 任务调度器根据模块的工作特性，采用两种不同的调度模式：
>
> 1.  **自主运行模式 (Autonomous Mode)**:
>     *   **适用模块**: 数据驱动型模块，如数据接收、信号处理、数据处理。
>     *   **协调机制**: 调度器通过 `start()` 方法触发模块启动。模块内部将创建并管理自己的工作线程，进入由数据或事件驱动的独立循环。`start()` 方法会立即返回。调度器此后仅负责通过 `stop()` 或 `pause()` 等命令管理其生命周期，而**不会调用其 `run()` 方法**。
>
> 2.  **协同调度模式 (Cooperative Mode)**:
>     *   **适用模块**: 需要由调度器驱动的模块，如状态监控（定时采集）、UI刷新（按固定频率）。
>     *   **协调机制**: 调度器在其主循环中，按预定策略（如固定频率）**主动调用**这些模块的 `run()` 方法，以驱动其完成单次工作。

通过这样的补充，您就明确了调度器和不同类型模块之间的“契约”，解决了设计中的模糊地带，确保了系统可以完美耦合。
```

4. 问题四：基础服务模块的集成方式不明确
// TODO:
```markdown
*   **解决方案**:
    *   **采用依赖注入 (DI)**: 这是解决此类问题的最佳实践。
    *   **明确集成流程**: 在 99_模块集成策略.md 和 05_任务调度器设计.md 中明确以下流程：
        1.  **系统启动时**: `main` 函数首先初始化 `LogManager` 和 `ConfigManager`。
        2.  **模块创建时**: `TaskScheduler` 在创建每个业务模块实例时，通过其构造函数或 `initialize()` 方法，将 `ConfigManager` 和 `LogManager` 的引用（或`shared_ptr`）**注入**到模块中。
        3.  **模块运行时**: 模块内部直接使用被注入的服务实例，而不是通过全局单例 `getInstance()`。这大大提高了模块的可测试性和解耦性。
```

---

以下是我发现的另外5点问题及其分析：

---

### 问题1：系统启停流程与模块依赖关系存在理想化假设

*   **问题描述**
    在《任务调度器设计》(05_任务调度器设计.md) 的8.1和8.2节中，系统启动和关闭流程被描绘成一个严格的、线性的、分阶段的过程。然而，在《模块集成策略设计》(99_模块集成策略.md) 的4.1节“模块依赖图”中，展示了更复杂的依赖关系，例如多个业务模块（数据接收、信号处理等）并行地依赖于任务调度器。线性的启停流程未能充分体现和利用这种并行依赖关系，可能导致启动时间不必要地延长，并且在关闭时可能因等待单个模块而阻塞。

*   **涉及文档**
    *   05_任务调度器设计.md (主要问题来源)
    *   99_模块集成策略.md (用于交叉验证)
    *   00_总体架构设计.md (分层架构图)

*   **分析**
    1.  **启动效率问题**：当前的线性启动流程（例如，先启动`RECEIVER`，再启动`SIGNAL`，再启动`DATA`）是次优的。根据依赖图，这三个模块是同级（第3层），它们都只依赖于第2层的`SCHEDULER`。理论上，一旦调度器就绪，这三个业务模块完全可以并行启动，从而显著缩短系统总体的就绪时间。
    2.  **关闭鲁棒性问题**：优雅关闭流程（8.2）也是线性的。如果其中一个模块（如`DataProc`）在“完成当前任务”时卡住或超时，整个关闭流程将被阻塞，后续的模块（如`SignalProc`）将无法被正常关闭。一个更健壮的关闭机制应该能并行地向所有无依赖的模块发送停止信号，并使用类似`std::future`或屏障（Barrier）的机制等待它们全部完成，同时处理个别模块的关闭超时。
    3.  **与架构不一致**：这种线性的启停设计，与文档中反复强调的“并行”、“解耦”、“高性能”的核心设计理念存在轻微的偏差。架构设计上是并行的，但启停策略却是串行的。

*   **修改建议**
    1.  **重构启动流程**：修改《任务调度器设计》8.1节的启动流程图和逻辑。应改为基于依赖图的拓扑排序启动。调度器在启动一个层级的模块后，可以并发启动所有依赖于该层级的下一层模块。例如，当核心服务层启动完毕后，应并发地初始化和启动所有业务处理层模块。
    2.  **优化关闭机制**：修改8.2节的关闭流程。调度器应首先向最上层模块（如UI）发送停止信号，然后可以并行地向所有业务模块发送停止信号。使用超时机制和并发原语（如`std::promise`/`std::future`）来管理关闭过程，确保即使某个模块关闭失败，也不会完全阻塞其他模块的资源释放。
    3.  **明确依赖驱动**：在文档中明确指出“系统启停是由模块依赖关系驱动的并行过程”，而不仅仅是固定的线性顺序。

---

### 问题2：配置热更新机制缺乏对模块状态和数据一致性的详细考量

*   **问题描述**
    《配置管理模块设计》(06_配置管理模块设计.md) 的6.1节“热更新机制”设计得非常全面，包含了“原子切换”、“暂存区”等高级概念。但它主要从配置模块自身的角度出发，缺少与任务调度器和业务模块如何安全协作的详细说明。例如，当一个关键配置（如`signal_processor`的`fft_size`）在`Running`状态下被热更新时，系统如何保证数据处理流水线的一致性和模块的平滑过渡？

*   **涉及文档**
    *   06_配置管理模块设计.md (主要问题来源)
    *   05_任务调度器设计.md (协作方)
    *   02_信号处理模块设计.md (受影响方)

*   **分析**
    1.  **状态不匹配风险**：直接在`Running`状态下应用某些配置变更可能是危险的。例如，更改FFT尺寸需要重新分配GPU内存和算法资源，这通常需要在模块`Stopped`或`Ready`状态下进行。当前设计没有明确配置变更是否需要模块状态转换。
    2.  **数据流不一致**：如果热更新发生在数据处理流水线中途，可能导致前一阶段的数据使用旧配置处理，后一阶段使用新配置，造成数据错乱或处理失败。
    3.  **“原子切换”的实现挑战**：配置模块的“原子切换”只保证了配置值的瞬间变更，但无法保证使用这些配置的业务模块（如信号处理模块）能够“原子地”响应这一变更。真正的原子性需要一个跨模块的事务性或协调过程。

*   **修改建议**
    1.  **引入配置变更等级**：在《配置管理模块设计》中，将配置项分为不同等级：
        *   **动态配置 (Dynamic)**：可随时在`Running`状态下安全更新（如日志级别、UI主题）。
        *   **重启配置 (Restart-Required)**：需要模块`stop`后`start`才能生效（如网络端口、线程池大小）。
        *   **重载配置 (Reload-Required)**：需要模块执行一个特定的`reload`或`reconfigure`方法（如算法参数、滤波器系数）。
    2.  **设计协调流程**：在《模块集成策略设计》(99_模块集成策略.md)中增加一节“配置热更新协作流程”。该流程应由`ConfigManager`发起，但由`TaskScheduler`协调。
        *   当`ConfigManager`检测到`Restart-Required`配置变更时，它应通知`TaskScheduler`。
        *   `TaskScheduler`负责将目标模块优雅地`stop`，应用新配置，然后`start`。
    3.  **定义`IConfigurable`接口**：在《模块集成策略设计》的接口定义中，可以考虑为需要复杂重载逻辑的模块设计一个`IConfigurable`接口，包含`ErrorCode reloadConfig(const NewConfig&)`方法，让模块自己负责安全地应用新配置。

---

### 问题3：GPU错误处理策略过于笼统，缺乏与调度和业务的联动

*   **问题描述**
    《信号处理模块设计》(02_信号处理模块设计.md) 第9节“错误处理设计”中提到了GPU错误分类和恢复策略，如“降级处理：GPU故障时切换到CPU实现”。这是一个很好的想法，但整个项目文档体系中并未设计CPU实现的算法和相应的调度机制。这使得该恢复策略目前只是一个悬空的“理想设计”。

*   **涉及文档**
    *   02_信号处理模块设计.md (主要问题来源)
    *   05_任务调度器设计.md (应参与恢复决策)
    *   00_总体架构设计.md (未体现CPU/GPU双路径)

*   **分析**
    1.  **降级路径缺失**：整个项目的设计高度依赖GPU（技术选型、架构设计、性能约束都指向这一点）。虽然提到了CPU降级，但没有任何一个模块设计了CPU算法的实现（例如，`信号处理模块设计`的算法模拟器全是`CUDA_FFT`等）。没有备用路径，降级策略就无法实施。
    2.  **调度器角色缺失**：当GPU发生严重错误时（如硬件故障），`信号处理模块`自身是无法决定“切换到CPU实现”的。这个决策应该是系统级的，应由`任务调度器`来做出。调度器需要接收到严重的GPU错误事件，然后决定是尝试重启模块、将系统降级还是直接安全关闭。
    3.  **对性能预期的影响**：如果真的实现了CPU降级，系统的性能将发生断崖式下跌（延迟从ms级变为百ms甚至s级）。`需求分析与目标`和`总体架构设计`中定义的性能约束将完全无法满足。文档需要说明在降级模式下，系统的服务等级（SLA）会发生怎样的变化。

*   **修改建议**
    1.  **明确MVP范围**：在《信号处理模块设计》中明确指出，MVP及V2.0阶段，GPU是唯一计算路径。CPU降级是远期（如V3.0）的演进目标，当前版本若遇严重GPU错误，采取的策略是“记录详细日志、安全关闭模块并告警”。
    2.  **设计错误上报与决策流程**：在《任务调度器设计》中，增加对来自`信号处理模块`的致命错误事件（如`GPU_FATAL_ERROR`）的处理逻辑。调度器接收到该事件后，应负责协调系统的响应，而不是让信号处理模块自己决定。
    3.  **（如果坚持要降级）补充设计**：
        *   在《信号处理模块设计》的算法模拟器中，为关键算法（如FFT）增加一个纯CPU实现版本（如`CPU_FFT`）。
        *   在《任务调度器设计》中，增加一个“降级模式”状态，并设计相应的资源重分配和调度策略。
        *   在《需求分析与目标》中，补充降级模式下的性能指标和功能限制。

---

### 问题4：日志与监控设计缺乏与业务和调试的深度结合

*   **问题描述**
    《日志监控模块设计》(07_日志监控模块设计.md) 定义了优秀的通用日志和监控框架（基于spdlog、指标收集等）。但它更多是平台级的，缺少如何利用日志和监控来追踪一次完整的业务处理流程（例如，一个数据包从接收到最终显示的全过程）的指导性设计，即缺乏“分布式追踪”（Distributed Tracing）或“全链路监控”的思想。

*   **涉及文档**
    *   07_日志监控模块设计.md (主要问题来源)
    *   01_数据接收模块设计.md, 02_信号处理模块设计.md, 03_数据处理模块设计.md (日志调用方)
    *   99_模块集成策略.md (应定义集成规范)

*   **分析**
    1.  **问题定位困难**：当系统出现性能瓶颈（如端到端延迟超标）或数据处理错误时，当前的日志是分散在各个模块的日志文件中的。开发或运维人员需要手动关联不同文件中的日志，通过时间戳来猜测处理流程，这非常低效且容易出错。
    2.  **性能分析粒度粗**：监控模块收集的是模块级的宏观指标（如CPU使用率、处理包数）。但如果想知道“某个特定数据包在哪一处理阶段耗时最长”，当前的监控体系无法回答。
    3.  **错失现代可观测性（Observability）实践**：现代高性能系统设计非常强调可观测性，其中“Trace ID”或“Correlation ID”是核心实践。它能将分散的日志和指标串联成一个完整的调用链，极大提升调试和优化的效率。

*   **修改建议**
    1.  **引入Trace ID**：在《模块集成策略设计》的3.2节“数据交换格式”中，为`DataObject`或其`Metadata`增加一个`trace_id`字段。
    2.  **生成与传递Trace ID**：
        *   在《数据接收模块设计》中，当一个外部数据包首次进入系统时，为其生成一个唯一的`trace_id`。
        *   要求所有后续模块（信号处理、数据处理）在处理和转发`DataObject`时，必须保持并传递这个`trace_id`。
    3.  **在日志中集成Trace ID**：修改《日志监控模块设计》3.2节的日志格式，增加`trace_id`字段。
        *   新格式示例：`[时间戳] [级别] [模块名] [TraceID] 消息内容`
        *   提供一个日志宏的变体或上下文管理器，让业务代码可以方便地将`trace_id`注入到日志中。
    4.  **在监控中利用Trace ID**：在《日志监控模块设计》中增加说明，鼓励在记录性能指标时附带`trace_id`，以便进行更细粒度的性能分析（例如，记录每个阶段的开始和结束时间戳，并与`trace_id`关联）。

---

### 问题5：模块职责边界存在轻微重叠和模糊

*   **问题描述**
    在《数据处理模块设计》(03_数据处理模块设计.md) 和《任务调度器设计》(05_任务调度器设计.md) 中，都提到了“性能优化”和“错误处理”的职责。虽然每个模块内部都需要处理自己的错误和性能，但系统级的策略（如负载均衡、故障恢复）的决策权归属不够清晰，存在职责重叠的可能。

*   **涉及文档**
    *   03_数据处理模块设计.md (职责定义)
    *   05_任务调度器设计.md (职责定义)
    *   02_信号处理模块设计.md (职责定义)

*   **分析**
    1.  **决策权模糊**：《数据处理模块设计》的职责图中有“性能优化”，《信号处理模块设计》中有“负载均衡”，而《任务调度器设计》的核心职责就是“资源调度”和“负载均衡”。当系统需要进行负载均衡时，到底由谁来决策？是信号处理模块自己决定少处理一些任务，还是任务调度器强制给它分配更少的资源？
    2.  **恢复策略冲突**：当一个模块（如数据处理模块）发生错误时，它内部的`ERROR_HANDLER`可能会尝试恢复。但如果这个错误是系统性的，`任务调度器`的`HEALTH_MONITOR`和`自动恢复策略`也可能介入。这两个恢复机制可能会发生冲突。
    3.  **“单一职责原则”的挑战**：最佳实践是，业务模块（如数据/信号处理）应专注于其核心业务逻辑，并将系统级的、协调性的决策（如负载均衡、故障转移）交由专门的协调者（任务调度器）处理。业务模块应该是策略的“执行者”，而不是“决策者”。

*   **修改建议**
    1.  **明确职责划分**：
        *   在各个模块设计文档的“模块职责定义”一节中，重新明确职责边界。
        *   **业务模块**（数据接收、信号处理、数据处理）：负责**执行**性能策略和错误处理。它们负责**检测**内部问题（如处理队列过长、算法执行超时）并**上报**事件给任务调度器。它们的“性能优化”应聚焦于算法和代码层面的优化。
        *   **任务调度器**：负责**决策**系统级的性能和恢复策略。它根据从各模块收到的状态和事件，做出负载均衡、资源重分配、模块重启或系统降级等决策。
    2.  **修改职责图**：
        *   在《信号处理模块设计》和《数据处理模块设计》的职责图中，将“负载均衡”、“性能优化”等修改为“执行性能策略”、“上报性能指标”，以强调其执行者角色。
        *   在《任务调度器设计》的职责图中，强化其“全局负载均衡决策”、“系统级故障恢复决策”的职责。
    3.  **定义交互协议**：在《模块集成策略设计》中，详细定义模块与调度器之间的状态/事件交互协议。例如，定义`PERFORMANCE_DEGRADATION`事件，当业务模块发现自身性能下降时，可以发送此事件给调度器，由调度器来决定下一步行动。

---

以上是我发现的5个更深层次的问题。它们主要关注于系统在动态运行、异常处理和长期维护中可能遇到的挑战，希望能对您的项目设计提供有价值的参考。您的文档体系已经非常完善和专业，这些建议旨在使其更加健壮和贴近工程实践。
